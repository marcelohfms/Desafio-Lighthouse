{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "58cf901c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import class_weight, resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "c0ee2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = pd.read_csv(\"desafio_manutencao_preditiva_treino.csv\")\n",
    "df_teste = pd.read_csv(\"desafio_manutencao_preditiva_teste.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c93f9",
   "metadata": {},
   "source": [
    "# Definição dos tipos de variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "bc9037b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type', 'air_temperature_k', 'process_temperature_k',\n",
       "       'rotational_speed_rpm', 'torque_nm', 'tool_wear_min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'failure_type'\n",
    "features = df_treino.columns.drop([target, 'udi', 'product_id'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "a964eb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_temperature_k', 'process_temperature_k', 'rotational_speed_rpm',\n",
       "       'torque_nm', 'tool_wear_min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical = df_treino[features].select_dtypes('number').columns\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "5802b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = 'type'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7530cf",
   "metadata": {},
   "source": [
    "# Separação do dataset em entrada e saída (X e y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d1ee4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_treino[features]\n",
    "y = df_treino[target] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d619cb",
   "metadata": {},
   "source": [
    "# Divisão do dataset em treino e validação\n",
    "\n",
    "Como os dados de teste não possuem a variável target, é importante que separemos uma parte do dataset de treino para validar o modelo e garantir que não haja overfitting. Dividiremos então o dataset em treino e validação. Usaremos 25% dos dados para validação.\n",
    "\n",
    "Além disso, é importante dividir os dados antes do pré-processamento para evitar uma falha chamada _data leakage_, que é o compartilhamento de informações entre o conjunto de dados usado no treinamento do modelo e o dataset usado para avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "f1f02c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em Dados de Treino e Validação.\n",
    "X_treino, X_val, y_treino, y_val = train_test_split(X, y, test_size = 0.25, random_state = 101, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fef7cc",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos Dados\n",
    "\n",
    "Muitos algoritmos preditivos esperam receber os dados padronizados e codificados. Portanto, é necessário transformar os dados para adequá-los aos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "93bac177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cópia dos dados = boa prática\n",
    "X_treino_copy = X_treino.copy()\n",
    "y_treino_copy = y_treino.copy()\n",
    "X_val_copy = X_val.copy()\n",
    "y_val_copy = y_val.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7b76e",
   "metadata": {},
   "source": [
    "## Encoding da variável categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3b763c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número de classes da variável categórica\n",
    "X_treino[categorical].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfd070",
   "metadata": {},
   "source": [
    "A variável type possui apenas 3 classes, vamos aplicar a técnica de OneHotEncoding para sua codificação;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "0b5a0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>302.1</td>\n",
       "      <td>311.2</td>\n",
       "      <td>1598</td>\n",
       "      <td>37.9</td>\n",
       "      <td>148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>301.4</td>\n",
       "      <td>310.6</td>\n",
       "      <td>1630</td>\n",
       "      <td>30.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>298.0</td>\n",
       "      <td>308.3</td>\n",
       "      <td>1473</td>\n",
       "      <td>42.6</td>\n",
       "      <td>107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>297.3</td>\n",
       "      <td>308.1</td>\n",
       "      <td>1709</td>\n",
       "      <td>30.6</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>298.0</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1436</td>\n",
       "      <td>48.2</td>\n",
       "      <td>201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>298.8</td>\n",
       "      <td>310.0</td>\n",
       "      <td>1295</td>\n",
       "      <td>52.7</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>299.5</td>\n",
       "      <td>309.1</td>\n",
       "      <td>1420</td>\n",
       "      <td>49.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>297.1</td>\n",
       "      <td>308.0</td>\n",
       "      <td>1348</td>\n",
       "      <td>58.0</td>\n",
       "      <td>162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>298.7</td>\n",
       "      <td>309.6</td>\n",
       "      <td>1424</td>\n",
       "      <td>50.4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>297.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1636</td>\n",
       "      <td>31.3</td>\n",
       "      <td>192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2685              302.1                  311.2                  1598   \n",
       "2281              301.4                  310.6                  1630   \n",
       "202               298.0                  308.3                  1473   \n",
       "6057              297.3                  308.1                  1709   \n",
       "998               298.0                  308.7                  1436   \n",
       "...                 ...                    ...                   ...   \n",
       "5535              298.8                  310.0                  1295   \n",
       "1450              299.5                  309.1                  1420   \n",
       "769               297.1                  308.0                  1348   \n",
       "6508              298.7                  309.6                  1424   \n",
       "830               297.2                  308.6                  1636   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2685       37.9            148  1.0  0.0  0.0  \n",
       "2281       30.3              2  0.0  0.0  1.0  \n",
       "202        42.6            107  0.0  1.0  0.0  \n",
       "6057       30.6            196  0.0  0.0  1.0  \n",
       "998        48.2            201  0.0  0.0  1.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "5535       52.7             74  0.0  0.0  1.0  \n",
       "1450       49.5              5  0.0  0.0  1.0  \n",
       "769        58.0            162  0.0  1.0  0.0  \n",
       "6508       50.4             32  0.0  0.0  1.0  \n",
       "830        31.3            192  0.0  1.0  0.0  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OneHotEncoding\n",
    "onehot = OneHotEncoder()\n",
    "cat_encoded = onehot.fit_transform(X_treino[[categorical]])\n",
    "X_treino[onehot.categories_[0]] = cat_encoded.toarray()\n",
    "X_treino.drop(columns='type', inplace = True)\n",
    "X_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669d6fc",
   "metadata": {},
   "source": [
    "## Padronização e normalização das variáveis numéricas\n",
    "\n",
    "Os dados podem ser padronizados ou normalizados. A padronização consiste em deixá-los numa distribuição normal com média 0 e desvio padrão 1. Já a normalização trata de deixar os dados numa escala única, geralmente de 0 a 1 ou de -1 a 1. \n",
    "\n",
    "Para os dados cuja distribuição não é gaussiana (ou normal), a normalização geralmente é uma melhor opção.\n",
    "\n",
    "Para os dados nesse problema, será aplicada a padronização para as variáveis numéricas cuja distribuição é normal e a normalização para a variável tool_wear_min que tem uma distribuição uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "065430da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = [col for col in numerical if col != 'tool_wear_min']\n",
    "scaler = StandardScaler()\n",
    "X_treino[scale] = scaler.fit_transform(X_treino[scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4020852a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>0.338430</td>\n",
       "      <td>-0.217692</td>\n",
       "      <td>148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0.709081</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>0.518479</td>\n",
       "      <td>-0.978057</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>-0.364888</td>\n",
       "      <td>0.252533</td>\n",
       "      <td>107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>0.962976</td>\n",
       "      <td>-0.948043</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>-0.573070</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-1.366413</td>\n",
       "      <td>1.263018</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.245078</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>-0.663095</td>\n",
       "      <td>0.942865</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>-1.450332</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>-1.068206</td>\n",
       "      <td>1.793273</td>\n",
       "      <td>162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>-0.646829</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.640589</td>\n",
       "      <td>1.032908</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>0.552238</td>\n",
       "      <td>-0.878009</td>\n",
       "      <td>192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2685           1.060613               0.813864              0.338430   \n",
       "2281           0.709081               0.410828              0.518479   \n",
       "202           -0.998362              -1.134143             -0.364888   \n",
       "6057          -1.349894              -1.268489              0.962976   \n",
       "998           -0.998362              -0.865453             -0.573070   \n",
       "...                 ...                    ...                   ...   \n",
       "5535          -0.596610               0.007792             -1.366413   \n",
       "1450          -0.245078              -0.596762             -0.663095   \n",
       "769           -1.450332              -1.335661             -1.068206   \n",
       "6508          -0.646829              -0.260899             -0.640589   \n",
       "830           -1.400113              -0.932625              0.552238   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2685  -0.217692            148  1.0  0.0  0.0  \n",
       "2281  -0.978057              2  0.0  0.0  1.0  \n",
       "202    0.252533            107  0.0  1.0  0.0  \n",
       "6057  -0.948043            196  0.0  0.0  1.0  \n",
       "998    0.812802            201  0.0  0.0  1.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "5535   1.263018             74  0.0  0.0  1.0  \n",
       "1450   0.942865              5  0.0  0.0  1.0  \n",
       "769    1.793273            162  0.0  1.0  0.0  \n",
       "6508   1.032908             32  0.0  0.0  1.0  \n",
       "830   -0.878009            192  0.0  1.0  0.0  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "c5bf8374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>0.338430</td>\n",
       "      <td>-0.217692</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0.709081</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>0.518479</td>\n",
       "      <td>-0.978057</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>-0.364888</td>\n",
       "      <td>0.252533</td>\n",
       "      <td>0.434959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>0.962976</td>\n",
       "      <td>-0.948043</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>-0.573070</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-1.366413</td>\n",
       "      <td>1.263018</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.245078</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>-0.663095</td>\n",
       "      <td>0.942865</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>-1.450332</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>-1.068206</td>\n",
       "      <td>1.793273</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>-0.646829</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.640589</td>\n",
       "      <td>1.032908</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>0.552238</td>\n",
       "      <td>-0.878009</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2685           1.060613               0.813864              0.338430   \n",
       "2281           0.709081               0.410828              0.518479   \n",
       "202           -0.998362              -1.134143             -0.364888   \n",
       "6057          -1.349894              -1.268489              0.962976   \n",
       "998           -0.998362              -0.865453             -0.573070   \n",
       "...                 ...                    ...                   ...   \n",
       "5535          -0.596610               0.007792             -1.366413   \n",
       "1450          -0.245078              -0.596762             -0.663095   \n",
       "769           -1.450332              -1.335661             -1.068206   \n",
       "6508          -0.646829              -0.260899             -0.640589   \n",
       "830           -1.400113              -0.932625              0.552238   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2685  -0.217692       0.601626  1.0  0.0  0.0  \n",
       "2281  -0.978057       0.008130  0.0  0.0  1.0  \n",
       "202    0.252533       0.434959  0.0  1.0  0.0  \n",
       "6057  -0.948043       0.796748  0.0  0.0  1.0  \n",
       "998    0.812802       0.817073  0.0  0.0  1.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "5535   1.263018       0.300813  0.0  0.0  1.0  \n",
       "1450   0.942865       0.020325  0.0  0.0  1.0  \n",
       "769    1.793273       0.658537  0.0  1.0  0.0  \n",
       "6508   1.032908       0.130081  0.0  0.0  1.0  \n",
       "830   -0.878009       0.780488  0.0  1.0  0.0  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = ['tool_wear_min']\n",
    "normalizer = MinMaxScaler()\n",
    "X_treino[normalize] = normalizer.fit_transform(X_treino[normalize])\n",
    "X_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7f6a6",
   "metadata": {},
   "source": [
    "Na análise exploratória, vimos que as variáveis rotational_speed_rpm e torque_nm possuíam valores _outliers_, vamos verificar agora quantos valores desse existem no dataset de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "a53bb455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457986</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>5.948093</td>\n",
       "      <td>-2.638855</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>4.985954</td>\n",
       "      <td>-2.558816</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.201316</td>\n",
       "      <td>3.106689</td>\n",
       "      <td>-2.108600</td>\n",
       "      <td>0.581301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.558424</td>\n",
       "      <td>0.746691</td>\n",
       "      <td>3.101062</td>\n",
       "      <td>-2.028562</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>3.658090</td>\n",
       "      <td>-2.308696</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.402834</td>\n",
       "      <td>-1.191990</td>\n",
       "      <td>3.534108</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.106454</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-1.591474</td>\n",
       "      <td>3.173936</td>\n",
       "      <td>0.605691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.558424</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.270762</td>\n",
       "      <td>3.354022</td>\n",
       "      <td>0.199187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.257111</td>\n",
       "      <td>-0.126553</td>\n",
       "      <td>-1.203243</td>\n",
       "      <td>3.273984</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.552764</td>\n",
       "      <td>-1.552089</td>\n",
       "      <td>3.043873</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    air_temperature_k  process_temperature_k  rotational_speed_rpm  torque_nm  \\\n",
       "0            0.457986               0.007792              5.948093  -2.638855   \n",
       "1            0.206892               0.343655              4.985954  -2.558816   \n",
       "2           -0.998362              -1.201316              3.106689  -2.108600   \n",
       "3            0.558424               0.746691              3.101062  -2.028562   \n",
       "4           -1.349894              -1.134143              3.658090  -2.308696   \n",
       "..                ...                    ...                   ...        ...   \n",
       "86          -1.400113              -1.402834             -1.191990   3.534108   \n",
       "87           0.106454               0.679519             -1.591474   3.173936   \n",
       "88           0.558424               0.276483             -1.270762   3.354022   \n",
       "89           0.257111              -0.126553             -1.203243   3.273984   \n",
       "90           1.412145               1.552764             -1.552089   3.043873   \n",
       "\n",
       "    tool_wear_min    H    L    M  \n",
       "0        0.841463  0.0  1.0  0.0  \n",
       "1        0.048780  0.0  1.0  0.0  \n",
       "2        0.581301  0.0  0.0  1.0  \n",
       "3        0.658537  0.0  0.0  1.0  \n",
       "4        0.130081  0.0  1.0  0.0  \n",
       "..            ...  ...  ...  ...  \n",
       "86       0.699187  0.0  1.0  0.0  \n",
       "87       0.605691  0.0  1.0  0.0  \n",
       "88       0.199187  0.0  1.0  0.0  \n",
       "89       0.646341  1.0  0.0  0.0  \n",
       "90       0.951220  0.0  0.0  1.0  \n",
       "\n",
       "[91 rows x 8 columns]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# União de todas as linhas onde uma das duas variáveis possui um outlier\n",
    "outliers = pd.merge(X_treino.loc[abs(X_treino['rotational_speed_rpm']) > 3],\n",
    "                    X_treino.loc[abs(X_treino['torque_nm']) > 3],\n",
    "                    how='outer')\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "f52e192f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0182"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proporção de outliers\n",
    "len(outliers)/len(X_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701175e",
   "metadata": {},
   "source": [
    "Como a proporção de outliers é de menos de 2%, não há grande prejuízo em apenas remover essas linhas para que elas não comprometam o treinamento do modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c8a2a97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>0.338430</td>\n",
       "      <td>-0.217692</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0.709081</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>0.518479</td>\n",
       "      <td>-0.978057</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>-0.364888</td>\n",
       "      <td>0.252533</td>\n",
       "      <td>0.434959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>0.962976</td>\n",
       "      <td>-0.948043</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>-0.573070</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-1.366413</td>\n",
       "      <td>1.263018</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.245078</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>-0.663095</td>\n",
       "      <td>0.942865</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>-1.450332</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>-1.068206</td>\n",
       "      <td>1.793273</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>-0.646829</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.640589</td>\n",
       "      <td>1.032908</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>0.552238</td>\n",
       "      <td>-0.878009</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4909 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2685           1.060613               0.813864              0.338430   \n",
       "2281           0.709081               0.410828              0.518479   \n",
       "202           -0.998362              -1.134143             -0.364888   \n",
       "6057          -1.349894              -1.268489              0.962976   \n",
       "998           -0.998362              -0.865453             -0.573070   \n",
       "...                 ...                    ...                   ...   \n",
       "5535          -0.596610               0.007792             -1.366413   \n",
       "1450          -0.245078              -0.596762             -0.663095   \n",
       "769           -1.450332              -1.335661             -1.068206   \n",
       "6508          -0.646829              -0.260899             -0.640589   \n",
       "830           -1.400113              -0.932625              0.552238   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2685  -0.217692       0.601626  1.0  0.0  0.0  \n",
       "2281  -0.978057       0.008130  0.0  0.0  1.0  \n",
       "202    0.252533       0.434959  0.0  1.0  0.0  \n",
       "6057  -0.948043       0.796748  0.0  0.0  1.0  \n",
       "998    0.812802       0.817073  0.0  0.0  1.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "5535   1.263018       0.300813  0.0  0.0  1.0  \n",
       "1450   0.942865       0.020325  0.0  0.0  1.0  \n",
       "769    1.793273       0.658537  0.0  1.0  0.0  \n",
       "6508   1.032908       0.130081  0.0  0.0  1.0  \n",
       "830   -0.878009       0.780488  0.0  1.0  0.0  \n",
       "\n",
       "[4909 rows x 8 columns]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treino = X_treino.loc[abs(X_treino['rotational_speed_rpm']) < 3]\n",
    "X_treino = X_treino.loc[abs(X_treino['torque_nm']) < 3]\n",
    "X_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177add46",
   "metadata": {},
   "source": [
    "Se removemos essas linhas do dataset de entrada X, também as removeremos da saída y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "53b1826e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2685    No Failure\n",
       "2281    No Failure\n",
       "202     No Failure\n",
       "6057    No Failure\n",
       "998     No Failure\n",
       "           ...    \n",
       "5535    No Failure\n",
       "1450    No Failure\n",
       "769     No Failure\n",
       "6508    No Failure\n",
       "830     No Failure\n",
       "Name: failure_type, Length: 4909, dtype: object"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo as mesmas linhas do dataset y\n",
    "y_treino = y_treino.loc[y_treino_copy.index.isin(X_treino.index)]\n",
    "y_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea2c0a",
   "metadata": {},
   "source": [
    "## Encoding da variável target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7092f413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número de classes da variável target\n",
    "y_treino.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd9ea6",
   "metadata": {},
   "source": [
    "A variável target failure_type possui 6 classes distintas. Vamos aplicar a técnica de label encoding para sua codificação, a fim de que haja apenas uma coluna para a variável a ser prevista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "47512bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2685    1\n",
       "2281    1\n",
       "202     1\n",
       "6057    1\n",
       "998     1\n",
       "       ..\n",
       "5535    1\n",
       "1450    1\n",
       "769     1\n",
       "6508    1\n",
       "830     1\n",
       "Length: 4909, dtype: int32"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_treino = pd.Series(le.fit_transform(y_treino), index = y_treino.index)\n",
    "y_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29995548",
   "metadata": {},
   "source": [
    "## Pré-processamento do dataset de validação\n",
    "\n",
    "Agora, com as transformações treinadas, é preciso transformar todos os dados de validação da mesma forma que os dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "5735ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variável target\n",
    "y_val = pd.Series(le.transform(y_val), index = y_val.index)\n",
    "\n",
    "# One Hot Encoding\n",
    "cat_encoded_val = onehot.transform(X_val[[categorical]])\n",
    "X_val[onehot.categories_[0]] = cat_encoded_val.toarray()\n",
    "X_val.drop(columns='type', inplace = True)\n",
    "\n",
    "# Scaling\n",
    "X_val[scale] = scaler.transform(X_val[scale])\n",
    "\n",
    "# Normalizing\n",
    "X_val[normalize] = normalizer.transform(X_val[normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "92beb2a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5089    1\n",
       "2028    1\n",
       "2438    1\n",
       "4076    1\n",
       "2330    4\n",
       "       ..\n",
       "6252    1\n",
       "6532    1\n",
       "6002    1\n",
       "664     1\n",
       "454     1\n",
       "Length: 1667, dtype: int32"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "42b32358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>-1.078106</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>0.307330</td>\n",
       "      <td>-0.059381</td>\n",
       "      <td>-0.499925</td>\n",
       "      <td>0.232524</td>\n",
       "      <td>0.686992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>-0.691228</td>\n",
       "      <td>0.402605</td>\n",
       "      <td>0.174797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>0.457986</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>0.142480</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>-0.777961</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>2.785976</td>\n",
       "      <td>-2.338711</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532</th>\n",
       "      <td>-0.747267</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>1.030495</td>\n",
       "      <td>-1.068101</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>0.940470</td>\n",
       "      <td>-1.408264</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>-1.902302</td>\n",
       "      <td>-2.141734</td>\n",
       "      <td>0.659143</td>\n",
       "      <td>-0.898019</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>-1.098799</td>\n",
       "      <td>-0.663935</td>\n",
       "      <td>-0.460539</td>\n",
       "      <td>0.112466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "5089           0.206892               0.948209              0.698529   \n",
       "2028           0.307330              -0.059381             -0.499925   \n",
       "2438           1.060613               0.881037             -0.691228   \n",
       "4076           0.457986               0.612346             -0.342382   \n",
       "2330           0.809518               0.343655              0.360936   \n",
       "...                 ...                    ...                   ...   \n",
       "6252          -1.249456              -1.335661              2.785976   \n",
       "6532          -0.747267              -0.529589              1.030495   \n",
       "6002          -1.400113              -1.335661              0.940470   \n",
       "664           -1.902302              -2.141734              0.659143   \n",
       "454           -1.098799              -0.663935             -0.460539   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "5089  -1.078106       0.524390  0.0  1.0  0.0  \n",
       "2028   0.232524       0.686992  0.0  0.0  1.0  \n",
       "2438   0.402605       0.174797  0.0  1.0  0.0  \n",
       "4076   0.142480       0.406504  0.0  1.0  0.0  \n",
       "2330  -0.777961       0.008130  1.0  0.0  0.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "6252  -2.338711       0.520325  0.0  0.0  1.0  \n",
       "6532  -1.068101       0.691057  0.0  0.0  1.0  \n",
       "6002  -1.408264       0.813008  0.0  1.0  0.0  \n",
       "664   -0.898019       0.552846  0.0  1.0  0.0  \n",
       "454    0.112466       0.000000  0.0  1.0  0.0  \n",
       "\n",
       "[1667 rows x 8 columns]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8a0b5",
   "metadata": {},
   "source": [
    "## Balanceamento das classes da variável target\n",
    "\n",
    "A última etapa do pré-processamento é balancear as instâncias da variável target. Como visto na Análise Exploratória, há muito mais máquinas sem falhas do que com os 5 tipos de falhas apresentados. Se o modelo preditivo for alimentado dessa forma, ele aprenderá muito melhor sobre máquinas sem falhas do que sobre máquinas com falhas, portanto, é preciso aplicar alguma técnica de balanceamento de classes.\n",
    "\n",
    "Pode-se aplicar o _oversampling_, que cria mais registros das classes que minoritárias ou o _undersampling_, que remove registros das classes majoritárias. \n",
    "\n",
    "O dataset possui poucos registros - apenas cerca de 6,5 mil - portanto, se aplicado o _undersampling_, teremos ainda menos dados para o treinamento e perderemos características demais sobre os dados.\n",
    "\n",
    "O _oversampling_ também não funcionaria muito bem para esse modelo, visto que a diferença entre as classes é muito grande e são muitas classes, o que faria com que fossem seriam criados registros artificias demais para resolver a diferença.\n",
    "\n",
    "A abordagem que faremos será a de converter o problema de classificação multiclasse em um problema de classificação binário, respondendo a pergunta: \"Há ou não falha nas máquinas?\". Com essa mudança na pergunta-chave do problema, haverão apenas duas classes a serem balanceadas, e poderemos então aplicar uma técnica de _oversampling_, nesse caso o SMOTE. Após respondido esse primeiro questionamento, abordaremos então, para as máquinas em que o modelo previu falha, qual o tipo de falha presente, se houver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "bdc8f368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x=y_treino)\n",
    "plt.title('Tipos de falha por máquina');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "a0bf7126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definindo o y_treino binário - falha ou não falha\n",
    "y_treino_bin, y_val_bin = y_treino.where(y_treino == 1, 0), y_val.where(y_val == 1, 0)\n",
    "\n",
    "# Plot\n",
    "sns.countplot(x=y_treino_bin)\n",
    "plt.title('Falha (0) ou Não Falha (1)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "542c8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto balanceador\n",
    "smote = SMOTE(random_state = 1337)\n",
    "\n",
    "# Aplica o balanceador\n",
    "X_res, y_res = smote.fit_resample(X_treino, y_treino_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "e47070f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGzCAYAAADOnwhmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA580lEQVR4nO3deVxU9f7H8feAMqAICCJI4pJaZm6FqWO5RpJiZqll1xKXNsMKKTXuo3CprjfNLZe0a6kt3lxKb2lp7nWVtDDMJb3WxeWXAaYsrqBwfn/04/wcBxURGOy8no/HPB7O93znnM85c2Z8zznfc7AZhmEIAADAwjzcXQAAAIC7EYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgAAIDlEYgsbOPGjbLZbNq4caPZ1qlTJzVt2rRcll9QUKCmTZvq9ddfv+rXnjt3TuHh4Zo1a1YZVFb++vXrp2rVqunFF19UZmamAgIClJWV5e6yXBw4cEA2m03z58832wYOHChfX99yq6F79+564oknSvTafv366aGHHirlii6vXr16GjhwYLku81Lmz58vm82mAwcOlNkyKtL64v8V9dkdM2aMbDab+4qqYAhE16HCL7WiHi+99JK7yyu2f/7znzp8+LCGDRvm1J6bm6tRo0YpLCxMPj4+atOmjdasWePUp3LlyoqPj9frr7+us2fPlmfZl1UYMm02m5KTk12mFxUe9uzZo40bN2rs2LH67LPPFBQUpMjISAUEBJRJXRc/+vXrV2rLKWubN2/WV199pVGjRjm1v/766+rZs6dCQkJks9k0ZsyYIl8/atQoffLJJ9qxY0c5VAvgelLJ3QWg5MaNG6f69es7tZXX0Z3SMHHiRPXr10/+/v5O7QMHDtTSpUsVFxenRo0aaf78+erevbs2bNigu+66y+w3aNAgvfTSS1q4cKEGDx5c3uVf0ZgxY/T5559fsd+NN96o5ORk3XDDDYqLi1NaWppq1apVJjU999xzuuOOO5za6tWrVybLKgsTJ07U3XffrYYNGzq1v/zyywoNDdVtt92m1atXX/L1t912m1q1aqVJkybp/fffL+tyK5zHHntM/fr1k91ud3cpqABefvnl6+pHdFkjEF3HunXrplatWrm7jBL54YcftGPHDk2aNMmpfdu2bfr44481ceJEvfjii5KkAQMGqGnTpho5cqS2bNli9g0ICFDXrl01f/78CheIWrZsqRUrVmj79u26/fbbL9vX29tbN9xwgyTJw8NDYWFhZVZX+/bt1adPnzKbf1nKyMjQypUrNXv2bJdpqampqlevnn7//XcFBwdfdj4PPfSQRo8erVmzZpXrqb6KwNPTU56enu4u47p1+vRpValSxd1llJpKlSqpUiViQCFOmf0JHTx4UM8884xuvvlm+fj4KCgoSH379r2qcQN79uxR586dVaVKFd1www2aMGGC0/S8vDwlJiYqIiJC/v7+qlq1qtq3b68NGzYUa/7Lly+Xl5eXOnTo4NS+dOlSeXp66sknnzTbvL29NWTIECUlJenw4cNO/e+55x79+9//1vHjx6+4zIyMDA0ZMkQhISHy9vZWixYttGDBAqc+RY2rkoo+/345zz77rKpXr37JUzcXWrZsmbp3766wsDDZ7XY1aNBAr776qvLz8136LlmyRBEREfLx8VGNGjX06KOP6tdffy1WTZdz/Phxvfjii2rWrJl8fX3l5+enbt26XdWppV9//VW9evWSr6+vgoOD9eKLL7qsw5tvvql27dopKChIPj4+ioiI0NKlS4s1/5UrV+r8+fOKjIx0mXY1R7nuuecenTp1yuU07LUyDEOvvfaaateurSpVqqhz587avXt3kX2zsrIUFxen8PBw2e12NWzYUG+88YYKCgrMPoX73JtvvqkpU6aobt268vHxUceOHbVr1y6Xea5fv17t27dX1apVFRAQoPvvv18//fSTU5+ixhB9//33ioqKUo0aNeTj46P69esX6wfG1azvf//7X/Xt21eBgYGqUqWK2rZtq5UrV15xGZJ0/vx5vfrqq2rQoIHsdrvq1aunv/71r8rNzTX79OjRQzfeeGORr3c4HC4/HD/88EPzcxQYGKh+/fq5fLcUjqdMTk5Whw4dVKVKFf31r3+VVLxtVtx93WazadiwYVqyZImaNGkiHx8fORwO7dy5U5I0Z84cNWzYUN7e3urUqZPL9/iFdbZr186sp6gfDhcragxRYT3Lly9X06ZNZbfbdeutt2rVqlUur9+4caNatWolb29vNWjQQHPmzLmuxyURDa9j2dnZ+v33353aatSooe+++05btmxRv379VLt2bR04cEBvv/22OnXqpD179lzxF05mZqbuvfdePfjgg3rooYe0dOlSjRo1Ss2aNVO3bt0kSTk5OZo7d64eeeQRPfHEEzpx4oTeffddRUVFadu2bWrZsuVll7FlyxY1bdpUlStXdmr/4YcfdNNNN8nPz8+pvXXr1pKklJQUhYeHm+0REREyDENbtmxRjx49Lrm8M2fOqFOnTvr55581bNgw1a9fX0uWLNHAgQOVlZWl559//rL1Xi0/Pz8NHz5ciYmJVzxK9N5776latWqKj49X1apVtWHDBiUmJionJ0cTJ040+82fP1+DBg3SHXfcofHjxys9PV3Tpk3T5s2b9cMPPxRrzNGJEydc9pnAwED997//1fLly9W3b1/Vr19f6enpmjNnjjp27Kg9e/Zc8ahVfn6+oqKi1KZNG7355ptau3atJk2apAYNGmjo0KFmv2nTpqlnz57q37+/8vLy9PHHH6tv375asWKFoqOjL7uMLVu2KCgoSHXr1r3iel5O4X86mzdv1gMPPHBN87pQYmKiXnvtNXXv3l3du3fX9u3b1bVrV+Xl5Tn1O336tDp27Khff/1VTz31lOrUqaMtW7YoISFBv/32m6ZOnerU//3339eJEycUGxurs2fPatq0aerSpYt27typkJAQSdLatWvVrVs33XjjjRozZozOnDmj6dOn684779T27dsvGRgzMjLUtWtXBQcH66WXXlJAQIAOHDigTz/9tNTWNz09Xe3atdPp06f13HPPKSgoSAsWLFDPnj21dOnSK74Hjz/+uBYsWKA+ffrohRde0NatWzV+/Hj99NNPWrZsmSTp4Ycf1oABA/Tdd985nRI+ePCgvv32W6fP0euvv65XXnlFDz30kB5//HEdPXpU06dPV4cOHVw+R8eOHVO3bt3Ur18/PfroowoJCSn2Nruaff2bb77RZ599ptjYWEnS+PHj1aNHD40cOVKzZs3SM888o8zMTE2YMEGDBw/W+vXrnV6fmZmp7t2766GHHtIjjzyixYsXa+jQofLy8irR0fN///vf+vTTT/XMM8+oWrVqeuutt9S7d28dOnRIQUFBkv74rr733ntVq1YtjR07Vvn5+Ro3btwVj9BWaAauO/PmzTMkFfkwDMM4ffq0y2uSkpIMScb7779vtm3YsMGQZGzYsMFs69ixo0u/3NxcIzQ01Ojdu7fZdv78eSM3N9dpGZmZmUZISIgxePDgK65D7dq1neZX6NZbbzW6dOni0r57925DkjF79myn9iNHjhiSjDfeeOOyy5s6daohyfjwww/Ntry8PMPhcBi+vr5GTk6OYRhFbxPDMIzU1FRDkjFv3rzLLqfw9UuWLDGysrKM6tWrGz179jSnx8TEGFWrVnV6zalTp1zm89RTTxlVqlQxzp49a9Zas2ZNo2nTpsaZM2fMfitWrDAkGYmJicWqq6hHamqqcfbsWSM/P99lne12uzFu3LjLboeYmBhDklM/wzCM2267zYiIiHBqu3jfzMvLM5o2bVrke36xu+66y2V+Fzt69KghyRg9evRl+910001Gt27drrjM4srIyDC8vLyM6Ohoo6CgwGz/61//akgyYmJizLZXX33VqFq1qvGf//zHaR4vvfSS4enpaRw6dMgwjP/f1j4+Psb//M//mP22bt1qSDKGDx9utrVs2dKoWbOmcezYMbNtx44dhoeHhzFgwACzrfC7IzU11TAMw1i2bJkhyfjuu+/KbH3j4uIMScY333xjtp04ccKoX7++Ua9ePZf97kIpKSmGJOPxxx93an/xxRcNScb69esNwzCM7Oxsw263Gy+88IJTvwkTJhg2m804ePCgYRiGceDAAcPT09N4/fXXnfrt3LnTqFSpklN74Xfhxd85xd1mxd3XJRl2u918TwzDMObMmWNIMkJDQ83vJsMwjISEBKf378I6J02aZLbl5uaa+0ReXp5hGEV/dkePHm3+v3FhPV5eXsbPP/9stu3YscOQZEyfPt1su++++4wqVaoYv/76q9m2f/9+o1KlSi7zvF5wyuw6NnPmTK1Zs8bpIUk+Pj5mn3PnzunYsWNq2LChAgICtH379ivO19fXV48++qj53MvLS61bt9Z///tfs83T01NeXl6S/rh8/vjx4zp//rxatWpVrGUcO3ZM1atXd2k/c+ZMkQM+vb29zekXKpzHxUc9LvbFF18oNDRUjzzyiNlWuXJlPffcczp58qQ2bdp0xZqvlr+/v+Li4vTZZ5/phx9+uGS/C4/YFR7Bad++vU6fPq29e/dK+uMQfUZGhp555hlzW0hSdHS0GjduXOzTD4mJiS77TGhoqOx2uzw8/vg6yM/P17Fjx+Tr66ubb765WO+nJD399NNOz9u3b++0z0jO+2ZmZqays7PVvn37a9pnSqJ69epX3Geuxtq1a5WXl6dnn33W6XRBXFycS98lS5aoffv2Zg2Fj8jISOXn5+vrr7926t+rVy9zjJn0x9HSNm3a6IsvvpAk/fbbb0pJSdHAgQMVGBho9mvevLnuueces19RCo+GrFixQufOnSuT9f3iiy/UunVrpwsifH199eSTT+rAgQPas2fPJZdTWHt8fLxT+wsvvCBJ5n5feIp38eLFMgzD7Ldo0SK1bdtWderUkSR9+umnKigo0EMPPeS07UNDQ9WoUSOXU/52u12DBg1yaivuNruaff3uu+92OorXpk0bSVLv3r1VrVo1l/aLP1eVKlXSU089ZT738vLSU089pYyMjCKvdr2SyMhINWjQwHzevHlz+fn5mcvNz8/X2rVr1atXL6ejxw0bNjTPIlyPCETXsdatWysyMtLpIf0RGhITE83xCTVq1FBwcLCysrKUnZ19xfnWrl3b5Rxw9erVlZmZ6dS2YMECNW/eXN7e3goKClJwcLBWrlxZrGVIcvriKuTj4+M0NqBQ4aX1F37JXDiPK52zPnjwoBo1amT+p1/olltuMaeXheeff14BAQGXHUu0e/duPfDAA/L395efn5+Cg4PNQFq4LQvru/nmm11e37hx42LX36xZM5d9xtvbWwUFBZoyZYoaNWrktM/8+OOPxXo/vb29XQ6VF7XPrFixQm3btpW3t7cCAwMVHByst99++5r2mZIwDOOK+8zx48eVlpZmPi5XY+H2b9SokVN7cHCwS4jbv3+/Vq1apeDgYKdH4ec3IyPDqf/F85Skm266yRxLcrl945ZbbtHvv/+uU6dOFVl3x44d1bt3b40dO1Y1atTQ/fffr3nz5hX5GSzp+h48ePCStV04r0stx8PDw+WqwtDQUAUEBDi99uGHH9bhw4eVlJQkSfrll1+UnJyshx9+2Oyzf/9+GYahRo0auWz/n376yWXb33DDDeYPv0LF3WZXs68XBrZChVfeXjg84ML2iz9XYWFhqlq1qlPbTTfdJEkluufUxfVIzp/njIwMnTlzxuV9kVRk2/WCMUR/Qs8++6zmzZunuLg4ORwO+fv7m/ebuXDQ5qVc6iqUC/8z+vDDDzVw4ED16tVLI0aMUM2aNeXp6anx48frl19+ueIygoKCXD7UklSrVq0iBwn/9ttvkuQylqVwHjVq1LjiMovjUv9JFjXAuTgKjxKNGTOmyKNEWVlZ6tixo/z8/DRu3Dg1aNBA3t7e2r59u0aNGlWs96s0/O1vf9Mrr7yiwYMH69VXX1VgYKA8PDwUFxd3TfvMhb755hv17NlTHTp00KxZs1SrVi1VrlxZ8+bN08KFC6/4+kvtMyWRmZlZZNC40IMPPuh05DAmJqbYg+ovp6CgQPfcc49GjhxZ5PTC/8jKg81m09KlS/Xtt9/q888/1+rVqzV48GBNmjRJ3377bYW5Cq84g3Tvu+8+ValSRYsXL1a7du20ePFieXh4qG/fvmafgoIC2Ww2ffnll0Xusxev78U/wAprudI2u9p9/VKfn+J8F5cFdy3X3QhEf0JLly5VTEyM0yXtZ8+eLdU7Hy9dulQ33nijPv30U6cvq9GjRxfr9Y0bN1ZqaqpLe8uWLbVhwwbl5OQ4DazeunWrOf1ChfMo/LV5KXXr1tWPP/6ogoICp6NEhaekCgfqFv66vXhbXcsRpLi4OE2dOlVjx451Gfi8ceNGHTt2TJ9++qnTFXcXb5vC+vbt26cuXbo4Tdu3b981DzReunSpOnfurHfffdepPSsrq9TC5ieffCJvb2+tXr3a6bTovHnzivX6xo0b65NPPrnmOs6fP6/Dhw+rZ8+el+03adIkpwB2uYHlhdt///79Tlc7HT161CXENWjQQCdPnizyarmi7N+/36XtP//5j3mK5cJ942J79+5VjRo1XI4eXKxt27Zq27atXn/9dS1cuFD9+/fXxx9/rMcff7zI/lezvnXr1r1kbRfO61LLKSgo0P79+50+4+np6crKynJ6bdWqVdWjRw8tWbJEkydP1qJFi9S+fXun961BgwYyDEP169e/5uB5uW12rfv61Tpy5IhOnTrl9D7/5z//kVQ29xmrWbOmvL299fPPP7tMK6rtesEpsz8hT09PlyQ/ffr0Eh/luNQyJOdfDFu3bjUPV1+Jw+HQrl27XA4z9+nTR/n5+XrnnXfMttzcXM2bN09t2rRxOYScnJwsm80mh8Nx2eV1795daWlpWrRokdl2/vx5TZ8+Xb6+vurYsaOkP76APT09XcZxXMufCCk8SvSvf/1LKSkpTtOK2o55eXkuy2vVqpVq1qyp2bNnO22zL7/8Uj/99NMVr9C6kqL2mSVLlpTKJf0XLsNmsznthwcOHNDy5cuL9XqHw6HMzEyX8RNXa8+ePTp79qzatWt32X4RERFOpxabNGlyyb6RkZGqXLmypk+f7rQdL75iTPrjPkhJSUlF3kAyKytL58+fd2pbvny50/uwbds2bd261RyrUatWLbVs2VILFixwCvK7du3SV199pe7du1+y7szMTJf3vfBHx+VOm13N+nbv3l3btm1z+m44deqU3nnnHdWrV++y27Ww9ovnO3nyZEly2e8ffvhhHTlyRHPnztWOHTucTpdJfxz18/T01NixY13W2zAMHTt27JK1FCrONrvWff1qnT9/XnPmzDGf5+Xlac6cOQoODlZERESpL8/T01ORkZFavny5jhw5Yrb//PPP+vLLL0t9eeWFI0R/Qj169NAHH3wgf39/NWnSRElJSVq7dq15uWRpLePTTz/VAw88oOjoaKWmpmr27Nlq0qSJTp48ecXX33///Xr11Ve1adMmde3a1Wxv06aN+vbtq4SEBGVkZKhhw4ZasGCBDhw44HL0QpLWrFmjO++884rr9uSTT2rOnDkaOHCgkpOTVa9ePS1dulSbN2/W1KlTzYGL/v7+6tu3r6ZPny6bzaYGDRpoxYoVLmMLrtbzzz+vKVOmaMeOHU6/4tq1a6fq1asrJiZGzz33nGw2mz744AOXL9zKlSvrjTfe0KBBg9SxY0c98sgj5mX39erV0/Dhw6+pvh49emjcuHEaNGiQ2rVrp507d+qjjz665L1dSiI6OlqTJ0/Wvffeq7/85S/KyMjQzJkz1bBhQ/3444/Fen2lSpW0du1ap/tUSdIHH3yggwcP6vTp05Kkr7/+Wq+99pqkP+7OfOGRhDVr1qhKlSq65557Sm3dCu+7VHi5dPfu3fXDDz/oyy+/dDnCNmLECH322Wfq0aOHBg4cqIiICJ06dUo7d+7U0qVLdeDAAafXNGzYUHfddZeGDh2q3NxcTZ06VUFBQU6n3CZOnKhu3brJ4XBoyJAh5mX3/v7+lx2/tmDBAs2aNUsPPPCAGjRooBMnTugf//iH/Pz8LhukrmZ9X3rpJf3zn/9Ut27d9NxzzykwMFALFixQamqqPvnkE5dxfRdq0aKFYmJi9M4775inl7dt26YFCxaoV69e6ty5s1P/7t27m38T0NPTU71793aa3qBBA7322mtKSEjQgQMH1KtXL1WrVk2pqalatmyZnnzySfOGsNeyza51X79aYWFheuONN3TgwAHddNNNWrRokVJSUvTOO++43NqktIwZM0ZfffWV7rzzTg0dOlT5+fmaMWOGmjZt6vLD77pRrte0oVQUXjp7qcs+MzMzjUGDBhk1atQwfH19jaioKGPv3r1G3bp1nS6HvdRl97feeqvLPGNiYoy6deuazwsKCoy//e1vRt26dQ273W7cdtttxooVK1z6XU7z5s2NIUOGuLSfOXPGePHFF43Q0FDDbrcbd9xxh7Fq1SqXfllZWYaXl5cxd+7cYi0vPT3d3C5eXl5Gs2bNiryM/ujRo0bv3r2NKlWqGNWrVzeeeuopY9euXVd92f3FCi9xvfiy+82bNxtt27Y1fHx8jLCwMGPkyJHG6tWri7z8f9GiRcZtt91m2O12IzAw0Ojfv7/TJdklqcswDOPs2bPGCy+8YNSqVcvw8fEx7rzzTiMpKcno2LGj0bFjR7PfpS67v3idLlzfC7377rtGo0aNDLvdbjRu3NiYN29ekf0upWfPnsbdd9/t0l546XFRj4u3YZs2bYxHH320WMu7Gvn5+cbYsWPNbdipUydj165dLp87w/jjsvOEhASjYcOGhpeXl1GjRg2jXbt2xptvvulymfTEiRONSZMmGeHh4Ybdbjfat29v7Nixw2X5a9euNe68807Dx8fH8PPzM+677z5jz549Tn0uvux++/btxiOPPGLUqVPHsNvtRs2aNY0ePXoY33//famu7y+//GL06dPHCAgIMLy9vY3WrVsbK1asKNZ2PXfunDF27Fijfv36RuXKlY3w8HAjISHBvCXFxfr3729IMiIjIy85z08++cS46667jKpVqxpVq1Y1GjdubMTGxhr79u0z+1zqu7C426y4+7okIzY21qntwvf+QkV9jgvr/P777w2Hw2F4e3sbdevWNWbMmFHkPItz2f3F9RiGUeT7um7dOuO2224zvLy8jAYNGhhz5841XnjhBcPb29vl9dcDm2H8yUdJocL64IMPFBsbq0OHDpXoD5lOnTpVEyZM0C+//FLk4Ef8+XzzzTfq1KmT9u7de8VB0UVJSUnR7bffru3bt1/x5qHuduDAAdWvX9/pz9gAF+vUqZN+//33Iu9e7g69evXS7t27ixz7VtExhghu079/f9WpU0czZ8686teeO3dOkydP1ssvv0wYspD27dura9euLn9Kprj+/ve/q0+fPhU+DAHXg4vvC7d//3598cUX6tSpk3sKukaMIYLbeHh4lPhXTeXKlXXo0KFSrgjXg2sZtPnxxx+XYiWAtd14440aOHCgbrzxRh08eFBvv/22vLy8LnlLiYqOQAQAAK7avffeq3/+859KS0uT3W6Xw+HQ3/72txKdzq4IGEMEAAAsjzFEAADA8ghEAADA8hhDVAwFBQU6cuSIqlWrVqy/qQMAANzPMAydOHFCYWFhl70JqEQgKpYjR464/MkIAABwfTh8+LBq16592T4EomIo/LMOhw8fdvqDowAAoOLKyclReHi4+f/45RCIiqHwNJmfnx+BCACA60xxhrswqBoAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFheJXcXgP8XMeJ9d5cAVEjJEwe4u4RrdmhcM3eXAFRIdRJ3ursESRwhAgAAIBABAAAQiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOURiAAAgOVVmED097//XTabTXFxcWbb2bNnFRsbq6CgIPn6+qp3795KT093et2hQ4cUHR2tKlWqqGbNmhoxYoTOnz/v1Gfjxo26/fbbZbfb1bBhQ82fP78c1ggAAFwvKkQg+u677zRnzhw1b97cqX348OH6/PPPtWTJEm3atElHjhzRgw8+aE7Pz89XdHS08vLytGXLFi1YsEDz589XYmKi2Sc1NVXR0dHq3LmzUlJSFBcXp8cff1yrV68ut/UDAAAVm9sD0cmTJ9W/f3/94x//UPXq1c327Oxsvfvuu5o8ebK6dOmiiIgIzZs3T1u2bNG3334rSfrqq6+0Z88effjhh2rZsqW6deumV199VTNnzlReXp4kafbs2apfv74mTZqkW265RcOGDVOfPn00ZcqUS9aUm5urnJwcpwcAAPjzcnsgio2NVXR0tCIjI53ak5OTde7cOaf2xo0bq06dOkpKSpIkJSUlqVmzZgoJCTH7REVFKScnR7t37zb7XDzvqKgocx5FGT9+vPz9/c1HeHj4Na8nAACouNwaiD7++GNt375d48ePd5mWlpYmLy8vBQQEOLWHhIQoLS3N7HNhGCqcXjjtcn1ycnJ05syZIutKSEhQdna2+Th8+HCJ1g8AAFwfKrlrwYcPH9bzzz+vNWvWyNvb211lFMlut8tut7u7DAAAUE7cdoQoOTlZGRkZuv3221WpUiVVqlRJmzZt0ltvvaVKlSopJCREeXl5ysrKcnpdenq6QkNDJUmhoaEuV50VPr9SHz8/P/n4+JTR2gEAgOuJ2wLR3XffrZ07dyolJcV8tGrVSv379zf/XblyZa1bt858zb59+3To0CE5HA5JksPh0M6dO5WRkWH2WbNmjfz8/NSkSROzz4XzKOxTOA8AAAC3nTKrVq2amjZt6tRWtWpVBQUFme1DhgxRfHy8AgMD5efnp2effVYOh0Nt27aVJHXt2lVNmjTRY489pgkTJigtLU0vv/yyYmNjzVNeTz/9tGbMmKGRI0dq8ODBWr9+vRYvXqyVK1eW7woDAIAKy22BqDimTJkiDw8P9e7dW7m5uYqKitKsWbPM6Z6enlqxYoWGDh0qh8OhqlWrKiYmRuPGjTP71K9fXytXrtTw4cM1bdo01a5dW3PnzlVUVJQ7VgkAAFRANsMwDHcXUdHl5OTI399f2dnZ8vPzK7PlRIx4v8zmDVzPkicOcHcJ1+zQuGbuLgGokOok7iyzeV/N/99uvw8RAACAuxGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5RGIAACA5bk1EL399ttq3ry5/Pz85OfnJ4fDoS+//NKcfvbsWcXGxiooKEi+vr7q3bu30tPTneZx6NAhRUdHq0qVKqpZs6ZGjBih8+fPO/XZuHGjbr/9dtntdjVs2FDz588vj9UDAADXCbcGotq1a+vvf/+7kpOT9f3336tLly66//77tXv3bknS8OHD9fnnn2vJkiXatGmTjhw5ogcffNB8fX5+vqKjo5WXl6ctW7ZowYIFmj9/vhITE80+qampio6OVufOnZWSkqK4uDg9/vjjWr16dbmvLwAAqJhshmEY7i7iQoGBgZo4caL69Omj4OBgLVy4UH369JEk7d27V7fccouSkpLUtm1bffnll+rRo4eOHDmikJAQSdLs2bM1atQoHT16VF5eXho1apRWrlypXbt2mcvo16+fsrKytGrVqiJryM3NVW5urvk8JydH4eHhys7Olp+fX5mte8SI98ts3sD1LHniAHeXcM0OjWvm7hKACqlO4s4ym3dOTo78/f2L9f93hRlDlJ+fr48//linTp2Sw+FQcnKyzp07p8jISLNP48aNVadOHSUlJUmSkpKS1KxZMzMMSVJUVJRycnLMo0xJSUlO8yjsUziPoowfP17+/v7mIzw8vDRXFQAAVDBuD0Q7d+6Ur6+v7Ha7nn76aS1btkxNmjRRWlqavLy8FBAQ4NQ/JCREaWlpkqS0tDSnMFQ4vXDa5frk5OTozJkzRdaUkJCg7Oxs83H48OHSWFUAAFBBVXJ3ATfffLNSUlKUnZ2tpUuXKiYmRps2bXJrTXa7XXa73a01AACA8uP2QOTl5aWGDRtKkiIiIvTdd99p2rRpevjhh5WXl6esrCyno0Tp6ekKDQ2VJIWGhmrbtm1O8yu8Cu3CPhdfmZaeni4/Pz/5+PiU1WoBAIDriNtPmV2soKBAubm5ioiIUOXKlbVu3Tpz2r59+3To0CE5HA5JksPh0M6dO5WRkWH2WbNmjfz8/NSkSROzz4XzKOxTOA8AAAC3HiFKSEhQt27dVKdOHZ04cUILFy7Uxo0btXr1avn7+2vIkCGKj49XYGCg/Pz89Oyzz8rhcKht27aSpK5du6pJkyZ67LHHNGHCBKWlpenll19WbGysecrr6aef1owZMzRy5EgNHjxY69ev1+LFi7Vy5Up3rjoAAKhA3BqIMjIyNGDAAP3222/y9/dX8+bNtXr1at1zzz2SpClTpsjDw0O9e/dWbm6uoqKiNGvWLPP1np6eWrFihYYOHSqHw6GqVasqJiZG48aNM/vUr19fK1eu1PDhwzVt2jTVrl1bc+fOVVRUVLmvLwAAqJgq3H2IKqKruY/BteA+REDRuA8R8OfFfYgAAAAqCAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwPAIRAACwvBIFoi5duigrK8ulPScnR126dLnWmgAAAMpViQLRxo0blZeX59J+9uxZffPNN9dcFAAAQHmqdDWdf/zxR/Pfe/bsUVpamvk8Pz9fq1at0g033FB61QEAAJSDqwpELVu2lM1mk81mK/LUmI+Pj6ZPn15qxQEAAJSHqwpEqampMgxDN954o7Zt26bg4GBzmpeXl2rWrClPT89SLxIAAKAsXVUgqlu3riSpoKCgTIoBAABwh6sKRBfav3+/NmzYoIyMDJeAlJiYeM2FAQAAlJcSBaJ//OMfGjp0qGrUqKHQ0FDZbDZzms1mIxABAIDrSokC0WuvvabXX39do0aNKu16AAAAyl2J7kOUmZmpvn37lnYtAAAAblGiQNS3b1999dVXpV0LAACAW5TolFnDhg31yiuv6Ntvv1WzZs1UuXJlp+nPPfdcqRQHAABQHkoUiN555x35+vpq06ZN2rRpk9M0m81GIAIAANeVEgWi1NTU0q4DAADAbUo0hggAAODPpERHiAYPHnzZ6e+9916JigEAAHCHEgWizMxMp+fnzp3Trl27lJWVVeQffQUAAKjIShSIli1b5tJWUFCgoUOHqkGDBtdcFAAAQHkqtTFEHh4eio+P15QpU0prlgAAAOWiVAdV//LLLzp//nxpzhIAAKDMleiUWXx8vNNzwzD022+/aeXKlYqJiSmVwgAAAMpLiQLRDz/84PTcw8NDwcHBmjRp0hWvQAMAAKhoShSINmzYUNp1AAAAuE2JAlGho0ePat++fZKkm2++WcHBwaVSFAAAQHkq0aDqU6dOafDgwapVq5Y6dOigDh06KCwsTEOGDNHp06dLu0YAAIAyVaJAFB8fr02bNunzzz9XVlaWsrKy9K9//UubNm3SCy+8UNo1AgAAlKkSnTL75JNPtHTpUnXq1Mls6969u3x8fPTQQw/p7bffLq36AAAAylyJjhCdPn1aISEhLu01a9bklBkAALjulCgQORwOjR49WmfPnjXbzpw5o7Fjx8rhcJRacQAAAOWhRKfMpk6dqnvvvVe1a9dWixYtJEk7duyQ3W7XV199VaoFAgAAlLUSBaJmzZpp//79+uijj7R3715J0iOPPKL+/fvLx8enVAsEAAAoayUKROPHj1dISIieeOIJp/b33ntPR48e1ahRo0qlOAAAgPJQojFEc+bMUePGjV3ab731Vs2ePfuaiwIAAChPJQpEaWlpqlWrlkt7cHCwfvvtt2suCgAAoDyVKBCFh4dr8+bNLu2bN29WWFjYNRcFAABQnko0huiJJ55QXFyczp07py5dukiS1q1bp5EjR3KnagAAcN0pUSAaMWKEjh07pmeeeUZ5eXmSJG9vb40aNUoJCQmlWiAAAEBZK1EgstlseuONN/TKK6/op59+ko+Pjxo1aiS73V7a9QEAAJS5EgWiQr6+vrrjjjtKqxYAAAC3KNGgagAAgD8TAhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8AhEAALA8twai8ePH64477lC1atVUs2ZN9erVS/v27XPqc/bsWcXGxiooKEi+vr7q3bu30tPTnfocOnRI0dHRqlKlimrWrKkRI0bo/PnzTn02btyo22+/XXa7XQ0bNtT8+fPLevUAAMB1wq2BaNOmTYqNjdW3336rNWvW6Ny5c+ratatOnTpl9hk+fLg+//xzLVmyRJs2bdKRI0f04IMPmtPz8/MVHR2tvLw8bdmyRQsWLND8+fOVmJho9klNTVV0dLQ6d+6slJQUxcXF6fHHH9fq1avLdX0BAEDFZDMMw3B3EYWOHj2qmjVratOmTerQoYOys7MVHByshQsXqk+fPpKkvXv36pZbblFSUpLatm2rL7/8Uj169NCRI0cUEhIiSZo9e7ZGjRqlo0ePysvLS6NGjdLKlSu1a9cuc1n9+vVTVlaWVq1a5VJHbm6ucnNzzec5OTkKDw9Xdna2/Pz8ymz9I0a8X2bzBq5nyRMHuLuEa3ZoXDN3lwBUSHUSd5bZvHNycuTv71+s/78r1Bii7OxsSVJgYKAkKTk5WefOnVNkZKTZp3HjxqpTp46SkpIkSUlJSWrWrJkZhiQpKipKOTk52r17t9nnwnkU9imcx8XGjx8vf39/8xEeHl56KwkAACqcChOICgoKFBcXpzvvvFNNmzaVJKWlpcnLy0sBAQFOfUNCQpSWlmb2uTAMFU4vnHa5Pjk5OTpz5oxLLQkJCcrOzjYfhw8fLpV1BAAAFVMldxdQKDY2Vrt27dK///1vd5ciu90uu93u7jIAAEA5qRBHiIYNG6YVK1Zow4YNql27ttkeGhqqvLw8ZWVlOfVPT09XaGio2efiq84Kn1+pj5+fn3x8fEp7dQAAwHXGrYHIMAwNGzZMy5Yt0/r161W/fn2n6REREapcubLWrVtntu3bt0+HDh2Sw+GQJDkcDu3cuVMZGRlmnzVr1sjPz09NmjQx+1w4j8I+hfMAAADW5tZTZrGxsVq4cKH+9a9/qVq1auaYH39/f/n4+Mjf319DhgxRfHy8AgMD5efnp2effVYOh0Nt27aVJHXt2lVNmjTRY489pgkTJigtLU0vv/yyYmNjzdNeTz/9tGbMmKGRI0dq8ODBWr9+vRYvXqyVK1e6bd0BAEDF4dYjRG+//bays7PVqVMn1apVy3wsWrTI7DNlyhT16NFDvXv3VocOHRQaGqpPP/3UnO7p6akVK1bI09NTDodDjz76qAYMGKBx48aZferXr6+VK1dqzZo1atGihSZNmqS5c+cqKiqqXNcXAABUTBXqPkQV1dXcx+BacB8ioGjchwj48+I+RAAAABUEgQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFgegQgAAFieWwPR119/rfvuu09hYWGy2Wxavny503TDMJSYmKhatWrJx8dHkZGR2r9/v1Of48ePq3///vLz81NAQICGDBmikydPOvX58ccf1b59e3l7eys8PFwTJkwo61UDAADXEbcGolOnTqlFixaaOXNmkdMnTJigt956S7Nnz9bWrVtVtWpVRUVF6ezZs2af/v37a/fu3VqzZo1WrFihr7/+Wk8++aQ5PScnR127dlXdunWVnJysiRMnasyYMXrnnXfKfP0AAMD1oZI7F96tWzd169atyGmGYWjq1Kl6+eWXdf/990uS3n//fYWEhGj58uXq16+ffvrpJ61atUrfffedWrVqJUmaPn26unfvrjfffFNhYWH66KOPlJeXp/fee09eXl669dZblZKSosmTJzsFJwAAYF0VdgxRamqq0tLSFBkZabb5+/urTZs2SkpKkiQlJSUpICDADEOSFBkZKQ8PD23dutXs06FDB3l5eZl9oqKitG/fPmVmZha57NzcXOXk5Dg9AADAn1eFDURpaWmSpJCQEKf2kJAQc1paWppq1qzpNL1SpUoKDAx06lPUPC5cxsXGjx8vf39/8xEeHn7tKwQAACqsChuI3CkhIUHZ2dnm4/Dhw+4uCQAAlKEKG4hCQ0MlSenp6U7t6enp5rTQ0FBlZGQ4TT9//ryOHz/u1KeoeVy4jIvZ7Xb5+fk5PQAAwJ9XhQ1E9evXV2hoqNatW2e25eTkaOvWrXI4HJIkh8OhrKwsJScnm33Wr1+vgoICtWnTxuzz9ddf69y5c2afNWvW6Oabb1b16tXLaW0AAEBF5tZAdPLkSaWkpCglJUXSHwOpU1JSdOjQIdlsNsXFxem1117TZ599pp07d2rAgAEKCwtTr169JEm33HKL7r33Xj3xxBPatm2bNm/erGHDhqlfv34KCwuTJP3lL3+Rl5eXhgwZot27d2vRokWaNm2a4uPj3bTWAACgonHrZffff/+9OnfubD4vDCkxMTGaP3++Ro4cqVOnTunJJ59UVlaW7rrrLq1atUre3t7maz766CMNGzZMd999tzw8PNS7d2+99dZb5nR/f3999dVXio2NVUREhGrUqKHExEQuuQcAACabYRiGu4uo6HJycuTv76/s7OwyHU8UMeL9Mps3cD1LnjjA3SVcs0Pjmrm7BKBCqpO4s8zmfTX/f1fYMUQAAADlhUAEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsj0AEAAAsz1KBaObMmapXr568vb3Vpk0bbdu2zd0lAQCACsAygWjRokWKj4/X6NGjtX37drVo0UJRUVHKyMhwd2kAAMDNLBOIJk+erCeeeEKDBg1SkyZNNHv2bFWpUkXvvfeeu0sDAABuVsndBZSHvLw8JScnKyEhwWzz8PBQZGSkkpKSXPrn5uYqNzfXfJ6dnS1JysnJKdM683PPlOn8getVWX/2ysOJs/nuLgGokMry8104b8MwrtjXEoHo999/V35+vkJCQpzaQ0JCtHfvXpf+48eP19ixY13aw8PDy6xGAJfmP/1pd5cAoKyM9y/zRZw4cUL+/pdfjiUC0dVKSEhQfHy8+bygoEDHjx9XUFCQbDabGytDecjJyVF4eLgOHz4sPz8/d5cDoBTx+bYWwzB04sQJhYWFXbGvJQJRjRo15OnpqfT0dKf29PR0hYaGuvS32+2y2+1ObQEBAWVZIiogPz8/vjCBPyk+39ZxpSNDhSwxqNrLy0sRERFat26d2VZQUKB169bJ4XC4sTIAAFARWOIIkSTFx8crJiZGrVq1UuvWrTV16lSdOnVKgwYNcndpAADAzSwTiB5++GEdPXpUiYmJSktLU8uWLbVq1SqXgdaA3W7X6NGjXU6bArj+8fnGpdiM4lyLBgAA8CdmiTFEAAAAl0MgAgAAlkcgAgAAlkcgAgAAlkcgAgAAlkcgAi4yc+ZM1atXT97e3mrTpo22bdvm7pIAlIKvv/5a9913n8LCwmSz2bR8+XJ3l4QKhEAEXGDRokWKj4/X6NGjtX37drVo0UJRUVHKyMhwd2kArtGpU6fUokULzZw5092loALiPkTABdq0aaM77rhDM2bMkPTHn3gJDw/Xs88+q5deesnN1QEoLTabTcuWLVOvXr3cXQoqCI4QAf8nLy9PycnJioyMNNs8PDwUGRmppKQkN1YGAChrBCLg//z+++/Kz893+XMuISEhSktLc1NVAIDyQCACAACWRyAC/k+NGjXk6emp9PR0p/b09HSFhoa6qSoAQHkgEAH/x8vLSxEREVq3bp3ZVlBQoHXr1snhcLixMgBAWavk7gKAiiQ+Pl4xMTFq1aqVWrduralTp+rUqVMaNGiQu0sDcI1Onjypn3/+2XyempqqlJQUBQYGqk6dOm6sDBUBl90DF5kxY4YmTpyotLQ0tWzZUm+99ZbatGnj7rIAXKONGzeqc+fOLu0xMTGaP39++ReECoVABAAALI8xRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPIIRAAAwPL+FxPZnJwTLCsJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=y_res)\n",
    "plt.title('Falha (0) ou Não Falha (1) - depois do oversampling');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a48498",
   "metadata": {},
   "source": [
    "# Criação do Modelo Preditivo - Classificação Binária\n",
    "\n",
    "Após toda a etapa de Análise Exploratória e Pré-Processamento de Dados, podemos finalmente começar a criar e avaliar os modelos preditivos para esse problema. Como o problema é para prever uma classe, um tipo de falha, então é um problema de classificação. Mais especificamente, classificação multiclasse, visto que a variável target possui mais de 2 classes possíveis. No entanto, como mencionado anteriormente, primeiro transformaremos essa classificação multiclasse em classificação binária, e depois faremos a mudança para classificação multiclasse apenas daquelas máquinas em que uma falha for prevista.\n",
    "\n",
    "Entre os algoritmos mais comuns para esse tipo de problema estão:\n",
    "Regressão Logística, KNN, Naive Bayes, Decision Tree, SVM (Support Vector Machines) e Redes Neurais.\n",
    "\n",
    "Criaremos alguns modelos e utilizaremos algumas métricas de avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "c1bf1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a função que treina e avalia o modelo\n",
    "def train_and_score_model(algoritmo, X_treino, y_treino, X_teste, y_teste, nome = None, version = '1'):\n",
    "    # Cria o modelo\n",
    "    modelo = algoritmo\n",
    "    \n",
    "    # Treinamento\n",
    "    start = time.time()\n",
    "    modelo.fit(X_treino, y_treino)\n",
    "    end = time.time()\n",
    "    print('Tempo de Treinamento do Modelo:', end - start)\n",
    "    \n",
    "    # Fazendo as previsões para o dataset de teste\n",
    "    previsoes = modelo.predict(X_teste)\n",
    "    \n",
    "    # Avaliação do modelo\n",
    "    # Matriz de confusão\n",
    "    print('\\nMatriz de confusão\\n', confusion_matrix(y_teste, previsoes))\n",
    "    \n",
    "    # Dicionário de métricas e metadados\n",
    "    dict_model =   {'Modelo': nome,\n",
    "                    'Versão': version,\n",
    "                    'Precision':precision_score(y_teste, previsoes),\n",
    "                    'Recall':recall_score(y_teste, previsoes),\n",
    "                    'F1 Score':f1_score(y_teste, previsoes),\n",
    "                    'Acurácia':accuracy_score(y_teste, previsoes)}\n",
    "        \n",
    "    return modelo, dict_model, previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d24a3d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.012994527816772461\n",
      "\n",
      "Matriz de confusão\n",
      " [[   6   52]\n",
      " [   6 1603]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'KNN',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9685800604229607,\n",
       " 'Recall': 0.9962709757613425,\n",
       " 'F1 Score': 0.9822303921568628,\n",
       " 'Acurácia': 0.9652069586082783}"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 1 - KNN\n",
    "modelo1, dict1, previsoes1 = train_and_score_model(KNeighborsClassifier(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'KNN', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "5b85033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.005000114440917969\n",
      "\n",
      "Matriz de confusão\n",
      " [[  19   39]\n",
      " [  14 1595]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Naive Bayes',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9761321909424725,\n",
       " 'Recall': 0.9912989434431324,\n",
       " 'F1 Score': 0.9836571076164047,\n",
       " 'Acurácia': 0.9682063587282543}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 2 - Naive Bayes\n",
    "modelo2, dict2, previsoes2 = train_and_score_model(GaussianNB(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'Naive Bayes', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "06659399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.013995885848999023\n",
      "\n",
      "Matriz de confusão\n",
      " [[  29   29]\n",
      " [  13 1596]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Decision Tree Classifier',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9821538461538462,\n",
       " 'Recall': 0.9919204474829086,\n",
       " 'F1 Score': 0.987012987012987,\n",
       " 'Acurácia': 0.9748050389922016}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 3 - Decision Tree Classifier\n",
    "modelo3, dict3, previsoes3 = train_and_score_model(DecisionTreeClassifier(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'Decision Tree Classifier', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "218739c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.49699950218200684\n",
      "\n",
      "Matriz de confusão\n",
      " [[  26   32]\n",
      " [   0 1609]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Random Forest Classifier',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9804996953077392,\n",
       " 'Recall': 1.0,\n",
       " 'F1 Score': 0.9901538461538462,\n",
       " 'Acurácia': 0.9808038392321535}"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 4 - Decision Tree Classifier\n",
    "modelo4, dict4, previsoes4 = train_and_score_model(RandomForestClassifier(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'Random Forest Classifier', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "50ab2b5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.08999204635620117\n",
      "\n",
      "Matriz de confusão\n",
      " [[   1   57]\n",
      " [   0 1609]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'SVM Classifier',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9657863145258103,\n",
       " 'Recall': 1.0,\n",
       " 'F1 Score': 0.9825954198473282,\n",
       " 'Acurácia': 0.9658068386322736}"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 5 - SVM Classifier\n",
    "modelo5, dict5, previsoes5 = train_and_score_model(svm.SVC(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'SVM Classifier', \n",
    "                                      version = 'Binary Classification 1')\n",
    "dict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "b980db05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.09099483489990234\n",
      "\n",
      "Matriz de confusão\n",
      " [[   1   57]\n",
      " [   0 1609]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'XGBoost Classifier',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9657863145258103,\n",
       " 'Recall': 1.0,\n",
       " 'F1 Score': 0.9825954198473282,\n",
       " 'Acurácia': 0.9658068386322736}"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 6 - XGBoost Classifier\n",
    "modelo6, dict6, previsoes = train_and_score_model(svm.SVC(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'XGBoost Classifier', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "82afacc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict1</th>\n",
       "      <th>dict2</th>\n",
       "      <th>dict3</th>\n",
       "      <th>dict4</th>\n",
       "      <th>dict5</th>\n",
       "      <th>dict6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>XGBoost Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versão</th>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.96858</td>\n",
       "      <td>0.976132</td>\n",
       "      <td>0.982154</td>\n",
       "      <td>0.9805</td>\n",
       "      <td>0.965786</td>\n",
       "      <td>0.965786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.996271</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.99192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.98223</td>\n",
       "      <td>0.983657</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.990154</td>\n",
       "      <td>0.982595</td>\n",
       "      <td>0.982595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acurácia</th>\n",
       "      <td>0.965207</td>\n",
       "      <td>0.968206</td>\n",
       "      <td>0.974805</td>\n",
       "      <td>0.980804</td>\n",
       "      <td>0.965807</td>\n",
       "      <td>0.965807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dict1                    dict2  \\\n",
       "Modelo                         KNN              Naive Bayes   \n",
       "Versão     Binary Classification 1  Binary Classification 1   \n",
       "Precision                  0.96858                 0.976132   \n",
       "Recall                    0.996271                 0.991299   \n",
       "F1 Score                   0.98223                 0.983657   \n",
       "Acurácia                  0.965207                 0.968206   \n",
       "\n",
       "                              dict3                     dict4  \\\n",
       "Modelo     Decision Tree Classifier  Random Forest Classifier   \n",
       "Versão      Binary Classification 1   Binary Classification 1   \n",
       "Precision                  0.982154                    0.9805   \n",
       "Recall                      0.99192                       1.0   \n",
       "F1 Score                   0.987013                  0.990154   \n",
       "Acurácia                   0.974805                  0.980804   \n",
       "\n",
       "                             dict5                    dict6  \n",
       "Modelo              SVM Classifier       XGBoost Classifier  \n",
       "Versão     Binary Classification 1  Binary Classification 1  \n",
       "Precision                 0.965786                 0.965786  \n",
       "Recall                         1.0                      1.0  \n",
       "F1 Score                  0.982595                 0.982595  \n",
       "Acurácia                  0.965807                 0.965807  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatena todos os dicionários em um dataframe do Pandas\n",
    "resumo_bin1 = pd.DataFrame({'dict1':pd.Series(dict1),\n",
    "                           'dict2':pd.Series(dict2),\n",
    "                           'dict3':pd.Series(dict3),\n",
    "                           'dict4':pd.Series(dict4),\n",
    "                           'dict5':pd.Series(dict5),\n",
    "                           'dict6':pd.Series(dict6)})\n",
    "\n",
    "resumo_bin1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058c484",
   "metadata": {},
   "source": [
    "## Repetindo os modelos com o balanceamento de classes (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "5408f73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.028002500534057617\n",
      "\n",
      "Matriz de confusão\n",
      " [[  36   22]\n",
      " [ 107 1502]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'KNN',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9855643044619422,\n",
       " 'Recall': 0.9334990677439403,\n",
       " 'F1 Score': 0.9588254069581871,\n",
       " 'Acurácia': 0.9226154769046191}"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 1 - KNN - com SMOTE\n",
    "modelo1, dict1, previsoes1 = train_and_score_model(KNeighborsClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'KNN', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "aa628f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.006005525588989258\n",
      "\n",
      "Matriz de confusão\n",
      " [[  40   18]\n",
      " [ 481 1128]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Naive Bayes',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9842931937172775,\n",
       " 'Recall': 0.7010565568676196,\n",
       " 'F1 Score': 0.8188747731397459,\n",
       " 'Acurácia': 0.7006598680263947}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 2 - Naive Bayes - com SMOTE\n",
    "modelo2, dict2, previsoes2 = train_and_score_model(GaussianNB(), X_res, y_res, X_val, y_val_bin, nome = 'Naive Bayes', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "763bd13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.04700493812561035\n",
      "\n",
      "Matriz de confusão\n",
      " [[  38   20]\n",
      " [  51 1558]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Decision Tree Classifier',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9873257287705957,\n",
       " 'Recall': 0.9683032939714108,\n",
       " 'F1 Score': 0.9777219956071539,\n",
       " 'Acurácia': 0.9574085182963408}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 3 - Decision Tree - com SMOTE\n",
    "modelo3, dict3, previsoes3 = train_and_score_model(DecisionTreeClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'Decision Tree Classifier', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "8e83ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 1.3679895401000977\n",
      "\n",
      "Matriz de confusão\n",
      " [[  35   23]\n",
      " [  40 1569]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Random Forest Classifier',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9855527638190955,\n",
       " 'Recall': 0.9751398384089497,\n",
       " 'F1 Score': 0.9803186504217434,\n",
       " 'Acurácia': 0.9622075584883023}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 4 - Random Forest - com SMOTE\n",
    "modelo4, dict4, previsoes4 = train_and_score_model(RandomForestClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'Random Forest Classifier', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "1ad66d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 1.1769928932189941\n",
      "\n",
      "Matriz de confusão\n",
      " [[  36   22]\n",
      " [  40 1569]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'SVM Classifier',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9861722187303583,\n",
       " 'Recall': 0.9751398384089497,\n",
       " 'F1 Score': 0.9806250000000001,\n",
       " 'Acurácia': 0.9628074385122976}"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 5 - SVM Classifier - com SMOTE\n",
    "modelo5, dict5, previsoes5 = train_and_score_model(RandomForestClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'SVM Classifier', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "63bec297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.26099276542663574\n",
      "\n",
      "Matriz de confusão\n",
      " [[  36   22]\n",
      " [  30 1579]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'XGBoost Classifier',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9862585883822611,\n",
       " 'Recall': 0.9813548788067122,\n",
       " 'F1 Score': 0.9838006230529595,\n",
       " 'Acurácia': 0.9688062387522496}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 6 - XGBoost Classifier - com SMOTE\n",
    "modelo6, dict6, previsoes6 = train_and_score_model(XGBClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'XGBoost Classifier', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "81e15857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict1</th>\n",
       "      <th>dict2</th>\n",
       "      <th>dict3</th>\n",
       "      <th>dict4</th>\n",
       "      <th>dict5</th>\n",
       "      <th>dict6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>XGBoost Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versão</th>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.985564</td>\n",
       "      <td>0.984293</td>\n",
       "      <td>0.987326</td>\n",
       "      <td>0.985553</td>\n",
       "      <td>0.986172</td>\n",
       "      <td>0.986259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.933499</td>\n",
       "      <td>0.701057</td>\n",
       "      <td>0.968303</td>\n",
       "      <td>0.97514</td>\n",
       "      <td>0.97514</td>\n",
       "      <td>0.981355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.958825</td>\n",
       "      <td>0.818875</td>\n",
       "      <td>0.977722</td>\n",
       "      <td>0.980319</td>\n",
       "      <td>0.980625</td>\n",
       "      <td>0.983801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acurácia</th>\n",
       "      <td>0.922615</td>\n",
       "      <td>0.70066</td>\n",
       "      <td>0.957409</td>\n",
       "      <td>0.962208</td>\n",
       "      <td>0.962807</td>\n",
       "      <td>0.968806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dict1                    dict2  \\\n",
       "Modelo                         KNN              Naive Bayes   \n",
       "Versão     Binary Classification 2  Binary Classification 2   \n",
       "Precision                 0.985564                 0.984293   \n",
       "Recall                    0.933499                 0.701057   \n",
       "F1 Score                  0.958825                 0.818875   \n",
       "Acurácia                  0.922615                  0.70066   \n",
       "\n",
       "                              dict3                     dict4  \\\n",
       "Modelo     Decision Tree Classifier  Random Forest Classifier   \n",
       "Versão      Binary Classification 2   Binary Classification 2   \n",
       "Precision                  0.987326                  0.985553   \n",
       "Recall                     0.968303                   0.97514   \n",
       "F1 Score                   0.977722                  0.980319   \n",
       "Acurácia                   0.957409                  0.962208   \n",
       "\n",
       "                             dict5                    dict6  \n",
       "Modelo              SVM Classifier       XGBoost Classifier  \n",
       "Versão     Binary Classification 2  Binary Classification 2  \n",
       "Precision                 0.986172                 0.986259  \n",
       "Recall                     0.97514                 0.981355  \n",
       "F1 Score                  0.980625                 0.983801  \n",
       "Acurácia                  0.962807                 0.968806  "
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatena todos os dicionários em um dataframe do Pandas\n",
    "resumo_bin2 = pd.DataFrame({'dict1':pd.Series(dict1),\n",
    "                           'dict2':pd.Series(dict2),\n",
    "                           'dict3':pd.Series(dict3),\n",
    "                           'dict4':pd.Series(dict4),\n",
    "                           'dict5':pd.Series(dict5),\n",
    "                           'dict6':pd.Series(dict6)})\n",
    "\n",
    "resumo_bin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f1af1eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict1</th>\n",
       "      <th>dict2</th>\n",
       "      <th>dict3</th>\n",
       "      <th>dict4</th>\n",
       "      <th>dict5</th>\n",
       "      <th>dict6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>XGBoost Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versão</th>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.96858</td>\n",
       "      <td>0.976132</td>\n",
       "      <td>0.982154</td>\n",
       "      <td>0.9805</td>\n",
       "      <td>0.965786</td>\n",
       "      <td>0.965786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.996271</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.99192</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.98223</td>\n",
       "      <td>0.983657</td>\n",
       "      <td>0.987013</td>\n",
       "      <td>0.990154</td>\n",
       "      <td>0.982595</td>\n",
       "      <td>0.982595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acurácia</th>\n",
       "      <td>0.965207</td>\n",
       "      <td>0.968206</td>\n",
       "      <td>0.974805</td>\n",
       "      <td>0.980804</td>\n",
       "      <td>0.965807</td>\n",
       "      <td>0.965807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dict1                    dict2  \\\n",
       "Modelo                         KNN              Naive Bayes   \n",
       "Versão     Binary Classification 1  Binary Classification 1   \n",
       "Precision                  0.96858                 0.976132   \n",
       "Recall                    0.996271                 0.991299   \n",
       "F1 Score                   0.98223                 0.983657   \n",
       "Acurácia                  0.965207                 0.968206   \n",
       "\n",
       "                              dict3                     dict4  \\\n",
       "Modelo     Decision Tree Classifier  Random Forest Classifier   \n",
       "Versão      Binary Classification 1   Binary Classification 1   \n",
       "Precision                  0.982154                    0.9805   \n",
       "Recall                      0.99192                       1.0   \n",
       "F1 Score                   0.987013                  0.990154   \n",
       "Acurácia                   0.974805                  0.980804   \n",
       "\n",
       "                             dict5                    dict6  \n",
       "Modelo              SVM Classifier       XGBoost Classifier  \n",
       "Versão     Binary Classification 1  Binary Classification 1  \n",
       "Precision                 0.965786                 0.965786  \n",
       "Recall                         1.0                      1.0  \n",
       "F1 Score                  0.982595                 0.982595  \n",
       "Acurácia                  0.965807                 0.965807  "
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumo_bin1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ea186",
   "metadata": {},
   "source": [
    "## Avaliando as métricas\n",
    "\n",
    "Para avaliar as métricas é necessário definir: qual a classe positiva e qual a classe negativa do problema?\n",
    "\n",
    "Classe positiva: Máquinas sem falha (1)\n",
    "Classe negativa: Máquinas com falha (0)\n",
    "\n",
    "**Precision**: Essa métrica responde à pergunta: que proporção das previsões positivas são realmente positivas? Para esse problema, significa quantas máquinas que foram previstas como sem falhas realmente não possuem falhas. É necessário otimizar essa métrica se o problema requerer um baixo número de **falsos positivos**. Estes significariam máquinas que têm falhas mas foram previstas como se não tivessem. Como nosso problema é de manutenção preditiva, esse tipo de falha é muito mais grave do que prever falhas nas máquinas que não a possuem, ou falsos negativos.\n",
    "\n",
    "**Recall**: Responde à pergunta: que proporção dos verdadeiros positivos estão corretamente classificados? Para esse problema, significa as máquinas que não possuem falhas e foram classificadas corretamente. É necessário otimizar essa métrica se o problema requerer um baixo número de **falsos negativos**. Estes significariam máquinas que não possuem falhas mas foram previstas como se tivessem. Como nosso problema é de manutenção preditiva, esse tipo de falha é menos grave do que prever falsos positivos.\n",
    "\n",
    "**F1 Score**: É a média harmônica entre precision e recall. Fortemente afetado caso uma das duas métricas seja muito baixa. É necessário otimizar essa métrica quando o problema requerer um bom equilíbrio de falsos positivos e negativos.\n",
    "\n",
    "**Acurácia**: É simplesmente o percentual de acerto do modelo. \n",
    "\n",
    "Considerando nosso problema, Precision será nossa métrica base para a classificação binária. \n",
    "Posteriormente, na avaliação da classificação das falhas, utilizaremos a métrica de ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d7b04",
   "metadata": {},
   "source": [
    "# Criação do Modelo Preditivo - Classificação Multiclasse\n",
    "\n",
    "O melhor modelo encontrado para a classificação binária foi o Decision Tree com aplicação de SMOTE, com Precision de 0.987967. Seguido pelo modelo XGBoost Classifier, também com aplicação de SMOTE e Precision de 0.986259.\n",
    "\n",
    "Agora, criaremos outro modelo usando dados de treinamento apenas nos casos em que há falha, para prever qual o tipo de falha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "f3b4213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando os dados em que há falha - Treino\n",
    "y_falha_treino = y_treino[y_treino != 1]\n",
    "X_falha_treino = X_treino.loc[y_falha_treino.index]\n",
    "\n",
    "# Filtrando os dados em que há falha - Validação\n",
    "y_falha_val = y_val[y_val != 1]\n",
    "X_falha_val = X_val.loc[y_falha_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4129a74a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>1.864115</td>\n",
       "      <td>1.821454</td>\n",
       "      <td>-1.006314</td>\n",
       "      <td>1.673215</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>-0.646829</td>\n",
       "      <td>-0.126553</td>\n",
       "      <td>-1.034447</td>\n",
       "      <td>1.323047</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>1.512583</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>1.613186</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>1.311707</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.006314</td>\n",
       "      <td>1.713234</td>\n",
       "      <td>0.272358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>1.211270</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-1.124471</td>\n",
       "      <td>2.753734</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.899410</td>\n",
       "      <td>1.303037</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>1.361926</td>\n",
       "      <td>1.552764</td>\n",
       "      <td>-1.079459</td>\n",
       "      <td>2.113426</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>-0.194859</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>1.851970</td>\n",
       "      <td>-1.668389</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>-0.193726</td>\n",
       "      <td>-0.854397</td>\n",
       "      <td>2.263499</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>1.462364</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-1.169484</td>\n",
       "      <td>1.363066</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "3330           1.864115               1.821454             -1.006314   \n",
       "950           -0.646829              -0.126553             -1.034447   \n",
       "3082           1.512583               0.881037             -0.972555   \n",
       "3009           1.311707               0.276483             -1.006314   \n",
       "2588           1.211270               0.679519             -1.124471   \n",
       "...                 ...                    ...                   ...   \n",
       "2371           1.010394               0.612346             -0.899410   \n",
       "3658           1.361926               1.552764             -1.079459   \n",
       "1448          -0.194859              -0.529589              1.851970   \n",
       "2917           1.010394              -0.193726             -0.854397   \n",
       "3098           1.462364               0.813864             -1.169484   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "3330   1.673215       0.788618  0.0  1.0  0.0  \n",
       "950    1.323047       0.861789  0.0  1.0  0.0  \n",
       "3082   1.613186       0.593496  0.0  1.0  0.0  \n",
       "3009   1.713234       0.272358  0.0  1.0  0.0  \n",
       "2588   2.753734       0.788618  0.0  1.0  0.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "2371   1.303037       0.861789  0.0  1.0  0.0  \n",
       "3658   2.113426       0.691057  0.0  1.0  0.0  \n",
       "1448  -1.668389       0.914634  0.0  0.0  1.0  \n",
       "2917   2.263499       0.577236  0.0  0.0  1.0  \n",
       "3098   1.363066       0.813008  0.0  0.0  1.0  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_falha_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "2134f5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3330    2\n",
       "950     2\n",
       "3082    0\n",
       "3009    0\n",
       "2588    3\n",
       "       ..\n",
       "2371    2\n",
       "3658    4\n",
       "1448    5\n",
       "2917    3\n",
       "3098    0\n",
       "Length: 150, dtype: int32"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_falha_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "75ade596",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>-0.777961</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>1.663240</td>\n",
       "      <td>1.082555</td>\n",
       "      <td>-1.130098</td>\n",
       "      <td>1.673215</td>\n",
       "      <td>0.760163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>0.257111</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>6.161902</td>\n",
       "      <td>-2.788927</td>\n",
       "      <td>0.069106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4938</th>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>0.867325</td>\n",
       "      <td>-1.108120</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6274</th>\n",
       "      <td>-0.948143</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>-1.034447</td>\n",
       "      <td>1.383076</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.062579</td>\n",
       "      <td>1.503134</td>\n",
       "      <td>0.760163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-1.507076</td>\n",
       "      <td>2.523623</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5720</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>-1.146978</td>\n",
       "      <td>3.193945</td>\n",
       "      <td>0.613821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>1.211270</td>\n",
       "      <td>0.209310</td>\n",
       "      <td>-0.899410</td>\n",
       "      <td>1.423095</td>\n",
       "      <td>0.670732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>1.311707</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>6.403843</td>\n",
       "      <td>-2.928994</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>-1.801864</td>\n",
       "      <td>-1.738698</td>\n",
       "      <td>7.287211</td>\n",
       "      <td>-3.449244</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>-0.395735</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>-1.085086</td>\n",
       "      <td>2.063402</td>\n",
       "      <td>0.776423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6433</th>\n",
       "      <td>-0.445954</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-0.961302</td>\n",
       "      <td>1.623191</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>1.864115</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.158231</td>\n",
       "      <td>1.263018</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>0.434081</td>\n",
       "      <td>-0.467813</td>\n",
       "      <td>0.882114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>-0.345516</td>\n",
       "      <td>-0.731107</td>\n",
       "      <td>-1.574595</td>\n",
       "      <td>2.933820</td>\n",
       "      <td>0.483740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>0.859737</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.747493</td>\n",
       "      <td>0.632716</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>2.093417</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>-0.395735</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>-1.523956</td>\n",
       "      <td>2.813763</td>\n",
       "      <td>0.308943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>0.960175</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.417052</td>\n",
       "      <td>2.273503</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>1.161051</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.966928</td>\n",
       "      <td>0.832812</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>1.361926</td>\n",
       "      <td>1.619936</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>0.622711</td>\n",
       "      <td>1.020325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>1.361926</td>\n",
       "      <td>0.545173</td>\n",
       "      <td>-1.478944</td>\n",
       "      <td>1.493129</td>\n",
       "      <td>0.101626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-0.781252</td>\n",
       "      <td>1.012898</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.478001</td>\n",
       "      <td>-0.978181</td>\n",
       "      <td>0.772783</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-1.495823</td>\n",
       "      <td>1.913330</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>-0.044203</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>-1.405799</td>\n",
       "      <td>2.953830</td>\n",
       "      <td>0.252033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.349533</td>\n",
       "      <td>2.383556</td>\n",
       "      <td>0.239837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-1.107592</td>\n",
       "      <td>0.692745</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>-0.445954</td>\n",
       "      <td>-0.798280</td>\n",
       "      <td>-1.175110</td>\n",
       "      <td>1.353062</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.662730</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.201316</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>1.283028</td>\n",
       "      <td>0.886179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-1.299675</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>7.517899</td>\n",
       "      <td>-3.589311</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.017567</td>\n",
       "      <td>1.793273</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.074965</td>\n",
       "      <td>-1.191990</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>0.313008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-0.860024</td>\n",
       "      <td>1.633196</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>1.110832</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>-1.028820</td>\n",
       "      <td>1.142961</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>-0.781252</td>\n",
       "      <td>0.632716</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-0.905036</td>\n",
       "      <td>0.152485</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>0.257111</td>\n",
       "      <td>-0.059381</td>\n",
       "      <td>-0.792505</td>\n",
       "      <td>0.582692</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.351246</td>\n",
       "      <td>-0.094814</td>\n",
       "      <td>-0.417789</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-1.546462</td>\n",
       "      <td>2.053398</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>-2.153396</td>\n",
       "      <td>-2.544770</td>\n",
       "      <td>4.119467</td>\n",
       "      <td>-2.548811</td>\n",
       "      <td>0.605691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>1.512583</td>\n",
       "      <td>1.485591</td>\n",
       "      <td>-1.580221</td>\n",
       "      <td>3.123912</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>-0.295297</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>-0.516805</td>\n",
       "      <td>2.283508</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>1.512583</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-0.921916</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>0.264228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.619936</td>\n",
       "      <td>-0.713734</td>\n",
       "      <td>1.373071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>1.361926</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-0.516805</td>\n",
       "      <td>0.082452</td>\n",
       "      <td>0.857724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4160</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.074965</td>\n",
       "      <td>-0.747493</td>\n",
       "      <td>2.113426</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4113</th>\n",
       "      <td>0.558424</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-1.439558</td>\n",
       "      <td>2.783748</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>-0.897924</td>\n",
       "      <td>-1.066971</td>\n",
       "      <td>-1.282015</td>\n",
       "      <td>2.093417</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>-1.651207</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-1.574595</td>\n",
       "      <td>2.893801</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>1.613021</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>-1.051326</td>\n",
       "      <td>1.373071</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.545173</td>\n",
       "      <td>0.434081</td>\n",
       "      <td>-1.108120</td>\n",
       "      <td>0.955285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2847</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-1.023194</td>\n",
       "      <td>1.543153</td>\n",
       "      <td>0.410569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>1.864115</td>\n",
       "      <td>1.351246</td>\n",
       "      <td>6.330698</td>\n",
       "      <td>-2.868965</td>\n",
       "      <td>0.288618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>1.311707</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-1.428305</td>\n",
       "      <td>2.793753</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>0.156673</td>\n",
       "      <td>0.209310</td>\n",
       "      <td>-0.989434</td>\n",
       "      <td>2.023383</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2330           0.809518               0.343655              0.360936   \n",
       "3156           1.663240               1.082555             -1.130098   \n",
       "4667           0.257111               0.343655              6.161902   \n",
       "4938           0.006016               0.948209              0.867325   \n",
       "6274          -0.948143              -0.932625             -1.034447   \n",
       "3038           1.261488               0.276483             -1.062579   \n",
       "2473           1.010394               0.813864             -1.507076   \n",
       "5720          -1.249456              -1.268489             -1.146978   \n",
       "2982           1.211270               0.209310             -0.899410   \n",
       "2961           1.311707               0.142137              6.403843   \n",
       "570           -1.801864              -1.738698              7.287211   \n",
       "1726          -0.395735              -0.596762             -1.085086   \n",
       "6433          -0.445954               0.007792             -0.961302   \n",
       "3188           1.864115               1.284073             -1.158231   \n",
       "6005          -1.349894              -1.268489              0.434081   \n",
       "1424          -0.345516              -0.731107             -1.574595   \n",
       "2425           0.859737               0.612346             -0.747493   \n",
       "3164           1.713458               1.216900             -1.304521   \n",
       "1633          -0.395735              -0.932625             -1.523956   \n",
       "2771           0.960175               0.276483             -1.417052   \n",
       "2568           1.161051               0.612346             -0.966928   \n",
       "3611           1.361926               1.619936             -0.342382   \n",
       "3048           1.361926               0.545173             -1.478944   \n",
       "5338           0.357548               1.284073             -0.781252   \n",
       "2763           1.060613               0.478001             -0.978181   \n",
       "5166           0.206892               1.216900             -1.495823   \n",
       "4842          -0.044203               0.142137             -1.405799   \n",
       "3238           1.713458               1.284073             -1.349533   \n",
       "3162           1.713458               1.216900             -1.107592   \n",
       "1665          -0.445954              -0.798280             -1.175110   \n",
       "5238           0.206892               1.284073             -0.561817   \n",
       "1061          -0.998362              -1.201316             -0.972555   \n",
       "320           -1.299675              -0.865453              7.517899   \n",
       "2738           1.010394               0.276483             -1.017567   \n",
       "2901           0.809518               0.074965             -1.191990   \n",
       "730           -1.500550              -1.470007             -0.860024   \n",
       "2796           1.110832               0.410828             -1.028820   \n",
       "817           -1.500550              -1.134143             -0.781252   \n",
       "3235           1.713458               1.284073             -0.905036   \n",
       "4221           0.257111              -0.059381             -0.792505   \n",
       "3226           1.713458               1.351246             -0.094814   \n",
       "2787           1.010394               0.343655             -1.546462   \n",
       "609           -2.153396              -2.544770              4.119467   \n",
       "3749           1.512583               1.485591             -1.580221   \n",
       "1534          -0.295297              -0.865453             -0.516805   \n",
       "3055           1.512583               0.813864             -0.921916   \n",
       "3610           1.412145               1.619936             -0.713734   \n",
       "2994           1.361926               0.343655             -0.516805   \n",
       "4160           0.407767               0.074965             -0.747493   \n",
       "4113           0.558424               0.612346             -1.439558   \n",
       "1053          -0.897924              -1.066971             -1.282015   \n",
       "752           -1.651207              -1.470007             -1.574595   \n",
       "3173           1.613021               0.948209             -1.051326   \n",
       "2702           1.010394               0.545173              0.434081   \n",
       "2847           1.261488               0.612346             -1.023194   \n",
       "3189           1.864115               1.351246              6.330698   \n",
       "2850           1.311707               0.679519             -1.428305   \n",
       "4771           0.156673               0.209310             -0.989434   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2330  -0.777961       0.008130  1.0  0.0  0.0  \n",
       "3156   1.673215       0.760163  0.0  1.0  0.0  \n",
       "4667  -2.788927       0.069106  0.0  1.0  0.0  \n",
       "4938  -1.108120       0.878049  0.0  0.0  1.0  \n",
       "6274   1.383076       0.841463  0.0  1.0  0.0  \n",
       "3038   1.503134       0.760163  0.0  1.0  0.0  \n",
       "2473   2.523623       0.739837  0.0  1.0  0.0  \n",
       "5720   3.193945       0.613821  0.0  0.0  1.0  \n",
       "2982   1.423095       0.670732  0.0  0.0  1.0  \n",
       "2961  -2.928994       0.341463  0.0  0.0  1.0  \n",
       "570   -3.449244       0.865854  0.0  1.0  0.0  \n",
       "1726   2.063402       0.776423  0.0  1.0  0.0  \n",
       "6433   1.623191       0.825203  0.0  1.0  0.0  \n",
       "3188   1.263018       0.268293  1.0  0.0  0.0  \n",
       "6005  -0.467813       0.882114  0.0  1.0  0.0  \n",
       "1424   2.933820       0.483740  0.0  1.0  0.0  \n",
       "2425   0.632716       0.841463  0.0  1.0  0.0  \n",
       "3164   2.093417       0.873984  0.0  1.0  0.0  \n",
       "1633   2.813763       0.308943  0.0  1.0  0.0  \n",
       "2771   2.273503       0.089431  0.0  1.0  0.0  \n",
       "2568   0.832812       0.528455  1.0  0.0  0.0  \n",
       "3611   0.622711       1.020325  0.0  1.0  0.0  \n",
       "3048   1.493129       0.101626  1.0  0.0  0.0  \n",
       "5338   1.012898       0.902439  0.0  1.0  0.0  \n",
       "2763   0.772783       0.865854  0.0  0.0  1.0  \n",
       "5166   1.913330       0.804878  0.0  1.0  0.0  \n",
       "4842   2.953830       0.252033  0.0  1.0  0.0  \n",
       "3238   2.383556       0.239837  0.0  1.0  0.0  \n",
       "3162   0.692745       0.853659  0.0  0.0  1.0  \n",
       "1665   1.353062       0.841463  0.0  1.0  0.0  \n",
       "5238   0.662730       0.166667  1.0  0.0  0.0  \n",
       "1061   1.283028       0.886179  0.0  1.0  0.0  \n",
       "320   -3.589311       0.479675  0.0  1.0  0.0  \n",
       "2738   1.793273       0.439024  0.0  1.0  0.0  \n",
       "2901   0.812802       0.313008  0.0  1.0  0.0  \n",
       "730    1.633196       0.821138  0.0  1.0  0.0  \n",
       "2796   1.142961       0.512195  0.0  0.0  1.0  \n",
       "817    0.632716       0.536585  0.0  0.0  1.0  \n",
       "3235   0.152485       0.138211  0.0  1.0  0.0  \n",
       "4221   0.582692       0.853659  1.0  0.0  0.0  \n",
       "3226  -0.417789       0.873984  0.0  1.0  0.0  \n",
       "2787   2.053398       0.365854  0.0  1.0  0.0  \n",
       "609   -2.548811       0.605691  0.0  1.0  0.0  \n",
       "3749   3.123912       0.495935  0.0  0.0  1.0  \n",
       "1534   2.283508       0.544715  0.0  1.0  0.0  \n",
       "3055   0.812802       0.264228  0.0  1.0  0.0  \n",
       "3610   1.373071       1.000000  1.0  0.0  0.0  \n",
       "2994   0.082452       0.857724  0.0  1.0  0.0  \n",
       "4160   2.113426       0.768293  0.0  1.0  0.0  \n",
       "4113   2.783748       0.040650  0.0  1.0  0.0  \n",
       "1053   2.093417       0.768293  0.0  1.0  0.0  \n",
       "752    2.893801       0.426829  0.0  0.0  1.0  \n",
       "3173   1.373071       0.032520  0.0  1.0  0.0  \n",
       "2702  -1.108120       0.955285  0.0  1.0  0.0  \n",
       "2847   1.543153       0.410569  1.0  0.0  0.0  \n",
       "3189  -2.868965       0.288618  0.0  0.0  1.0  \n",
       "2850   2.793753       0.463415  0.0  1.0  0.0  \n",
       "4771   2.023383       0.837398  0.0  1.0  0.0  "
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_falha_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b7b2f0e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2330    4\n",
       "3156    0\n",
       "4667    3\n",
       "4938    5\n",
       "6274    2\n",
       "3038    0\n",
       "2473    2\n",
       "5720    3\n",
       "2982    0\n",
       "2961    3\n",
       "570     3\n",
       "1726    2\n",
       "6433    2\n",
       "3188    0\n",
       "6005    5\n",
       "1424    3\n",
       "2425    5\n",
       "3164    0\n",
       "1633    3\n",
       "2771    0\n",
       "2568    0\n",
       "3611    2\n",
       "3048    0\n",
       "5338    2\n",
       "2763    0\n",
       "5166    2\n",
       "4842    3\n",
       "3238    0\n",
       "3162    0\n",
       "1665    2\n",
       "5238    4\n",
       "1061    2\n",
       "320     3\n",
       "2738    0\n",
       "2901    0\n",
       "730     2\n",
       "2796    0\n",
       "817     4\n",
       "3235    0\n",
       "4221    5\n",
       "3226    5\n",
       "2787    0\n",
       "609     3\n",
       "3749    3\n",
       "1534    3\n",
       "3055    0\n",
       "3610    2\n",
       "2994    5\n",
       "4160    3\n",
       "4113    3\n",
       "1053    2\n",
       "752     3\n",
       "3173    0\n",
       "2702    5\n",
       "2847    0\n",
       "3189    3\n",
       "2850    3\n",
       "4771    2\n",
       "dtype: int32"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_falha_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7f475379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot das falhas no dataset de treino\n",
    "sns.countplot(x=y_falha_treino)\n",
    "plt.title('Tipo de falha nas máquinas no dataset de treino');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09764c",
   "metadata": {},
   "source": [
    "Nesse caso já possuímos um dataset com classes melhor balanceadas, não precisamos de _oversampling_. No entanto, alguns algoritmos de Machine Learning, como XGBoost, não aceitam receber os labels das _n_ classes fora da ordem 0 até a classe _n_-1. Então faremos um novo encoding, que será desfeito após a previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1e0f4b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_falha_treino_copia = y_falha_treino.copy()\n",
    "y_falha_val_copia = y_falha_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "fb9329d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novo encoding para ficar no intervalo (0,4)\n",
    "y_falha_treino[y_falha_treino > 0] = y_falha_treino - 1\n",
    "y_falha_val[y_falha_val > 0] = y_falha_val - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "cd6717d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot das falhas no dataset de treino - Após novo encoding\n",
    "sns.countplot(x=y_falha_treino)\n",
    "plt.title('Tipo de falha nas máquinas no dataset de treino');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "10277491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a função que treina e avalia o modelo multiclasse\n",
    "def train_and_score_multiclass_model(algoritmo, X_treino, y_treino, X_teste, y_teste, nome = None, version = '1', auc = False):\n",
    "    # Cria o modelo\n",
    "    modelo = algoritmo\n",
    "    \n",
    "    # Treinamento\n",
    "    start = time.time()\n",
    "    modelo.fit(X_treino, y_treino)\n",
    "    end = time.time()\n",
    "    print('Tempo de Treinamento do Modelo:', end - start)\n",
    "    \n",
    "    # Fazendo as previsões para o dataset de teste\n",
    "    previsoes = modelo.predict(X_teste)\n",
    "    \n",
    "    # Avaliação do modelo\n",
    "    # Matriz de confusão\n",
    "    print('\\nMatriz de confusão\\n', confusion_matrix(y_teste, previsoes))\n",
    "    \n",
    "    # Dicionário de métricas e metadados\n",
    "    dict_model =   {'Modelo': nome,\n",
    "                    'Versão': version,\n",
    "                    'Precision':precision_score(y_teste, previsoes, average = 'macro', zero_division=0),\n",
    "                    'Recall':recall_score(y_teste, previsoes, average = 'macro', zero_division=0),\n",
    "                    'F1 Score':f1_score(y_teste, previsoes, average = 'macro', zero_division=0),\n",
    "                    'Acurácia':accuracy_score(y_teste, previsoes)}\n",
    "    \n",
    "    if auc:\n",
    "        dict_model['ROC AUC'] = roc_auc_score(y_teste, previsoes, multi_class='ovr')\n",
    "        \n",
    "    return modelo, dict_model, previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "26efaa0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.0019979476928710938\n",
      "\n",
      "Matriz de confusão\n",
      " [[18  0  1  0  0]\n",
      " [ 2 10  1  0  0]\n",
      " [ 1  2  7  0  6]\n",
      " [ 0  1  0  1  1]\n",
      " [ 3  0  0  1  3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'KNN',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.6194017094017094,\n",
       " 'Recall': 0.5832007904376326,\n",
       " 'F1 Score': 0.5838762496053878,\n",
       " 'Acurácia': 0.6724137931034483}"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 1 - KNN - Multiclasse\n",
    "modelo1_multi, dict1, previsoes1 = train_and_score_multiclass_model(KNeighborsClassifier(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'KNN',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "bca94286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.0040018558502197266\n",
      "\n",
      "Matriz de confusão\n",
      " [[ 2  3  0 12  2]\n",
      " [ 0 11  0  2  0]\n",
      " [ 0  1  3  7  5]\n",
      " [ 0  0  1  2  0]\n",
      " [ 0  2  0  1  4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Naive Bayes',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.5688057040998218,\n",
       " 'Recall': 0.4754024484287642,\n",
       " 'F1 Score': 0.3632804232804233,\n",
       " 'Acurácia': 0.3793103448275862}"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 2 - Naive Bayes - Multiclasse\n",
    "modelo2_multi, dict2, previsoes2 = train_and_score_multiclass_model(GaussianNB(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'Naive Bayes',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "3e03b252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.00400233268737793\n",
      "\n",
      "Matriz de confusão\n",
      " [[16  1  1  0  1]\n",
      " [ 0 11  1  1  0]\n",
      " [ 0  1  9  5  1]\n",
      " [ 0  0  0  3  0]\n",
      " [ 1  0  0  1  5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Decision Tree Classifier',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.7239595698419228,\n",
       " 'Recall': 0.7930089647194911,\n",
       " 'F1 Score': 0.7155067155067155,\n",
       " 'Acurácia': 0.7586206896551724}"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 3 - Decision Tree - Multiclasse\n",
    "modelo3_multi, dict3, previsoes3 = train_and_score_multiclass_model(DecisionTreeClassifier(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'Decision Tree Classifier',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "d67c9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.1849987506866455\n",
      "\n",
      "Matriz de confusão\n",
      " [[19  0  0  0  0]\n",
      " [ 1 12  0  0  0]\n",
      " [ 0  1  9  5  1]\n",
      " [ 0  0  1  2  0]\n",
      " [ 0  1  0  1  5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Random Forest Classifier',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.758095238095238,\n",
       " 'Recall': 0.7733058608058608,\n",
       " 'F1 Score': 0.7376845376845377,\n",
       " 'Acurácia': 0.8103448275862069}"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 4 - Decision Tree - Multiclasse\n",
    "modelo4_multi, dict4, previsoes4 = train_and_score_multiclass_model(RandomForestClassifier(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'Random Forest Classifier',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "3c07325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.005006551742553711\n",
      "\n",
      "Matriz de confusão\n",
      " [[17  1  1  0  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  1  9  0  6]\n",
      " [ 0  0  0  1  2]\n",
      " [ 2  0  0  0  5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'SVM Classifier',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.8092037786774628,\n",
       " 'Recall': 0.7009711779448622,\n",
       " 'F1 Score': 0.7031231925968768,\n",
       " 'Acurácia': 0.7758620689655172}"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 5 - SVM Classifier - Multiclasse\n",
    "modelo5_multi, dict5, previsoes5 = train_and_score_multiclass_model(svm.SVC(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'SVM Classifier',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "f76a8c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.08599710464477539\n",
      "\n",
      "Matriz de confusão\n",
      " [[18  1  0  0  0]\n",
      " [ 1 11  0  1  0]\n",
      " [ 1  0  9  5  1]\n",
      " [ 0  0  0  2  1]\n",
      " [ 0  0  0  1  6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'XGBoost Classifier',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.7577777777777778,\n",
       " 'Recall': 0.7759663582032003,\n",
       " 'F1 Score': 0.7312820512820513,\n",
       " 'Acurácia': 0.7931034482758621}"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 6 - SVM Classifier - Multiclasse\n",
    "modelo6_multi, dict6, previsoes6 = train_and_score_multiclass_model(XGBClassifier(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'XGBoost Classifier',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "eaab7dfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict1</th>\n",
       "      <th>dict2</th>\n",
       "      <th>dict3</th>\n",
       "      <th>dict4</th>\n",
       "      <th>dict5</th>\n",
       "      <th>dict6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>XGBoost Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versão</th>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.619402</td>\n",
       "      <td>0.568806</td>\n",
       "      <td>0.72396</td>\n",
       "      <td>0.758095</td>\n",
       "      <td>0.809204</td>\n",
       "      <td>0.757778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.583201</td>\n",
       "      <td>0.475402</td>\n",
       "      <td>0.793009</td>\n",
       "      <td>0.773306</td>\n",
       "      <td>0.700971</td>\n",
       "      <td>0.775966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.583876</td>\n",
       "      <td>0.36328</td>\n",
       "      <td>0.715507</td>\n",
       "      <td>0.737685</td>\n",
       "      <td>0.703123</td>\n",
       "      <td>0.731282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acurácia</th>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.810345</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 dict1                        dict2  \\\n",
       "Modelo                             KNN                  Naive Bayes   \n",
       "Versão     Multiclass Classification 1  Multiclass Classification 1   \n",
       "Precision                     0.619402                     0.568806   \n",
       "Recall                        0.583201                     0.475402   \n",
       "F1 Score                      0.583876                      0.36328   \n",
       "Acurácia                      0.672414                      0.37931   \n",
       "\n",
       "                                 dict3                        dict4  \\\n",
       "Modelo        Decision Tree Classifier     Random Forest Classifier   \n",
       "Versão     Multiclass Classification 1  Multiclass Classification 1   \n",
       "Precision                      0.72396                     0.758095   \n",
       "Recall                        0.793009                     0.773306   \n",
       "F1 Score                      0.715507                     0.737685   \n",
       "Acurácia                      0.758621                     0.810345   \n",
       "\n",
       "                                 dict5                        dict6  \n",
       "Modelo                  SVM Classifier           XGBoost Classifier  \n",
       "Versão     Multiclass Classification 1  Multiclass Classification 1  \n",
       "Precision                     0.809204                     0.757778  \n",
       "Recall                        0.700971                     0.775966  \n",
       "F1 Score                      0.703123                     0.731282  \n",
       "Acurácia                      0.775862                     0.793103  "
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumo_multi1 = pd.DataFrame({'dict1':pd.Series(dict1),\n",
    "                           'dict2':pd.Series(dict2),\n",
    "                           'dict3':pd.Series(dict3),\n",
    "                           'dict4':pd.Series(dict4),\n",
    "                           'dict5':pd.Series(dict5),\n",
    "                           'dict6':pd.Series(dict6)})\n",
    "\n",
    "resumo_multi1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f845b",
   "metadata": {},
   "source": [
    "Para o modelo de classificação multiclasse, utilizaremos a métrica F1 Score, que é a média harmônica entre precision e recall para avaliar o modelo. Como agora não temos exatamente uma falha como mais ou menos grave que outra, um bom equilíbrio de falsos positivos e falsos negativos pode ser útil.\n",
    "\n",
    "Dentre os modelos utilizados para a classificação multiclasse, o Random Forest foi o que melhor respondeu. Vamos utilizar esse modelo como base e fazer um GridSearchCV para encontrar os melhores hiperparâmetros para ele e tentar melhorar a resposta final do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "96e87158",
   "metadata": {},
   "outputs": [],
   "source": [
    "?modelo4_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "9bb66e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV executou em 875.01 segundos para todas as combinações de candidatos a parâmetros do modelo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.14619908, 0.26000257, 0.38700166, 0.55700116, 0.13379655,\n",
       "        0.25580282, 0.37999907, 0.50980201, 0.15120234, 0.28480124,\n",
       "        0.38839884, 0.50440016, 0.13140011, 0.25179977, 0.38319621,\n",
       "        0.50279946, 0.13459878, 0.26299996, 0.38920269, 0.5047998 ,\n",
       "        0.13520117, 0.26380029, 0.4085999 , 0.53579874, 0.13760028,\n",
       "        0.27359862, 0.38560052, 0.53220277, 0.13180084, 0.2619997 ,\n",
       "        0.43960185, 0.52360268, 0.13080034, 0.27360315, 0.39360147,\n",
       "        0.53020072, 0.13080101, 0.27000413, 0.39060111, 0.52819762,\n",
       "        0.136201  , 0.27260151, 0.39379988, 0.5508019 , 0.13300157,\n",
       "        0.26879883, 0.4041996 , 0.53100309, 0.1357985 , 0.26500006,\n",
       "        0.42360053, 0.5463975 , 0.13119969, 0.26959887, 0.40339999,\n",
       "        0.52439995, 0.13279409, 0.26059823, 0.3875998 , 0.5424016 ,\n",
       "        0.13880043, 0.26659961, 0.40100255, 0.53439746, 0.13279977,\n",
       "        0.26699996, 0.40000038, 0.53440142, 0.13220053, 0.26559978,\n",
       "        0.39959941, 0.58220439, 0.13959932, 0.26979876, 0.41259823,\n",
       "        0.51780014, 0.1378037 , 0.26400189, 0.40160017, 0.52459798,\n",
       "        0.13199978, 0.26860094, 0.40139737, 0.54440269, 0.13239961,\n",
       "        0.26100025, 0.39940114, 0.52779818, 0.13140111, 0.2678009 ,\n",
       "        0.40560093, 0.53840189, 0.15439534, 0.27079935, 0.38679972,\n",
       "        0.53059945, 0.13140035, 0.26000004, 0.38840003, 0.53659844,\n",
       "        0.140799  , 0.27060089, 0.40880079, 0.54699955, 0.1369977 ,\n",
       "        0.26559901, 0.41099873, 0.54440117, 0.13640079, 0.27379823,\n",
       "        0.40279727, 0.53319983, 0.14220095, 0.37680159, 0.41160078,\n",
       "        0.54359865, 0.13340011, 0.26680002, 0.41120024, 0.54359708,\n",
       "        0.14659815, 0.27739882, 0.42820139, 0.54979949, 0.13859739,\n",
       "        0.29320374, 0.41959963, 0.56700115, 0.14160042, 0.27840033,\n",
       "        0.52919869, 0.65559616, 0.19519577, 0.35760212, 0.48240523,\n",
       "        0.6556035 , 0.16440234, 0.32059875, 0.49219933, 0.64379721,\n",
       "        0.13579893, 0.3208251 , 0.42259922, 0.55560112, 0.14539871,\n",
       "        0.29000206, 0.41879883, 0.55919981, 0.14000025, 0.27200046,\n",
       "        0.40879803, 0.59920297, 0.14159946, 0.26820021, 0.41199837,\n",
       "        0.54959874, 0.14519734, 0.27820339, 0.40779977, 0.53780146,\n",
       "        0.13860149, 0.2725986 , 0.41660404, 0.55439625, 0.13059821,\n",
       "        0.26080012, 0.38979988, 0.5225987 , 0.13020234, 0.26060181,\n",
       "        0.3920011 , 0.57500005, 0.13639688, 0.26600232, 0.40639844,\n",
       "        0.51520014, 0.1308033 , 0.27140002, 0.39519825, 0.5508007 ,\n",
       "        0.13659735, 0.27400007, 0.4116024 , 0.55140128, 0.13539987,\n",
       "        0.26980128, 0.41299973, 0.57319932, 0.13480048, 0.27499776,\n",
       "        0.41220016, 0.55359797, 0.16659632, 0.27900057, 0.40599799,\n",
       "        0.54140272, 0.13519936, 0.26939921, 0.39960232, 0.53980112,\n",
       "        0.13639979, 0.27139912, 0.42079978, 0.54780235, 0.13620086,\n",
       "        0.2699996 , 0.4085999 , 0.55279841, 0.1369988 , 0.2746016 ,\n",
       "        0.4132019 , 0.54699984, 0.15060096, 0.29919739, 0.40399957,\n",
       "        0.56139936, 0.1415998 , 0.27499905, 0.40539985, 0.53779817,\n",
       "        0.13719993, 0.27659774, 0.4222002 , 0.54360061, 0.13520136,\n",
       "        0.30619984, 0.40440011, 0.54140081, 0.14500031, 0.2727983 ,\n",
       "        0.40340014, 0.54819946, 0.13460002, 0.31479635, 0.41720381,\n",
       "        0.54499917, 0.13919978, 0.27140098, 0.41240244, 0.53160315,\n",
       "        0.10120039, 0.20380197, 0.29640088, 0.39499907, 0.09940028,\n",
       "        0.20520177, 0.29799881, 0.39440088, 0.1052002 , 0.19939933,\n",
       "        0.30260177, 0.39340119, 0.09799995, 0.19379935, 0.29559755,\n",
       "        0.39059958, 0.09999914, 0.20299973, 0.3387991 , 0.38760004,\n",
       "        0.10179996, 0.20740185, 0.30799937, 0.41060004, 0.10159898,\n",
       "        0.19959965, 0.30259953, 0.40139985, 0.10460181, 0.20240026,\n",
       "        0.30199785, 0.40620117, 0.10199981, 0.20300088, 0.30119972,\n",
       "        0.40439978, 0.10080018, 0.20660152, 0.30959864, 0.41139879,\n",
       "        0.10360141, 0.204001  , 0.30919976, 0.44840026, 0.12320065,\n",
       "        0.21280165, 0.30799909, 0.41260133, 0.10419974, 0.21540151,\n",
       "        0.31060052, 0.42159839, 0.10560007, 0.20620003, 0.30900044,\n",
       "        0.42059879, 0.10379901, 0.2102006 , 0.30819993, 0.43739719,\n",
       "        0.10480056, 0.20539846, 0.31059947, 0.4250001 , 0.10460014,\n",
       "        0.2082005 , 0.31460137, 0.42760057, 0.10359907, 0.21700034,\n",
       "        0.34019589, 0.42359829, 0.10539808, 0.20580025, 0.31039968,\n",
       "        0.41599936, 0.10440264, 0.2062005 , 0.30960126, 0.42639742,\n",
       "        0.10259886, 0.21540303, 0.30619965, 0.41359835, 0.10380111,\n",
       "        0.20940151, 0.30780025, 0.46580071, 0.10400004, 0.20460129,\n",
       "        0.30239997, 0.41299987, 0.10380216, 0.20360131, 0.30539927,\n",
       "        0.46780186, 0.10300112, 0.20439982, 0.3026022 , 0.4092011 ,\n",
       "        0.10839987, 0.21719832, 0.32120028, 0.43320079, 0.10900097,\n",
       "        0.21100044, 0.32019887, 0.43699975, 0.11060162, 0.21220088,\n",
       "        0.31820025, 0.43059859, 0.10960054, 0.21440144, 0.32739782,\n",
       "        0.42720146, 0.10639715, 0.21299763, 0.31620049, 0.42800174,\n",
       "        0.12400093, 0.25419974, 0.32819877, 0.44719648, 0.11860237,\n",
       "        0.22320056, 0.3310008 , 0.44580159, 0.10999947, 0.21739898,\n",
       "        0.34060106, 0.44160142, 0.11099882, 0.21619887, 0.32479982,\n",
       "        0.44720054, 0.11539979, 0.22200046, 0.33919754, 0.43540015,\n",
       "        0.1161994 , 0.21839981, 0.33200016, 0.447399  , 0.11639972,\n",
       "        0.26479626, 0.34539857, 0.46479897, 0.12980027, 0.22619996,\n",
       "        0.33979888, 0.45039992, 0.12879844, 0.27039962, 0.42540221,\n",
       "        0.53739762, 0.12240281, 0.24979701, 0.35139885, 0.54060502,\n",
       "        0.13000407, 0.28500109, 0.40139985, 0.53060102, 0.15020089,\n",
       "        0.27492423, 0.50010333, 0.53340478, 0.13410678, 0.24758081,\n",
       "        0.36040487, 0.50039854, 0.12250204, 0.21920352, 0.35660338,\n",
       "        0.46660295, 0.11400099, 0.22639961, 0.34879861, 0.45839758,\n",
       "        0.11779976, 0.31800413, 0.39399338, 0.47959867, 0.1374011 ,\n",
       "        0.27120028, 0.35980411, 0.52000036, 0.12299857, 0.23420653,\n",
       "        0.35460129, 0.47719755, 0.12480049, 0.23859758, 0.34819918,\n",
       "        0.48140182, 0.11620135, 0.23120365, 0.35340075, 0.47660303,\n",
       "        0.14400339, 0.27580371, 0.36559997, 0.56459908, 0.15879936,\n",
       "        0.28419881, 0.40199995, 0.55600176, 0.1642015 , 0.29739985,\n",
       "        0.46860099, 0.60779843, 0.14160256, 0.27060623, 0.41539836,\n",
       "        0.56339598, 0.12660179, 0.24400015, 0.42140017, 0.51280208,\n",
       "        0.13020263, 0.252599  , 0.39320474, 0.49199619, 0.11579876,\n",
       "        0.24719644, 0.36020055, 0.46080141, 0.11800265, 0.23899784,\n",
       "        0.36759553, 0.49120378, 0.16500111, 0.27319865, 0.36740079,\n",
       "        0.77219839, 0.15239897, 0.26760206, 0.40739942, 0.50519824]),\n",
       " 'std_fit_time': array([2.53945044e-02, 9.94728740e-03, 5.65711910e-03, 1.45644047e-02,\n",
       "        3.42884598e-03, 5.03841954e-03, 7.12804633e-03, 4.17116976e-03,\n",
       "        6.96846980e-03, 2.73548524e-02, 1.02282395e-02, 3.00501528e-03,\n",
       "        6.21615773e-03, 2.13518498e-03, 4.66414348e-03, 4.62312384e-03,\n",
       "        4.96200804e-03, 1.87387032e-02, 1.24161500e-02, 5.03566352e-03,\n",
       "        3.37207134e-03, 2.71279513e-03, 7.86342301e-03, 1.49186538e-02,\n",
       "        6.46793058e-03, 6.15084385e-03, 2.41660772e-03, 7.88636389e-03,\n",
       "        3.48825593e-03, 8.85482910e-03, 3.82928485e-02, 8.88921536e-03,\n",
       "        1.93918227e-03, 6.74249703e-03, 8.70972999e-03, 2.18658146e-02,\n",
       "        1.32681033e-03, 9.59413502e-03, 8.89221244e-03, 9.59913789e-03,\n",
       "        6.04601235e-03, 5.95325148e-03, 3.06003497e-03, 3.24654459e-02,\n",
       "        2.09610379e-03, 6.04614159e-03, 7.27930404e-03, 1.10103427e-02,\n",
       "        5.07636938e-03, 2.60793136e-03, 3.13030856e-02, 4.75511573e-02,\n",
       "        2.56183518e-03, 6.37735112e-03, 6.02143908e-03, 6.88774970e-03,\n",
       "        1.47228044e-03, 2.57727509e-03, 3.19941050e-03, 1.88968197e-02,\n",
       "        3.65817089e-03, 5.95320596e-03, 7.53789187e-03, 4.84042308e-03,\n",
       "        1.93935942e-03, 8.53226558e-03, 7.64225282e-03, 1.63805190e-02,\n",
       "        2.40037442e-03, 4.31782961e-03, 7.65691613e-03, 4.89943283e-02,\n",
       "        8.59253260e-03, 5.49165631e-03, 2.61664146e-02, 3.60003049e-03,\n",
       "        8.06288722e-03, 5.40412316e-03, 1.19437851e-02, 8.30755107e-03,\n",
       "        3.28648893e-03, 5.85179870e-03, 1.25142606e-02, 9.28661309e-03,\n",
       "        3.44112140e-03, 4.14724065e-03, 1.27048580e-02, 1.03601816e-02,\n",
       "        2.33161621e-03, 6.11397883e-03, 7.20027568e-03, 3.40757080e-02,\n",
       "        1.33342879e-02, 2.11691533e-02, 4.79130616e-03, 1.76245246e-02,\n",
       "        2.57854392e-03, 2.09742025e-03, 1.85478468e-03, 3.01718634e-02,\n",
       "        4.44582923e-03, 6.21747588e-03, 8.30368083e-03, 8.74102078e-03,\n",
       "        2.44779514e-03, 2.33354740e-03, 1.21493593e-02, 1.38516084e-02,\n",
       "        4.27175444e-03, 5.56399067e-03, 7.72814116e-03, 7.35939495e-03,\n",
       "        9.53978383e-03, 1.11045267e-01, 8.68659858e-03, 6.94584479e-03,\n",
       "        4.90154472e-04, 2.71361775e-03, 1.97827223e-02, 6.46659741e-03,\n",
       "        1.10189219e-02, 8.30948651e-03, 1.34843574e-02, 8.99798387e-03,\n",
       "        3.72255517e-03, 2.72070568e-02, 9.04743508e-03, 2.95739459e-02,\n",
       "        2.41678415e-03, 8.84605024e-03, 8.98889794e-02, 3.96756928e-02,\n",
       "        1.44999119e-02, 4.66931918e-02, 1.39497158e-02, 1.34079666e-02,\n",
       "        4.17946899e-03, 5.01054019e-03, 7.75880209e-03, 4.13044794e-02,\n",
       "        3.99832009e-04, 2.24508893e-02, 1.88847314e-02, 1.25788090e-02,\n",
       "        5.00557436e-03, 1.11563989e-02, 8.66008936e-03, 9.36730497e-03,\n",
       "        4.19390903e-03, 4.24267055e-03, 6.96620275e-03, 5.00012449e-02,\n",
       "        5.27481968e-03, 3.06008772e-03, 6.09628349e-03, 1.52821697e-02,\n",
       "        7.35674755e-03, 6.76400557e-03, 4.95413708e-03, 8.77281887e-03,\n",
       "        9.49773563e-03, 7.05865265e-03, 1.87130158e-02, 2.41466603e-02,\n",
       "        2.24545917e-03, 4.06946396e-03, 2.71298143e-03, 5.38833540e-03,\n",
       "        1.93835705e-03, 5.85459755e-03, 6.92804557e-03, 5.12218836e-02,\n",
       "        3.38253114e-03, 6.78335762e-03, 7.05929440e-03, 2.92577696e-03,\n",
       "        1.94455410e-03, 5.08334844e-03, 8.42163811e-03, 1.81036117e-02,\n",
       "        1.49772793e-03, 5.58570096e-03, 9.43476754e-03, 1.47032249e-02,\n",
       "        2.57655529e-03, 2.71428974e-03, 1.13487115e-02, 3.38410200e-02,\n",
       "        1.47354616e-03, 7.48116814e-03, 1.13738650e-02, 3.92333716e-02,\n",
       "        1.79481647e-02, 1.16596286e-02, 4.60452058e-03, 1.02476573e-02,\n",
       "        2.31497077e-03, 2.93814954e-03, 5.99110383e-03, 1.07966430e-02,\n",
       "        1.85502132e-03, 4.92375822e-03, 1.16525986e-02, 7.22292238e-03,\n",
       "        2.40189278e-03, 1.89662604e-03, 5.46295140e-03, 1.52092161e-02,\n",
       "        3.03393741e-03, 4.27398766e-03, 1.03789772e-02, 1.01418778e-02,\n",
       "        1.48741429e-02, 2.71663651e-02, 2.89954636e-03, 1.84005998e-02,\n",
       "        1.74536068e-03, 6.16351148e-03, 7.60524180e-03, 9.17406793e-03,\n",
       "        1.93904553e-03, 6.40634473e-03, 1.24983212e-02, 3.61146142e-03,\n",
       "        7.49502629e-04, 3.13862860e-02, 3.26191631e-03, 7.41954629e-03,\n",
       "        8.21890233e-03, 5.63576732e-03, 2.15412232e-03, 1.68427325e-02,\n",
       "        7.98399036e-04, 1.97187745e-02, 8.79591303e-03, 9.95901196e-03,\n",
       "        5.56209436e-03, 2.05950621e-03, 8.37855872e-03, 7.97895357e-04,\n",
       "        2.78622219e-03, 7.93300026e-03, 6.73951533e-03, 1.13497369e-02,\n",
       "        2.79998780e-03, 6.42960587e-03, 1.00575988e-02, 6.97626835e-03,\n",
       "        3.48506526e-03, 2.15431734e-03, 4.75687340e-03, 9.11200922e-03,\n",
       "        6.32409775e-04, 1.60033790e-03, 4.79746889e-03, 6.21846533e-03,\n",
       "        2.44995482e-03, 1.19162999e-02, 3.60303025e-02, 5.85151147e-03,\n",
       "        7.47946040e-04, 4.17702398e-03, 9.09783734e-03, 7.25558596e-03,\n",
       "        2.24526151e-03, 4.89998908e-04, 4.84129406e-03, 3.61079532e-03,\n",
       "        3.00832573e-03, 1.01986093e-03, 3.16280372e-03, 9.74723429e-03,\n",
       "        2.52961993e-03, 3.03355925e-03, 2.78534910e-03, 6.40610275e-03,\n",
       "        3.99780615e-04, 7.83892104e-03, 6.62024340e-03, 8.21173035e-03,\n",
       "        8.02135779e-04, 1.09506214e-03, 6.21294457e-03, 3.68725360e-02,\n",
       "        1.25430885e-02, 7.22193979e-03, 3.74095293e-03, 3.72209913e-03,\n",
       "        1.72060126e-03, 1.51597739e-02, 8.26260624e-03, 8.73058527e-03,\n",
       "        4.02964122e-03, 3.86755040e-03, 9.77743108e-03, 1.02093938e-02,\n",
       "        2.92640673e-03, 8.18331602e-03, 4.53394923e-03, 2.70972902e-02,\n",
       "        2.22697008e-03, 1.85428269e-03, 6.34507380e-03, 1.64074140e-02,\n",
       "        2.24495495e-03, 4.11998816e-03, 9.52184173e-03, 1.53576183e-02,\n",
       "        1.20031957e-03, 1.33865733e-02, 3.51120565e-02, 6.59027537e-03,\n",
       "        3.55401491e-03, 4.21481146e-03, 9.46075203e-03, 1.22145276e-02,\n",
       "        2.33491913e-03, 5.70539961e-03, 6.05304446e-03, 1.30761175e-02,\n",
       "        2.72843243e-03, 7.17253758e-03, 3.12406293e-03, 9.47814541e-03,\n",
       "        2.39907563e-03, 1.01542150e-02, 8.03560313e-03, 9.93807785e-02,\n",
       "        8.94209780e-04, 4.49870281e-03, 3.38447863e-03, 1.58119825e-02,\n",
       "        1.93939376e-03, 3.61232224e-03, 4.58823164e-03, 5.07542575e-02,\n",
       "        1.09441226e-03, 2.72772185e-03, 2.93833809e-03, 3.54419637e-03,\n",
       "        1.85490504e-03, 8.08304733e-03, 6.52149722e-03, 8.51750339e-03,\n",
       "        1.89800952e-03, 4.20861133e-06, 6.40080589e-03, 4.00139241e-03,\n",
       "        2.33308774e-03, 1.46938770e-03, 2.39983003e-03, 5.74604629e-03,\n",
       "        2.24587848e-03, 4.75784238e-03, 1.05534062e-02, 5.91496332e-03,\n",
       "        4.86768021e-04, 2.18846851e-03, 2.48205844e-03, 9.61298611e-03,\n",
       "        9.25153179e-03, 2.98525773e-02, 4.25945008e-03, 5.77302209e-03,\n",
       "        1.85490626e-03, 6.40033197e-03, 7.40303546e-03, 1.24019477e-02,\n",
       "        2.52880945e-03, 3.07290965e-03, 9.49871563e-03, 6.21607548e-03,\n",
       "        4.51477776e-03, 2.03929728e-03, 3.71055486e-03, 1.55352681e-02,\n",
       "        6.11906292e-03, 1.08971330e-02, 1.10352054e-02, 9.35052324e-03,\n",
       "        2.48154959e-03, 1.85491845e-03, 8.55715891e-03, 1.43611547e-02,\n",
       "        4.32032641e-03, 2.82279945e-02, 1.21066305e-02, 3.65656944e-02,\n",
       "        2.22479821e-02, 7.57360835e-03, 8.08529811e-03, 2.00029256e-02,\n",
       "        1.09263721e-02, 3.00296147e-02, 5.82703987e-02, 3.58717341e-02,\n",
       "        5.60769399e-03, 1.78157689e-02, 1.52948619e-02, 8.32441349e-02,\n",
       "        1.67821825e-02, 3.12244383e-02, 2.76090172e-02, 2.26950302e-02,\n",
       "        1.13086846e-02, 1.56556237e-02, 2.91217203e-02, 5.46947319e-02,\n",
       "        7.50447422e-03, 1.25859967e-02, 1.81018446e-02, 2.56922658e-02,\n",
       "        7.53850759e-03, 5.81314705e-03, 3.34635820e-02, 1.73471151e-02,\n",
       "        6.89834011e-03, 9.84980507e-03, 1.47963646e-02, 1.93611818e-02,\n",
       "        3.31067762e-03, 4.50096854e-02, 6.48726011e-02, 2.65610560e-03,\n",
       "        1.00521461e-02, 1.55893999e-02, 1.01488728e-02, 4.51584763e-02,\n",
       "        4.68873492e-03, 4.96066700e-03, 1.16698256e-02, 1.57429254e-02,\n",
       "        3.06138868e-03, 1.10017963e-02, 6.76667254e-03, 1.45541856e-02,\n",
       "        4.26358769e-03, 8.05731431e-03, 1.12007592e-02, 1.08903223e-02,\n",
       "        7.42655231e-03, 2.47229503e-02, 9.79210101e-03, 3.57136408e-02,\n",
       "        9.80685290e-03, 4.79584207e-02, 1.66229532e-02, 1.57883178e-02,\n",
       "        4.50315083e-02, 5.02066980e-02, 1.26380073e-01, 9.08612299e-02,\n",
       "        1.82702482e-02, 1.88335663e-02, 3.36952751e-02, 5.67213057e-02,\n",
       "        7.11165655e-03, 7.92615892e-03, 7.77501671e-02, 3.08996283e-02,\n",
       "        6.01071399e-03, 1.69560876e-02, 1.44540933e-02, 3.19773282e-02,\n",
       "        1.72136878e-03, 1.96998449e-02, 6.70957412e-03, 4.16691822e-03,\n",
       "        5.54541214e-03, 9.52666047e-03, 1.46535397e-02, 1.24993206e-02,\n",
       "        2.08252786e-02, 3.10974344e-02, 1.96310453e-02, 2.41436416e-01,\n",
       "        2.14314878e-02, 1.35157630e-02, 2.85040799e-02, 4.50004734e-02]),\n",
       " 'mean_score_time': array([0.01040025, 0.01999831, 0.02680202, 0.03619761, 0.011201  ,\n",
       "        0.01859698, 0.02639999, 0.03479767, 0.01059704, 0.01899967,\n",
       "        0.02619977, 0.03539901, 0.01019993, 0.01820002, 0.02659998,\n",
       "        0.03480053, 0.01139975, 0.018999  , 0.02719984, 0.03540134,\n",
       "        0.01159992, 0.01859984, 0.02719898, 0.03560104, 0.01060119,\n",
       "        0.01939955, 0.0265995 , 0.03600025, 0.01040049, 0.01960039,\n",
       "        0.02779822, 0.03579755, 0.0099997 , 0.020398  , 0.02679896,\n",
       "        0.0354002 , 0.01039791, 0.01900134, 0.02699904, 0.0355998 ,\n",
       "        0.01039929, 0.01919866, 0.02619967, 0.03519979, 0.01020021,\n",
       "        0.01980042, 0.02839966, 0.03499923, 0.0102006 , 0.01880002,\n",
       "        0.0270031 , 0.03520217, 0.01099963, 0.01919813, 0.02880216,\n",
       "        0.03520103, 0.01060085, 0.01820045, 0.02719979, 0.03619967,\n",
       "        0.01219964, 0.01920109, 0.02739964, 0.03520093, 0.01000032,\n",
       "        0.01880212, 0.02720137, 0.03620133, 0.01020045, 0.0194005 ,\n",
       "        0.02679982, 0.03599691, 0.01060047, 0.01860123, 0.02719827,\n",
       "        0.03460021, 0.01039886, 0.01819835, 0.02659965, 0.0362009 ,\n",
       "        0.01060095, 0.0194006 , 0.02800283, 0.03599777, 0.01039948,\n",
       "        0.01880131, 0.02719913, 0.03480301, 0.00999894, 0.018399  ,\n",
       "        0.02679915, 0.03499823, 0.01100006, 0.01819987, 0.02620039,\n",
       "        0.03580098, 0.01019883, 0.01840119, 0.0265996 , 0.03419867,\n",
       "        0.01040006, 0.01859956, 0.02659912, 0.03560095, 0.01079931,\n",
       "        0.01839924, 0.02720189, 0.03519983, 0.01040001, 0.01839972,\n",
       "        0.02679987, 0.03559999, 0.01079826, 0.01880445, 0.02899928,\n",
       "        0.03500142, 0.01020002, 0.0188004 , 0.02720141, 0.03600121,\n",
       "        0.01060104, 0.01920114, 0.02799859, 0.03420124, 0.01080027,\n",
       "        0.02019629, 0.02739954, 0.0353981 , 0.01039886, 0.01879873,\n",
       "        0.03060145, 0.03640437, 0.01300483, 0.01939845, 0.02799869,\n",
       "        0.03640108, 0.01099677, 0.01879969, 0.02760005, 0.03579907,\n",
       "        0.01020055, 0.0191781 , 0.02780151, 0.0351984 , 0.01059937,\n",
       "        0.01919813, 0.0270021 , 0.03500109, 0.01040049, 0.0193994 ,\n",
       "        0.02700033, 0.03559546, 0.01019998, 0.01860166, 0.02779999,\n",
       "        0.03579993, 0.01060185, 0.01939869, 0.02780156, 0.03619785,\n",
       "        0.01039844, 0.01840014, 0.02939692, 0.03479991, 0.01000104,\n",
       "        0.01859984, 0.02620025, 0.03539977, 0.01059766, 0.0189971 ,\n",
       "        0.02699909, 0.03859854, 0.01040092, 0.01879787, 0.02700129,\n",
       "        0.0346015 , 0.01059828, 0.01880102, 0.02620111, 0.03680019,\n",
       "        0.01040063, 0.01820078, 0.02799945, 0.03479929, 0.01000023,\n",
       "        0.0181994 , 0.02839913, 0.03600006, 0.01059914, 0.01839976,\n",
       "        0.02760072, 0.03719912, 0.01120358, 0.01839991, 0.02660055,\n",
       "        0.03499665, 0.01060114, 0.01859994, 0.02719779, 0.0359993 ,\n",
       "        0.01019988, 0.01859965, 0.02799821, 0.03499665, 0.01079907,\n",
       "        0.01839991, 0.0270009 , 0.03579812, 0.0105998 , 0.01840291,\n",
       "        0.02679772, 0.03559875, 0.01119852, 0.01899848, 0.02659945,\n",
       "        0.03640084, 0.0110002 , 0.01899915, 0.02660208, 0.0348001 ,\n",
       "        0.01039939, 0.01939859, 0.02699943, 0.03479996, 0.01039882,\n",
       "        0.01939664, 0.02599993, 0.03559914, 0.01079793, 0.02000003,\n",
       "        0.02680025, 0.03479939, 0.01039977, 0.02120466, 0.02719665,\n",
       "        0.03519869, 0.01099968, 0.01859884, 0.02739758, 0.03559709,\n",
       "        0.01019945, 0.01899905, 0.0265986 , 0.03440003, 0.01020002,\n",
       "        0.01919627, 0.02620139, 0.03480096, 0.01080027, 0.01880002,\n",
       "        0.02659974, 0.03399882, 0.00999994, 0.01839991, 0.02760158,\n",
       "        0.03519959, 0.01040015, 0.01819997, 0.02780089, 0.03420053,\n",
       "        0.0099997 , 0.01860123, 0.02700086, 0.03639979, 0.01020017,\n",
       "        0.01860371, 0.02659912, 0.03500285, 0.0103981 , 0.01819968,\n",
       "        0.02679963, 0.03459845, 0.01020002, 0.01859899, 0.02640009,\n",
       "        0.03500252, 0.01039968, 0.01859994, 0.02780094, 0.03600183,\n",
       "        0.0103991 , 0.01859879, 0.02659822, 0.03600063, 0.0112    ,\n",
       "        0.01879916, 0.02700043, 0.03640032, 0.01060123, 0.01899877,\n",
       "        0.02659993, 0.03579969, 0.01020083, 0.01799989, 0.02680192,\n",
       "        0.03540187, 0.01080003, 0.01899948, 0.02680016, 0.03620009,\n",
       "        0.0102006 , 0.01880016, 0.02720308, 0.03640103, 0.01060052,\n",
       "        0.01919861, 0.0273994 , 0.03600016, 0.01019983, 0.01960149,\n",
       "        0.02860179, 0.03499994, 0.01079988, 0.01819987, 0.02759905,\n",
       "        0.03499956, 0.01019769, 0.01899934, 0.02719893, 0.03780046,\n",
       "        0.01060014, 0.01959748, 0.02739978, 0.03480077, 0.0101994 ,\n",
       "        0.01919694, 0.02660055, 0.03639984, 0.01040125, 0.01840029,\n",
       "        0.02639976, 0.03499966, 0.0101974 , 0.01819868, 0.02819991,\n",
       "        0.03599582, 0.01079869, 0.01840024, 0.02639904, 0.0353991 ,\n",
       "        0.00999904, 0.01879969, 0.02720041, 0.03559904, 0.01039996,\n",
       "        0.01839843, 0.02640023, 0.03519988, 0.01059904, 0.01880007,\n",
       "        0.02620087, 0.03579903, 0.01060004, 0.0183989 , 0.02740135,\n",
       "        0.03599858, 0.01060128, 0.01840119, 0.02619948, 0.03559947,\n",
       "        0.0114007 , 0.02099824, 0.02700114, 0.0358006 , 0.0107965 ,\n",
       "        0.01860065, 0.02719874, 0.03579865, 0.01120028, 0.01840014,\n",
       "        0.02700343, 0.03579826, 0.01040182, 0.0185998 , 0.02699909,\n",
       "        0.03600044, 0.01039982, 0.01879864, 0.02780147, 0.03559971,\n",
       "        0.01039934, 0.01899929, 0.02700033, 0.03560057, 0.01119843,\n",
       "        0.02060137, 0.02700047, 0.03739829, 0.01079898, 0.02000031,\n",
       "        0.02840314, 0.03599987, 0.01140246, 0.01899996, 0.03039746,\n",
       "        0.03899899, 0.011203  , 0.01900239, 0.0276    , 0.03599849,\n",
       "        0.01099663, 0.02719893, 0.02780309, 0.03840055, 0.01259756,\n",
       "        0.02059841, 0.03380017, 0.03799229, 0.01190534, 0.02040076,\n",
       "        0.02819734, 0.03920207, 0.01159878, 0.01959643, 0.02819757,\n",
       "        0.03679228, 0.01100106, 0.01980009, 0.02739973, 0.0372016 ,\n",
       "        0.01120281, 0.02540083, 0.02880068, 0.03780322, 0.01100073,\n",
       "        0.01979814, 0.02879667, 0.03800035, 0.01099925, 0.01939125,\n",
       "        0.02900209, 0.03739758, 0.01120319, 0.01900086, 0.0287992 ,\n",
       "        0.03759651, 0.01120167, 0.01859913, 0.02960124, 0.03739619,\n",
       "        0.01139927, 0.02079802, 0.02839928, 0.03780017, 0.0166019 ,\n",
       "        0.02019863, 0.0297998 , 0.03960176, 0.0146029 , 0.02319798,\n",
       "        0.03700099, 0.04499969, 0.01119976, 0.02079296, 0.03039842,\n",
       "        0.03779855, 0.0109973 , 0.02000079, 0.03220358, 0.03999672,\n",
       "        0.01119685, 0.02060127, 0.02859802, 0.03660083, 0.01079898,\n",
       "        0.01960316, 0.02799611, 0.03600311, 0.01079874, 0.01899762,\n",
       "        0.02880263, 0.03859763, 0.01279521, 0.01960425, 0.02940235,\n",
       "        0.04060035, 0.01279898, 0.01979856, 0.02940121, 0.03640199]),\n",
       " 'std_score_time': array([7.98132172e-04, 3.51557034e-03, 7.51898831e-04, 1.16582782e-03,\n",
       "        9.81259895e-04, 7.97681350e-04, 4.89668361e-04, 1.16553294e-03,\n",
       "        4.88661045e-04, 6.32635972e-04, 3.99971094e-04, 1.02019119e-03,\n",
       "        4.00352620e-04, 3.99899493e-04, 1.20027068e-03, 7.48775827e-04,\n",
       "        1.35714966e-03, 1.09589750e-03, 1.60151041e-03, 1.85446819e-03,\n",
       "        1.85607095e-03, 7.99822841e-04, 4.04289680e-04, 1.20132098e-03,\n",
       "        1.20167746e-03, 1.01828504e-03, 4.89667874e-04, 1.41485785e-03,\n",
       "        4.91971603e-04, 8.01625340e-04, 1.16389991e-03, 1.16171715e-03,\n",
       "        5.72204590e-07, 1.74125646e-03, 3.99358707e-04, 1.02192111e-03,\n",
       "        4.89207131e-04, 8.32159399e-06, 1.26350051e-03, 1.19999258e-03,\n",
       "        4.88559878e-04, 1.59931312e-03, 4.00377182e-04, 3.95478793e-04,\n",
       "        4.00117392e-04, 1.16634323e-03, 1.74375094e-03, 2.34863479e-06,\n",
       "        3.99662326e-04, 3.99708759e-04, 6.30620782e-04, 4.03571731e-04,\n",
       "        1.09549644e-03, 7.46621359e-04, 1.32881381e-03, 9.81331224e-04,\n",
       "        7.99741260e-04, 4.00400290e-04, 9.79608105e-04, 1.72037950e-03,\n",
       "        1.17166632e-03, 1.46993278e-03, 1.35540539e-03, 9.81740090e-04,\n",
       "        3.56832255e-07, 1.16666394e-03, 1.47004317e-03, 1.59793278e-03,\n",
       "        3.99494883e-04, 7.98275429e-04, 3.98233532e-04, 1.67029300e-03,\n",
       "        4.90991342e-04, 4.91290288e-04, 1.16578013e-03, 4.89745733e-04,\n",
       "        4.88521821e-04, 3.97634921e-04, 4.89493607e-04, 2.92729920e-03,\n",
       "        4.90584951e-04, 1.35444554e-03, 1.41350659e-03, 1.41387700e-03,\n",
       "        4.88310460e-04, 1.16750484e-03, 1.46820588e-03, 1.16664871e-03,\n",
       "        2.73340428e-06, 7.98225446e-04, 1.16592683e-03, 1.09353808e-03,\n",
       "        1.54797549e-03, 4.00209469e-04, 4.00018763e-04, 1.47130747e-03,\n",
       "        4.00308273e-04, 4.91403941e-04, 1.20010381e-03, 4.00356312e-04,\n",
       "        8.00013562e-04, 7.97118872e-04, 4.89068082e-04, 1.74596599e-03,\n",
       "        7.47283589e-04, 4.90663976e-04, 9.79283538e-04, 9.81530395e-04,\n",
       "        4.90310678e-04, 4.89395610e-04, 1.59969334e-03, 1.85455329e-03,\n",
       "        1.16279663e-03, 7.49462417e-04, 1.78776551e-03, 1.09528203e-03,\n",
       "        3.99947206e-04, 4.00260207e-04, 1.46809192e-03, 1.99949954e-03,\n",
       "        4.90569897e-04, 1.16534686e-03, 1.54828196e-03, 4.01687764e-04,\n",
       "        7.48685082e-04, 2.47863693e-03, 7.99536909e-04, 8.01403667e-04,\n",
       "        4.88035948e-04, 7.46646791e-04, 2.65185343e-03, 4.91266628e-04,\n",
       "        2.60326845e-03, 7.99658232e-04, 1.27259121e-03, 1.86083897e-03,\n",
       "        4.75260991e-06, 3.99562043e-04, 8.01717079e-04, 1.32360938e-03,\n",
       "        4.00519495e-04, 4.12876122e-04, 2.13981228e-03, 1.46887529e-03,\n",
       "        7.98828471e-04, 1.93947447e-03, 6.32355777e-04, 1.09671524e-03,\n",
       "        4.91776460e-04, 7.99901532e-04, 1.09536573e-03, 4.86812613e-04,\n",
       "        3.99971094e-04, 4.91644385e-04, 2.22727410e-03, 1.46983679e-03,\n",
       "        4.91527541e-04, 1.02030647e-03, 1.47192383e-03, 2.39893279e-03,\n",
       "        4.91795855e-04, 4.90310655e-04, 2.87151469e-03, 7.48162555e-04,\n",
       "        1.98675791e-06, 4.90154472e-04, 3.99732661e-04, 1.85467672e-03,\n",
       "        4.91984589e-04, 1.54683584e-03, 1.99766174e-03, 5.08081038e-03,\n",
       "        4.89576088e-04, 1.16426022e-03, 6.34144562e-04, 8.00575996e-04,\n",
       "        7.98693082e-04, 9.80894061e-04, 3.99072353e-04, 2.04053679e-03,\n",
       "        4.89901522e-04, 3.99640935e-04, 1.67408547e-03, 1.16691157e-03,\n",
       "        4.52367448e-07, 3.97116971e-04, 1.62410366e-03, 1.26587647e-03,\n",
       "        4.89699573e-04, 4.89745756e-04, 1.20125712e-03, 2.85558655e-03,\n",
       "        7.48320037e-04, 4.92478202e-04, 1.20153430e-03, 1.09536753e-03,\n",
       "        4.89085706e-04, 7.99989764e-04, 1.16275655e-03, 2.09769313e-03,\n",
       "        4.00018877e-04, 4.89804116e-04, 1.26342571e-03, 4.25054258e-06,\n",
       "        3.99310298e-04, 4.89628932e-04, 6.32487329e-04, 9.77877404e-04,\n",
       "        7.99930133e-04, 4.87783987e-04, 4.01723134e-04, 1.19872998e-03,\n",
       "        7.44425077e-04, 1.09567399e-03, 4.89823863e-04, 1.62697581e-03,\n",
       "        6.32862121e-04, 6.31505378e-04, 8.01054571e-04, 1.16626944e-03,\n",
       "        4.90331151e-04, 1.74187479e-03, 6.33166608e-04, 4.02169406e-04,\n",
       "        4.90995135e-04, 1.35737173e-03, 5.13569337e-07, 1.19845874e-03,\n",
       "        7.47426456e-04, 1.78854014e-03, 1.16635937e-03, 4.00328817e-04,\n",
       "        4.90310516e-04, 1.71882079e-03, 9.78794472e-04, 9.79862665e-04,\n",
       "        6.30382565e-04, 1.19767194e-03, 1.02027815e-03, 1.19698134e-03,\n",
       "        3.99041386e-04, 6.32715249e-04, 8.00504908e-04, 4.90622092e-04,\n",
       "        4.00304837e-04, 1.46974363e-03, 3.99406404e-04, 7.47493774e-04,\n",
       "        7.50345543e-04, 7.48391947e-04, 7.98520261e-04, 2.20894524e-06,\n",
       "        1.78416128e-07, 4.90018253e-04, 1.20213871e-03, 1.93797238e-03,\n",
       "        4.89804325e-04, 3.99804126e-04, 1.83376141e-03, 3.99781269e-04,\n",
       "        2.78041453e-07, 7.99088803e-04, 1.09567174e-03, 1.95931843e-03,\n",
       "        3.99875669e-04, 4.92956011e-04, 4.91111657e-04, 1.09458936e-03,\n",
       "        4.91385293e-04, 4.00066461e-04, 7.48506787e-04, 4.88798991e-04,\n",
       "        3.99947177e-04, 1.19903097e-03, 4.89881921e-04, 3.13554425e-06,\n",
       "        4.89999070e-04, 8.04701725e-04, 1.93953228e-03, 1.79123048e-03,\n",
       "        7.97748637e-04, 4.89008723e-04, 4.90373338e-04, 2.52579571e-03,\n",
       "        3.99331161e-04, 3.99415313e-04, 2.00068954e-03, 1.74152592e-03,\n",
       "        4.91207438e-04, 8.93140788e-04, 4.92362927e-04, 1.60028957e-03,\n",
       "        3.99665797e-04, 6.21719590e-07, 4.00836236e-04, 7.99641646e-04,\n",
       "        4.00018763e-04, 1.09562728e-03, 3.99947973e-04, 1.59904560e-03,\n",
       "        3.99187826e-04, 1.16567244e-03, 4.03049891e-04, 1.35648260e-03,\n",
       "        4.90234225e-04, 9.77000338e-04, 7.99846891e-04, 1.26625275e-03,\n",
       "        4.00042601e-04, 1.74289893e-03, 2.79951491e-03, 6.32032815e-04,\n",
       "        4.00066518e-04, 4.00329016e-04, 1.19895149e-03, 6.32720926e-04,\n",
       "        3.98739814e-04, 1.09580389e-03, 4.00477773e-04, 2.31494989e-03,\n",
       "        4.90115546e-04, 1.62475200e-03, 1.85426023e-03, 7.49666738e-04,\n",
       "        4.00382322e-04, 1.46822873e-03, 4.87998987e-04, 2.05972489e-03,\n",
       "        4.89203762e-04, 4.89914610e-04, 4.92300169e-04, 1.54837315e-03,\n",
       "        3.98533040e-04, 3.97944633e-04, 2.31542799e-03, 1.67335089e-03,\n",
       "        7.45782660e-04, 4.89842988e-04, 4.88995787e-04, 1.35594648e-03,\n",
       "        2.02866052e-06, 1.16628589e-03, 9.83283202e-04, 1.74407933e-03,\n",
       "        4.89570747e-04, 4.87825588e-04, 4.90446677e-04, 1.47240671e-03,\n",
       "        7.99551521e-04, 1.16733729e-03, 4.01926225e-04, 7.47635442e-04,\n",
       "        1.19819792e-03, 4.88793921e-04, 1.02094644e-03, 1.78949934e-03,\n",
       "        4.90957429e-04, 4.91305306e-04, 3.99995458e-04, 8.01099425e-04,\n",
       "        8.00132879e-04, 2.52828177e-03, 1.85905349e-06, 1.16698168e-03,\n",
       "        3.98627377e-04, 4.91794953e-04, 3.99137067e-04, 1.46819921e-03,\n",
       "        7.48239319e-04, 4.90115569e-04, 1.27041728e-03, 1.59886017e-03,\n",
       "        4.90198585e-04, 4.89823655e-04, 1.73636832e-06, 2.00214413e-03,\n",
       "        4.90174173e-04, 1.16422530e-03, 1.60136249e-03, 1.74368505e-03,\n",
       "        4.88815234e-04, 1.93926806e-06, 6.29394299e-04, 1.20048526e-03,\n",
       "        3.98923762e-04, 1.35851799e-03, 9.93378957e-07, 3.00486504e-03,\n",
       "        1.16602458e-03, 1.26512160e-03, 1.02045709e-03, 8.91004925e-04,\n",
       "        4.86108637e-04, 1.26746168e-03, 2.72910063e-03, 1.41134667e-03,\n",
       "        7.49938634e-04, 6.30076162e-04, 8.00467638e-04, 9.00973970e-04,\n",
       "        8.88681124e-04, 8.77332915e-03, 1.33024639e-03, 3.32532264e-03,\n",
       "        1.85776749e-03, 1.02030731e-03, 3.65544593e-03, 3.73911925e-03,\n",
       "        1.02050353e-03, 1.20226840e-03, 7.46475807e-04, 1.72351623e-03,\n",
       "        1.61929420e-03, 1.61996569e-03, 7.52093007e-04, 7.55677712e-04,\n",
       "        1.09841309e-03, 1.59984296e-03, 1.35691686e-03, 9.81337897e-04,\n",
       "        1.16835715e-03, 8.38223873e-03, 1.47149278e-03, 1.47052115e-03,\n",
       "        6.32938179e-04, 1.46875064e-03, 1.16480217e-03, 1.09507132e-03,\n",
       "        1.45728176e-06, 7.93970488e-04, 1.26384128e-03, 2.24949871e-03,\n",
       "        3.95014953e-04, 6.36415972e-04, 1.83361346e-03, 7.96765918e-04,\n",
       "        9.81232666e-04, 4.90174071e-04, 1.35237619e-03, 1.62519653e-03,\n",
       "        4.85553400e-04, 1.93652200e-03, 1.49852676e-03, 1.47254850e-03,\n",
       "        2.65618404e-03, 2.22736481e-03, 2.22635086e-03, 3.61286635e-03,\n",
       "        4.45364547e-03, 1.46885781e-03, 1.05465755e-02, 1.15231986e-02,\n",
       "        9.83648712e-04, 3.98324943e-04, 2.05626537e-03, 1.94128283e-03,\n",
       "        6.32870754e-04, 1.41471891e-03, 5.97985546e-03, 3.95058159e-03,\n",
       "        7.49679037e-04, 1.49398189e-03, 1.20070415e-03, 1.35739447e-03,\n",
       "        3.99734879e-04, 8.01636524e-04, 1.09088477e-03, 1.09772495e-03,\n",
       "        3.96185438e-04, 1.09188490e-03, 1.72094821e-03, 2.57662942e-03,\n",
       "        2.22720161e-03, 1.35375675e-03, 7.96511858e-04, 2.41666634e-03,\n",
       "        1.93936632e-03, 7.43826052e-04, 2.49915866e-03, 1.01899562e-03]),\n",
       " 'param_bootstrap': masked_array(data=[True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,\n",
       "                    5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4,\n",
       "                    4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2,\n",
       "                    2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1,\n",
       "                    2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1,\n",
       "                    1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5,\n",
       "                    1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,\n",
       "                    5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4,\n",
       "                    4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2,\n",
       "                    2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1,\n",
       "                    2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1,\n",
       "                    1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5,\n",
       "                    1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,\n",
       "                    5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4,\n",
       "                    4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400}],\n",
       " 'split0_test_score': array([0.62909091, 0.60483343, 0.63652174, 0.65863636, 0.78763285,\n",
       "        0.65863636, 0.80974747, 0.65863636, 0.67966387, 0.65809524,\n",
       "        0.65863636, 0.63652174, 0.63652174, 0.63652174, 0.63652174,\n",
       "        0.65863636, 0.65863636, 0.65863636, 0.65863636, 0.65863636,\n",
       "        0.7997619 , 0.6529972 , 0.83077498, 0.83077498, 0.67393939,\n",
       "        0.65188312, 0.77077498, 0.83077498, 0.77077498, 0.7997619 ,\n",
       "        0.78601307, 0.85292929, 0.65863636, 0.62909091, 0.62521645,\n",
       "        0.67966387, 0.79867495, 0.67966387, 0.7997619 , 0.83077498,\n",
       "        0.69142857, 0.82181818, 0.79292929, 0.7397619 , 0.62521645,\n",
       "        0.62521645, 0.70833333, 0.77077498, 0.77077498, 0.76181818,\n",
       "        0.77077498, 0.79292929, 0.7397619 , 0.69142857, 0.67966387,\n",
       "        0.76253968, 0.70833333, 0.7997619 , 0.7997619 , 0.67966387,\n",
       "        0.83077498, 0.77077498, 0.7997619 , 0.70833333, 0.7397619 ,\n",
       "        0.76253968, 0.70833333, 0.70833333, 0.66354978, 0.61378788,\n",
       "        0.83077498, 0.76181818, 0.64363636, 0.77077498, 0.77077498,\n",
       "        0.7397619 , 0.7147619 , 0.76181818, 0.62521645, 0.70181818,\n",
       "        0.65613445, 0.65613445, 0.65613445, 0.65863636, 0.65863636,\n",
       "        0.70777778, 0.65863636, 0.68430184, 0.65809524, 0.65613445,\n",
       "        0.67777778, 0.65613445, 0.75030303, 0.70603175, 0.65863636,\n",
       "        0.65613445, 0.68430184, 0.67966387, 0.68669082, 0.65613445,\n",
       "        0.67966387, 0.67966387, 0.77077498, 0.82888889, 0.77077498,\n",
       "        0.82888889, 0.83077498, 0.77077498, 0.80974747, 0.70181818,\n",
       "        0.67966387, 0.83077498, 0.67777778, 0.79803922, 0.67777778,\n",
       "        0.67777778, 0.6529972 , 0.76888889, 0.67777778, 0.67966387,\n",
       "        0.6529972 , 0.7397619 , 0.77077498, 0.78601307, 0.63953216,\n",
       "        0.73730994, 0.77077498, 0.73934641, 0.83077498, 0.66823529,\n",
       "        0.82888889, 0.83077498, 0.7397619 , 0.77077498, 0.77077498,\n",
       "        0.7997619 , 0.83077498, 0.66823529, 0.77077498, 0.79803922,\n",
       "        0.78601307, 0.78601307, 0.65188312, 0.66823529, 0.85292929,\n",
       "        0.70833333, 0.76253968, 0.77077498, 0.82181818, 0.77077498,\n",
       "        0.67966387, 0.6529972 , 0.67204482, 0.7997619 , 0.77077498,\n",
       "        0.79803922, 0.58878788, 0.83077498, 0.83077498, 0.76888889,\n",
       "        0.67777778, 0.67777778, 0.67777778, 0.67777778, 0.75271222,\n",
       "        0.65863636, 0.65613445, 0.63363636, 0.80974747, 0.67777778,\n",
       "        0.65863636, 0.67777778, 0.68669082, 0.65613445, 0.83541295,\n",
       "        0.67777778, 0.67777778, 0.65613445, 0.67966387, 0.63652174,\n",
       "        0.74974747, 0.77077498, 0.76888889, 0.83077498, 0.74974747,\n",
       "        0.67966387, 0.83077498, 0.83077498, 0.7898226 , 0.82888889,\n",
       "        0.83077498, 0.83077498, 0.67204482, 0.77077498, 0.77077498,\n",
       "        0.83077498, 0.67966387, 0.6529972 , 0.83077498, 0.77077498,\n",
       "        0.82888889, 0.83077498, 0.77077498, 0.66823529, 0.61378788,\n",
       "        0.85292929, 0.77077498, 0.83077498, 0.82888889, 0.83077498,\n",
       "        0.77077498, 0.78601307, 0.65111111, 0.65111111, 0.6529972 ,\n",
       "        0.7397619 , 0.76181818, 0.77077498, 0.7397619 , 0.67966387,\n",
       "        0.80920635, 0.83077498, 0.73934641, 0.83077498, 0.83077498,\n",
       "        0.77077498, 0.73934641, 0.78601307, 0.67204482, 0.83077498,\n",
       "        0.77077498, 0.77077498, 0.67966387, 0.7997619 , 0.83077498,\n",
       "        0.67966387, 0.80920635, 0.65111111, 0.83077498, 0.77077498,\n",
       "        0.65613445, 0.65863636, 0.65863636, 0.65863636, 0.65863636,\n",
       "        0.65863636, 0.65863636, 0.65863636, 0.66186869, 0.65863636,\n",
       "        0.65863636, 0.80974747, 0.65863636, 0.65863636, 0.65863636,\n",
       "        0.65863636, 0.80974747, 0.6810101 , 0.65863636, 0.65863636,\n",
       "        0.73934641, 0.755     , 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.78601307, 0.83077498, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.83077498, 0.70833333, 0.83077498, 0.77077498, 0.77077498,\n",
       "        0.77077498, 0.77077498, 0.77077498, 0.77077498, 0.77077498,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.71849206, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.70833333, 0.71849206, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.78601307, 0.73934641,\n",
       "        0.73934641, 0.70833333, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.67777778, 0.68430184, 0.67777778, 0.67777778, 0.80724556,\n",
       "        0.67777778, 0.68430184, 0.67777778, 0.67777778, 0.68430184,\n",
       "        0.67777778, 0.67777778, 0.70603175, 0.70603175, 0.65613445,\n",
       "        0.70603175, 0.63363636, 0.65613445, 0.70603175, 0.70603175,\n",
       "        0.78397661, 0.77077498, 0.78601307, 0.83077498, 0.76888889,\n",
       "        0.73934641, 0.73934641, 0.76888889, 0.73934641, 0.76888889,\n",
       "        0.82888889, 0.83077498, 0.76888889, 0.76888889, 0.76888889,\n",
       "        0.76888889, 0.83077498, 0.77077498, 0.77077498, 0.76888889,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.73934641, 0.76253968,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.73730994, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.80920635, 0.73934641, 0.77077498, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.76253968, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.73730994, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.67777778, 0.70603175, 0.67777778, 0.70603175, 0.70603175,\n",
       "        0.70603175, 0.85714286, 0.67777778, 0.67777778, 0.70603175,\n",
       "        0.65613445, 0.65613445, 0.70603175, 0.70603175, 0.70603175,\n",
       "        0.67777778, 0.68430184, 0.70603175, 0.67777778, 0.67777778,\n",
       "        0.83077498, 0.77077498, 0.77077498, 0.83077498, 0.76888889,\n",
       "        0.82888889, 0.83077498, 0.76888889, 0.77077498, 0.82888889,\n",
       "        0.73934641, 0.73934641, 0.83077498, 0.76888889, 0.82888889,\n",
       "        0.77077498, 0.76888889, 0.77077498, 0.76888889, 0.77077498,\n",
       "        0.78601307, 0.76253968, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.71849206,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73730994, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641]),\n",
       " 'split1_test_score': array([0.66833333, 0.66833333, 0.62208791, 0.646     , 0.646     ,\n",
       "        0.63034188, 0.66833333, 0.62208791, 0.58776224, 0.65377778,\n",
       "        0.646     , 0.62208791, 0.646     , 0.64285714, 0.646     ,\n",
       "        0.646     , 0.62208791, 0.61933333, 0.646     , 0.62208791,\n",
       "        0.69812253, 0.66797101, 0.64285714, 0.64285714, 0.66384615,\n",
       "        0.78939959, 0.69166667, 0.66797101, 0.66175889, 0.65377778,\n",
       "        0.66833333, 0.66384615, 0.69812253, 0.64333333, 0.64285714,\n",
       "        0.66797101, 0.79761905, 0.72005348, 0.66797101, 0.64285714,\n",
       "        0.61212121, 0.82309524, 0.66797101, 0.64285714, 0.63878788,\n",
       "        0.63800764, 0.82273292, 0.66797101, 0.60952381, 0.69166667,\n",
       "        0.66797101, 0.64285714, 0.69166667, 0.79761905, 0.63800764,\n",
       "        0.63878788, 0.64285714, 0.66384615, 0.69166667, 0.66797101,\n",
       "        0.61431169, 0.61431169, 0.69166667, 0.66797101, 0.72005348,\n",
       "        0.69652406, 0.66797101, 0.81860806, 0.69166667, 0.66797101,\n",
       "        0.63800764, 0.69166667, 0.66175889, 0.66797101, 0.66797101,\n",
       "        0.64285714, 0.66797101, 0.69166667, 0.66797101, 0.64130435,\n",
       "        0.646     , 0.65377778, 0.66833333, 0.66833333, 0.82309524,\n",
       "        0.646     , 0.66833333, 0.66833333, 0.67581699, 0.61431169,\n",
       "        0.68953964, 0.646     , 0.63034188, 0.646     , 0.646     ,\n",
       "        0.66833333, 0.63034188, 0.79354978, 0.67581699, 0.66833333,\n",
       "        0.60952381, 0.72005348, 0.68953964, 0.68953964, 0.76907359,\n",
       "        0.60952381, 0.85288443, 0.63463768, 0.79809524, 0.81860806,\n",
       "        0.68953964, 0.69166667, 0.63878788, 0.66175889, 0.68953964,\n",
       "        0.66833333, 0.72005348, 0.66175889, 0.84430155, 0.68953964,\n",
       "        0.79276955, 0.69812253, 0.79276955, 0.60952381, 0.78939959,\n",
       "        0.66797101, 0.69166667, 0.66175889, 0.64285714, 0.68953964,\n",
       "        0.84642857, 0.66833333, 0.64285714, 0.78939959, 0.69166667,\n",
       "        0.72005348, 0.66384615, 0.63463768, 0.60952381, 0.66384615,\n",
       "        0.84642857, 0.66797101, 0.66384615, 0.66175889, 0.87481538,\n",
       "        0.79761905, 0.63800764, 0.63800764, 0.64285714, 0.84642857,\n",
       "        0.60952381, 0.63878788, 0.81860806, 0.69166667, 0.84642857,\n",
       "        0.66833333, 0.79354978, 0.66175889, 0.63878788, 0.66175889,\n",
       "        0.66833333, 0.62208791, 0.66833333, 0.646     , 0.65377778,\n",
       "        0.59603989, 0.66833333, 0.66833333, 0.80338083, 0.646     ,\n",
       "        0.646     , 0.66833333, 0.66833333, 0.646     , 0.646     ,\n",
       "        0.646     , 0.58776224, 0.68953964, 0.62208791, 0.66833333,\n",
       "        0.66666667, 0.68953964, 0.66175889, 0.66384615, 0.63878788,\n",
       "        0.63463768, 0.68953964, 0.69812253, 0.82309524, 0.66833333,\n",
       "        0.68953964, 0.68953964, 0.78939959, 0.78939959, 0.68953964,\n",
       "        0.66833333, 0.63675889, 0.68953964, 0.68953964, 0.66175889,\n",
       "        0.61212121, 0.66175889, 0.64285714, 0.66797101, 0.67312253,\n",
       "        0.66797101, 0.84642857, 0.66175889, 0.84642857, 0.78939959,\n",
       "        0.63463768, 0.63463768, 0.82273292, 0.66797101, 0.68953964,\n",
       "        0.66175889, 0.66384615, 0.66384615, 0.63800764, 0.69166667,\n",
       "        0.70601023, 0.63800764, 0.69166667, 0.66384615, 0.61431169,\n",
       "        0.78939959, 0.84642857, 0.63800764, 0.80324675, 0.63878788,\n",
       "        0.61025641, 0.69166667, 0.66175889, 0.84642857, 0.69166667,\n",
       "        0.66384615, 0.84642857, 0.63878788, 0.68953964, 0.78939959,\n",
       "        0.59545455, 0.64333333, 0.61431169, 0.61431169, 0.61431169,\n",
       "        0.63878788, 0.61431169, 0.61431169, 0.59545455, 0.59545455,\n",
       "        0.59545455, 0.61431169, 0.59545455, 0.63878788, 0.61431169,\n",
       "        0.61933333, 0.58241026, 0.61933333, 0.59545455, 0.59545455,\n",
       "        0.66175889, 0.78939959, 0.63463768, 0.79908425, 0.89773292,\n",
       "        0.66175889, 0.78939959, 0.63463768, 0.63463768, 0.63571429,\n",
       "        0.78939959, 0.63463768, 0.7732111 , 0.66384615, 0.78939959,\n",
       "        0.68953964, 0.63878788, 0.63463768, 0.81860806, 0.8165208 ,\n",
       "        0.79276955, 0.66384615, 0.81860806, 0.66384615, 0.69166667,\n",
       "        0.81860806, 0.81860806, 0.66384615, 0.63800764, 0.89809524,\n",
       "        0.63463768, 0.69166667, 0.66384615, 0.66384615, 0.81860806,\n",
       "        0.81860806, 0.79908425, 0.69166667, 0.66384615, 0.81860806,\n",
       "        0.81      , 0.87225673, 0.81860806, 0.66384615, 0.78416149,\n",
       "        0.81860806, 0.79276955, 0.81860806, 0.63800764, 0.78416149,\n",
       "        0.66384615, 0.89809524, 0.69652406, 0.89809524, 0.81860806,\n",
       "        0.81860806, 0.59057971, 0.81860806, 0.81860806, 0.81860806,\n",
       "        0.8007619 , 0.8007619 , 0.68953964, 0.646     , 0.82309524,\n",
       "        0.646     , 0.646     , 0.8007619 , 0.646     , 0.66959596,\n",
       "        0.646     , 0.82309524, 0.646     , 0.646     , 0.646     ,\n",
       "        0.646     , 0.8007619 , 0.646     , 0.646     , 0.646     ,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.66833333, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.8007619 ,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.8007619 , 0.8007619 ,\n",
       "        0.82309524, 0.82309524, 0.85288443, 0.85288443, 0.66833333,\n",
       "        0.85288443, 0.82309524, 0.85288443, 0.85288443, 0.82309524,\n",
       "        0.69812253, 0.82309524, 0.85288443, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.82991342, 0.8007619 , 0.82309524,\n",
       "        0.66797101, 0.85288443, 0.85288443, 0.82309524, 0.82273292,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.66797101, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.79354978, 0.85288443, 0.85288443,\n",
       "        0.82309524, 0.85288443, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.646     , 0.8007619 , 0.646     , 0.646     , 0.646     ,\n",
       "        0.65065934, 0.66833333, 0.646     , 0.8007619 , 0.65065934,\n",
       "        0.646     , 0.646     , 0.8007619 , 0.646     , 0.646     ,\n",
       "        0.646     , 0.8007619 , 0.646     , 0.646     , 0.8007619 ,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.85288443,\n",
       "        0.82309524, 0.66833333, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.8007619 , 0.82309524, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.82991342, 0.82309524, 0.8007619 , 0.82309524,\n",
       "        0.68953964, 0.85288443, 0.82309524, 0.82309524, 0.79354978,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.82309524,\n",
       "        0.85288443, 0.82309524, 0.85288443, 0.82309524, 0.82309524,\n",
       "        0.85288443, 0.82309524, 0.85288443, 0.82309524, 0.85288443,\n",
       "        0.82273292, 0.85288443, 0.85288443, 0.85288443, 0.82788443,\n",
       "        0.85288443, 0.85288443, 0.82273292, 0.82309524, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.86077213, 0.85288443,\n",
       "        0.85288443, 0.82309524, 0.82309524, 0.85288443, 0.82309524]),\n",
       " 'split2_test_score': array([0.59823529, 0.628815  , 0.59823529, 0.58823529, 0.59823529,\n",
       "        0.58823529, 0.59823529, 0.59823529, 0.58823529, 0.58823529,\n",
       "        0.58823529, 0.628815  , 0.58823529, 0.58823529, 0.63823529,\n",
       "        0.59823529, 0.59823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.66338681, 0.63845174, 0.63672014, 0.81338681, 0.63823529,\n",
       "        0.63845174, 0.81338681, 0.66338681, 0.66338681, 0.693863  ,\n",
       "        0.66338681, 0.66338681, 0.63845174, 0.66338681, 0.693863  ,\n",
       "        0.66338681, 0.693863  , 0.66338681, 0.63845174, 0.66338681,\n",
       "        0.75338681, 0.75338681, 0.75338681, 0.67204482, 0.63672014,\n",
       "        0.81338681, 0.75338681, 0.63672014, 0.67214834, 0.66338681,\n",
       "        0.66338681, 0.693863  , 0.63672014, 0.693863  , 0.66338681,\n",
       "        0.66338681, 0.66338681, 0.66338681, 0.66338681, 0.693863  ,\n",
       "        0.81338681, 0.66338681, 0.63672014, 0.67204482, 0.78871148,\n",
       "        0.63672014, 0.6129972 , 0.66338681, 0.693863  , 0.66338681,\n",
       "        0.66338681, 0.693863  , 0.66338681, 0.6129972 , 0.693863  ,\n",
       "        0.66338681, 0.81338681, 0.693863  , 0.66338681, 0.66338681,\n",
       "        0.59823529, 0.63823529, 0.59823529, 0.59823529, 0.628815  ,\n",
       "        0.63823529, 0.63823529, 0.59823529, 0.63823529, 0.59823529,\n",
       "        0.63823529, 0.59823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.63823529, 0.628815  , 0.59823529, 0.58823529, 0.59823529,\n",
       "        0.81338681, 0.67214834, 0.66338681, 0.81338681, 0.66338681,\n",
       "        0.81338681, 0.81338681, 0.67214834, 0.66338681, 0.67214834,\n",
       "        0.81338681, 0.67214834, 0.843863  , 0.843863  , 0.81338681,\n",
       "        0.66338681, 0.67214834, 0.843863  , 0.66338681, 0.67214834,\n",
       "        0.6129972 , 0.81338681, 0.843863  , 0.78871148, 0.71823529,\n",
       "        0.66338681, 0.843863  , 0.67204482, 0.76368984, 0.66338681,\n",
       "        0.81338681, 0.75338681, 0.81338681, 0.693863  , 0.78845174,\n",
       "        0.843863  , 0.628815  , 0.81338681, 0.66338681, 0.81338681,\n",
       "        0.75338681, 0.843863  , 0.81338681, 0.76368984, 0.81823529,\n",
       "        0.81338681, 0.68005348, 0.75338681, 0.66338681, 0.81338681,\n",
       "        0.66338681, 0.75338681, 0.66338681, 0.81338681, 0.843863  ,\n",
       "        0.81338681, 0.64548167, 0.66338681, 0.843863  , 0.81338681,\n",
       "        0.59823529, 0.59823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.59823529, 0.59823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.59823529, 0.59823529, 0.59823529, 0.58823529, 0.63823529,\n",
       "        0.59823529, 0.58823529, 0.63823529, 0.59823529, 0.59823529,\n",
       "        0.81338681, 0.66338681, 0.67214834, 0.81338681, 0.66338681,\n",
       "        0.75338681, 0.81338681, 0.81338681, 0.66338681, 0.628815  ,\n",
       "        0.66338681, 0.64548167, 0.72966387, 0.66338681, 0.81338681,\n",
       "        0.81338681, 0.6615873 , 0.81338681, 0.63823529, 0.81338681,\n",
       "        0.81338681, 0.75338681, 0.81338681, 0.75338681, 0.67214834,\n",
       "        0.75338681, 0.78871148, 0.78871148, 0.75338681, 0.66338681,\n",
       "        0.67204482, 0.75338681, 0.67204482, 0.75338681, 0.843863  ,\n",
       "        0.66338681, 0.85292929, 0.81338681, 0.81338681, 0.66338681,\n",
       "        0.67204482, 0.843863  , 0.843863  , 0.75338681, 0.78871148,\n",
       "        0.72966387, 0.78871148, 0.67204482, 0.75338681, 0.63672014,\n",
       "        0.843863  , 0.81338681, 0.67214834, 0.843863  , 0.843863  ,\n",
       "        0.64548167, 0.693863  , 0.67214834, 0.81338681, 0.843863  ,\n",
       "        0.66338681, 0.63823529, 0.63823529, 0.63823529, 0.59823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.81823529, 0.843863  , 0.843863  , 0.70187166, 0.693863  ,\n",
       "        0.843863  , 0.843863  , 0.70187166, 0.81823529, 0.843863  ,\n",
       "        0.70187166, 0.63823529, 0.6473262 , 0.843863  , 0.843863  ,\n",
       "        0.70187166, 0.6473262 , 0.75338681, 0.81823529, 0.843863  ,\n",
       "        0.65511841, 0.76368984, 0.76368984, 0.76368984, 0.65511841,\n",
       "        0.81823529, 0.67633053, 0.76368984, 0.70187166, 0.76368984,\n",
       "        0.76368984, 0.76368984, 0.70187166, 0.76368984, 0.81823529,\n",
       "        0.81823529, 0.62187166, 0.81823529, 0.81823529, 0.81823529,\n",
       "        0.64702317, 0.76368984, 0.76368984, 0.76368984, 0.76368984,\n",
       "        0.76368984, 0.76368984, 0.65511841, 0.81823529, 0.64702317,\n",
       "        0.65511841, 0.65511841, 0.67633053, 0.76368984, 0.843863  ,\n",
       "        0.843863  , 0.81823529, 0.81823529, 0.81823529, 0.76368984,\n",
       "        0.64666667, 0.67777778, 0.63823529, 0.67777778, 0.63777778,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.67777778,\n",
       "        0.63823529, 0.63823529, 0.63777778, 0.63823529, 0.67777778,\n",
       "        0.63823529, 0.63823529, 0.67777778, 0.67777778, 0.67214834,\n",
       "        0.68005348, 0.71047619, 0.843863  , 0.88340548, 0.85292929,\n",
       "        0.73229437, 0.88340548, 0.88340548, 0.78871148, 0.74141414,\n",
       "        0.73229437, 0.82825397, 0.81823529, 0.88340548, 0.843863  ,\n",
       "        0.88340548, 0.78871148, 0.88340548, 0.73229437, 0.85292929,\n",
       "        0.88340548, 0.71047619, 0.843863  , 0.82825397, 0.76920635,\n",
       "        0.65511841, 0.88340548, 0.82825397, 0.80323232, 0.82825397,\n",
       "        0.88340548, 0.88340548, 0.67688312, 0.82825397, 0.88340548,\n",
       "        0.73229437, 0.70666667, 0.82825397, 0.85777778, 0.88340548,\n",
       "        0.78871148, 0.82825397, 0.88340548, 0.88340548, 0.74141414,\n",
       "        0.82825397, 0.82825397, 0.82825397, 0.88340548, 0.85777778,\n",
       "        0.80323232, 0.82825397, 0.76920635, 0.74141414, 0.88340548,\n",
       "        0.82825397, 0.82825397, 0.70666667, 0.82825397, 0.78871148,\n",
       "        0.63823529, 0.63823529, 0.71169082, 0.63823529, 0.63823529,\n",
       "        0.67777778, 0.67777778, 0.63823529, 0.59823529, 0.63823529,\n",
       "        0.63823529, 0.67777778, 0.67777778, 0.71169082, 0.63823529,\n",
       "        0.67777778, 0.63823529, 0.63823529, 0.63823529, 0.71169082,\n",
       "        0.78871148, 0.88340548, 0.88340548, 0.88340548, 0.85292929,\n",
       "        0.85777778, 0.88340548, 0.88340548, 0.72966387, 0.88340548,\n",
       "        0.74141414, 0.88340548, 0.88340548, 0.68502415, 0.73229437,\n",
       "        0.88340548, 0.88340548, 0.71587302, 0.73229437, 0.843863  ,\n",
       "        0.71959596, 0.75777778, 0.82825397, 0.88340548, 0.80323232,\n",
       "        0.80323232, 0.85777778, 0.82825397, 0.80323232, 0.82825397,\n",
       "        0.88340548, 0.88340548, 0.82825397, 0.80323232, 0.78871148,\n",
       "        0.82825397, 0.71047619, 0.70666667, 0.843863  , 0.88340548,\n",
       "        0.66141414, 0.82825397, 0.71047619, 0.82825397, 0.71959596,\n",
       "        0.82825397, 0.88340548, 0.78871148, 0.78871148, 0.80323232,\n",
       "        0.68545455, 0.82825397, 0.82825397, 0.71047619, 0.80323232,\n",
       "        0.82825397, 0.693863  , 0.843863  , 0.88340548, 0.88340548]),\n",
       " 'split3_test_score': array([0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.62494279, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.62494279, 0.61341615, 0.60545455, 0.60545455,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.69984962, 0.60545455, 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.6738756 , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.6738756 , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.60545455, 0.61341615, 0.60545455, 0.60545455, 0.63270677,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.61341615, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.62494279, 0.60545455, 0.60545455,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.64571429, 0.68      ,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.6738756 ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.61341615, 0.60545455, 0.60545455, 0.61341615, 0.60545455,\n",
       "        0.60545455, 0.61341615, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.64571429, 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.62494279, 0.60545455, 0.60545455, 0.61341615,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.62494279, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.64571429, 0.68      , 0.68      , 0.64571429, 0.64571429,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.64571429, 0.64571429, 0.64571429,\n",
       "        0.68      , 0.6738756 , 0.6738756 , 0.6738756 , 0.68      ,\n",
       "        0.6738756 , 0.6738756 , 0.68      , 0.6738756 , 0.6738756 ,\n",
       "        0.68      , 0.6738756 , 0.68      , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.68      , 0.68      , 0.68      , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.64080201, 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.68      ,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.61341615,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.61341615, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.64571429, 0.68      ,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.64571429, 0.64571429,\n",
       "        0.64571429, 0.64571429, 0.64571429, 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.64571429, 0.64571429, 0.68      ,\n",
       "        0.68      , 0.6738756 , 0.6738756 , 0.68      , 0.64080201,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.6738756 ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.64080201, 0.6738756 , 0.64080201, 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.61341615, 0.61341615, 0.60545455, 0.61341615, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.64571429, 0.64571429, 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.64571429, 0.64571429, 0.64571429,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.64571429,\n",
       "        0.64571429, 0.64571429, 0.64571429, 0.64571429, 0.64571429,\n",
       "        0.64571429, 0.64080201, 0.68      , 0.68      , 0.64571429,\n",
       "        0.6738756 , 0.64080201, 0.6738756 , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.6738756 , 0.68      , 0.68      ,\n",
       "        0.68      , 0.6738756 , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.64571429, 0.6738756 , 0.64571429, 0.64571429, 0.68      ,\n",
       "        0.68      , 0.6738756 , 0.6738756 , 0.6738756 , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ]),\n",
       " 'split4_test_score': array([0.61984127, 0.61984127, 0.66555556, 0.61206349, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61206349, 0.61984127,\n",
       "        0.61206349, 0.61206349, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.61984127, 0.6557265 ,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61206349, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.61206349,\n",
       "        0.66555556, 0.61984127, 0.61206349, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.61206349, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.6557265 , 0.6557265 , 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61239316, 0.61984127, 0.6557265 ,\n",
       "        0.6557265 , 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.6557265 , 0.66555556,\n",
       "        0.6557265 , 0.6557265 , 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61206349, 0.63786214, 0.61206349, 0.61984127, 0.61239316,\n",
       "        0.60247073, 0.66555556, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.60247073, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61239316, 0.61984127, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.60461538, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.6557265 , 0.61239316, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.61239316, 0.61239316, 0.66555556, 0.61984127,\n",
       "        0.61239316, 0.66555556, 0.66555556, 0.61984127, 0.6557265 ,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.61206349, 0.61984127,\n",
       "        0.6557265 , 0.61239316, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.6557265 , 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.61984127, 0.6557265 , 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61239316, 0.61206349, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.61984127, 0.6557265 ,\n",
       "        0.61984127, 0.6557265 , 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.6557265 , 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.6557265 , 0.61984127, 0.61984127,\n",
       "        0.61206349, 0.70017094, 0.66555556, 0.66239316, 0.64206349,\n",
       "        0.67619048, 0.61984127, 0.66555556, 0.66239316, 0.65461538,\n",
       "        0.66239316, 0.66239316, 0.65461538, 0.65461538, 0.66239316,\n",
       "        0.70017094, 0.66555556, 0.65461538, 0.65461538, 0.66239316,\n",
       "        0.66555556, 0.61206349, 0.66555556, 0.66555556, 0.70017094,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66239316, 0.66239316, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.70017094, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61206349, 0.61984127, 0.66555556, 0.66555556, 0.61206349,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.64531025, 0.61206349,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.68937729, 0.61206349, 0.66555556,\n",
       "        0.66555556, 0.66239316, 0.61984127, 0.66239316, 0.70017094,\n",
       "        0.66555556, 0.70017094, 0.70017094, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.66239316,\n",
       "        0.66555556, 0.66239316, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61206349, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.69920635, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.70017094, 0.70017094, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.6557265 , 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.6557265 , 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.70017094, 0.61984127, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.70017094, 0.66239316, 0.61984127,\n",
       "        0.70017094, 0.66555556, 0.61206349, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.61984127, 0.61206349,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.6557265 ,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.61984127, 0.61206349,\n",
       "        0.66555556, 0.66555556, 0.61239316, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61239316, 0.66555556, 0.66555556, 0.66555556]),\n",
       " 'mean_test_score': array([0.62419107, 0.62545552, 0.62557101, 0.62207794, 0.65143279,\n",
       "        0.62050187, 0.66946524, 0.62085108, 0.61619144, 0.62508083,\n",
       "        0.62363349, 0.62254409, 0.62310822, 0.61702644, 0.62921057,\n",
       "        0.62407794, 0.62319317, 0.62189248, 0.62563349, 0.62085108,\n",
       "        0.68536536, 0.65413796, 0.69118156, 0.71737204, 0.66234947,\n",
       "        0.67591514, 0.7242768 , 0.69239481, 0.67759683, 0.6917345 ,\n",
       "        0.69265775, 0.70514356, 0.67212316, 0.64136423, 0.64394287,\n",
       "        0.67131545, 0.71799965, 0.67103353, 0.69034804, 0.68737204,\n",
       "        0.68049843, 0.74877116, 0.71196853, 0.67090103, 0.63325601,\n",
       "        0.68443329, 0.72600172, 0.67228104, 0.66360054, 0.68334259,\n",
       "        0.68039481, 0.68589814, 0.68077504, 0.70372742, 0.66532277,\n",
       "        0.67291113, 0.66288371, 0.68387761, 0.69093133, 0.67544488,\n",
       "        0.71883999, 0.67880581, 0.69474085, 0.6719238 , 0.71881648,\n",
       "        0.68704301, 0.66697142, 0.70717675, 0.67696119, 0.65814025,\n",
       "        0.69357919, 0.69661487, 0.65372467, 0.67945975, 0.69563291,\n",
       "        0.66761387, 0.70279637, 0.68788227, 0.65128311, 0.6597805 ,\n",
       "        0.62165901, 0.64542385, 0.62959978, 0.63010016, 0.68176179,\n",
       "        0.64346178, 0.63810016, 0.63523326, 0.64863153, 0.61879545,\n",
       "        0.64616971, 0.62513311, 0.6373611 , 0.63670489, 0.62563349,\n",
       "        0.63759978, 0.64289377, 0.6632466 , 0.63520778, 0.62959978,\n",
       "        0.67899353, 0.67434139, 0.6938514 , 0.73547418, 0.69375819,\n",
       "        0.71032816, 0.76852036, 0.67548045, 0.71649987, 0.69848317,\n",
       "        0.70562917, 0.70802911, 0.68900881, 0.72070048, 0.6961091 ,\n",
       "        0.66186784, 0.67129377, 0.72401327, 0.69020434, 0.66823862,\n",
       "        0.67204095, 0.70873288, 0.75059262, 0.69681793, 0.69731964,\n",
       "        0.68284466, 0.72122918, 0.68374113, 0.70743265, 0.67334346,\n",
       "        0.75085197, 0.71046728, 0.69231228, 0.71991862, 0.71014693,\n",
       "        0.73270393, 0.68316586, 0.68173059, 0.67784823, 0.70816555,\n",
       "        0.73564432, 0.72868053, 0.69493433, 0.67870506, 0.77634129,\n",
       "        0.72383609, 0.68523127, 0.701545  , 0.68558068, 0.75522918,\n",
       "        0.65048315, 0.67814549, 0.69306191, 0.71937577, 0.74532442,\n",
       "        0.72309717, 0.6640425 , 0.70029525, 0.72265343, 0.71791803,\n",
       "        0.63392844, 0.62467936, 0.63392844, 0.62946178, 0.64600422,\n",
       "        0.61564147, 0.62959978, 0.62510016, 0.68733188, 0.62946178,\n",
       "        0.62722582, 0.63392844, 0.63571105, 0.62472543, 0.66898881,\n",
       "        0.62946178, 0.61740655, 0.64184104, 0.62505658, 0.62567724,\n",
       "        0.70016221, 0.68470854, 0.68967034, 0.72156984, 0.67035269,\n",
       "        0.67350593, 0.7358514 , 0.72156797, 0.72437204, 0.69431856,\n",
       "        0.7058514 , 0.69312751, 0.70733276, 0.71382339, 0.71470854,\n",
       "        0.73161013, 0.65589017, 0.68429584, 0.69167824, 0.70229525,\n",
       "        0.71084764, 0.70915239, 0.7145149 , 0.68702973, 0.65178   ,\n",
       "        0.72396853, 0.74115126, 0.71621732, 0.75288615, 0.72582339,\n",
       "        0.68460261, 0.69477577, 0.68914602, 0.6676049 , 0.69724822,\n",
       "        0.66609263, 0.71419736, 0.70115714, 0.69819953, 0.67605458,\n",
       "        0.70656339, 0.73164024, 0.71494347, 0.70956984, 0.70704779,\n",
       "        0.71793594, 0.74204259, 0.67232422, 0.70570393, 0.69036771,\n",
       "        0.70494713, 0.7242768 , 0.66985952, 0.76712181, 0.73322918,\n",
       "        0.65776659, 0.7390107 , 0.65955476, 0.72670854, 0.74077577,\n",
       "        0.62649877, 0.6491661 , 0.63643869, 0.63580621, 0.62374028,\n",
       "        0.64346091, 0.63119348, 0.63643869, 0.63268125, 0.63207155,\n",
       "        0.63203478, 0.66602843, 0.63047923, 0.63914589, 0.63580621,\n",
       "        0.6443661 , 0.66417827, 0.63972973, 0.63047923, 0.63203478,\n",
       "        0.70612209, 0.73606522, 0.72201386, 0.71031443, 0.73536551,\n",
       "        0.72058096, 0.75506148, 0.68428226, 0.70755499, 0.71289585,\n",
       "        0.72666321, 0.66535237, 0.71188395, 0.72417546, 0.74991862,\n",
       "        0.70154837, 0.67363178, 0.69401386, 0.74377763, 0.74848572,\n",
       "        0.70655798, 0.70126271, 0.73221509, 0.70126271, 0.68633741,\n",
       "        0.74312418, 0.71474323, 0.70248759, 0.6795605 , 0.74811253,\n",
       "        0.6966459 , 0.70682681, 0.68392134, 0.69709184, 0.74312418,\n",
       "        0.73398133, 0.70809465, 0.71896078, 0.71339668, 0.74312418,\n",
       "        0.69646173, 0.73380197, 0.73221509, 0.70126271, 0.71462737,\n",
       "        0.72560037, 0.71790453, 0.71050081, 0.70295504, 0.69129403,\n",
       "        0.67954842, 0.72639824, 0.69032643, 0.75744586, 0.74824972,\n",
       "        0.74824972, 0.68217304, 0.74788853, 0.73242577, 0.73343997,\n",
       "        0.67924329, 0.68613785, 0.64616971, 0.65388065, 0.71634113,\n",
       "        0.64660463, 0.65483252, 0.68448009, 0.63746178, 0.66053714,\n",
       "        0.64660463, 0.68202368, 0.64461339, 0.65225543, 0.64955199,\n",
       "        0.65225543, 0.66809625, 0.64104161, 0.66016392, 0.64989518,\n",
       "        0.71967903, 0.71928198, 0.74370537, 0.76970911, 0.74895094,\n",
       "        0.72120117, 0.75828054, 0.75733189, 0.70826237, 0.72893362,\n",
       "        0.72996681, 0.7586788 , 0.735155  , 0.76418903, 0.74495673,\n",
       "        0.76418903, 0.76455053, 0.76463218, 0.72302022, 0.75362713,\n",
       "        0.75828054, 0.73180313, 0.75313919, 0.75320807, 0.70128739,\n",
       "        0.71172382, 0.76761387, 0.75320807, 0.74779645, 0.74602535,\n",
       "        0.73328599, 0.75142339, 0.72096809, 0.74725023, 0.75509552,\n",
       "        0.73401615, 0.7268626 , 0.73947101, 0.75497404, 0.74913768,\n",
       "        0.70709201, 0.75198319, 0.7630135 , 0.75705566, 0.73791826,\n",
       "        0.75198319, 0.75000713, 0.74284034, 0.71941609, 0.74401214,\n",
       "        0.74820374, 0.75320807, 0.73200781, 0.73584011, 0.75509552,\n",
       "        0.74684294, 0.75320807, 0.72889061, 0.73720807, 0.74529958,\n",
       "        0.65512003, 0.67565727, 0.66129574, 0.65384775, 0.65225543,\n",
       "        0.66109579, 0.69485281, 0.65352771, 0.66892454, 0.64404444,\n",
       "        0.64919905, 0.65018447, 0.68041789, 0.66694653, 0.65225543,\n",
       "        0.65451313, 0.67886183, 0.65225543, 0.64660463, 0.69224812,\n",
       "        0.75077031, 0.75770911, 0.74856625, 0.77656625, 0.76405163,\n",
       "        0.76192063, 0.72961387, 0.75733189, 0.71781793, 0.75863348,\n",
       "        0.7254156 , 0.75828054, 0.76970911, 0.72451277, 0.73910967,\n",
       "        0.75770911, 0.74955267, 0.71505976, 0.722643  , 0.74065775,\n",
       "        0.7012837 , 0.73591189, 0.75658357, 0.75828054, 0.72751386,\n",
       "        0.73783601, 0.76060657, 0.75198319, 0.73906089, 0.73655182,\n",
       "        0.76423838, 0.76761387, 0.74135071, 0.7422459 , 0.73934174,\n",
       "        0.75320807, 0.7224698 , 0.71974776, 0.7435149 , 0.75509552,\n",
       "        0.71258492, 0.75198319, 0.72842764, 0.75198319, 0.72108072,\n",
       "        0.74635093, 0.7630135 , 0.73241213, 0.72293444, 0.7422459 ,\n",
       "        0.72464819, 0.76131653, 0.75198319, 0.73000518, 0.74820374,\n",
       "        0.75320807, 0.70973956, 0.75037204, 0.76423838, 0.75828054]),\n",
       " 'std_test_score': array([0.02456106, 0.02326638, 0.02401136, 0.02619441, 0.07003138,\n",
       "        0.02374471, 0.07598256, 0.02087489, 0.03390315, 0.02714297,\n",
       "        0.02578119, 0.01033697, 0.01966782, 0.02017306, 0.01462133,\n",
       "        0.02380706, 0.02003631, 0.01996187, 0.02321803, 0.02087489,\n",
       "        0.06260669, 0.01131399, 0.07151131, 0.08779573, 0.01465241,\n",
       "        0.06003454, 0.05757554, 0.07211755, 0.05183691, 0.05642404,\n",
       "        0.04702856, 0.07414733, 0.02367917, 0.02242423, 0.0277968 ,\n",
       "        0.00710412, 0.07001339, 0.03491067, 0.05636861, 0.07447956,\n",
       "        0.04545584, 0.06713501, 0.05173403, 0.04054498, 0.00940638,\n",
       "        0.06733054, 0.05690156, 0.0540609 , 0.05782475, 0.04619423,\n",
       "        0.0495562 , 0.05961532, 0.03511963, 0.04884933, 0.01530364,\n",
       "        0.04930527, 0.03037303, 0.06225008, 0.05962655, 0.01282536,\n",
       "        0.08704776, 0.05105401, 0.05563155, 0.02034102, 0.04397499,\n",
       "        0.04232522, 0.03097345, 0.05797712, 0.01511248, 0.02290912,\n",
       "        0.06991378, 0.03530281, 0.02048491, 0.05114949, 0.0388939 ,\n",
       "        0.04260367, 0.06053487, 0.0475967 , 0.0241566 , 0.03090608,\n",
       "        0.02433304, 0.01825354, 0.02780766, 0.0282988 , 0.07208652,\n",
       "        0.03513626, 0.0233877 , 0.03462911, 0.02484901, 0.02008033,\n",
       "        0.03253886, 0.02251778, 0.05756969, 0.03794541, 0.02321803,\n",
       "        0.02296593, 0.02823346, 0.07046435, 0.03905742, 0.02780766,\n",
       "        0.07392519, 0.03201567, 0.03963946, 0.07053037, 0.06371553,\n",
       "        0.09375172, 0.07930137, 0.05268098, 0.07180507, 0.06581916,\n",
       "        0.05441972, 0.06198603, 0.08225753, 0.0853856 , 0.063576  ,\n",
       "        0.02187347, 0.02607773, 0.07156516, 0.07943535, 0.02482011,\n",
       "        0.06226312, 0.06653532, 0.06797088, 0.07775437, 0.05256264,\n",
       "        0.02783273, 0.07790653, 0.02848396, 0.07870578, 0.00992477,\n",
       "        0.097317  , 0.07380058, 0.07318434, 0.05028262, 0.06198851,\n",
       "        0.08065833, 0.07763313, 0.07006268, 0.05228476, 0.08101153,\n",
       "        0.08174606, 0.07293041, 0.05989494, 0.04708678, 0.09071665,\n",
       "        0.0727188 , 0.04158839, 0.05153016, 0.07103709, 0.07161299,\n",
       "        0.03001951, 0.04001165, 0.06337596, 0.07633371, 0.09617467,\n",
       "        0.06806435, 0.07169685, 0.06556064, 0.09571323, 0.06171835,\n",
       "        0.03283191, 0.02799445, 0.03283191, 0.02916142, 0.0566704 ,\n",
       "        0.02305285, 0.02780766, 0.02480847, 0.09762174, 0.02916142,\n",
       "        0.02202172, 0.03283191, 0.03531257, 0.0241761 , 0.08440775,\n",
       "        0.02916142, 0.03285204, 0.02932721, 0.02871094, 0.02503287,\n",
       "        0.07289393, 0.04922463, 0.04009161, 0.08457529, 0.04470541,\n",
       "        0.04658687, 0.07103223, 0.08603332, 0.06808451, 0.06943695,\n",
       "        0.06319377, 0.07317517, 0.04684292, 0.05472275, 0.06887496,\n",
       "        0.07423188, 0.01460977, 0.06828544, 0.07416948, 0.07572855,\n",
       "        0.09319728, 0.07459386, 0.06582398, 0.03355677, 0.028741  ,\n",
       "        0.0720796 , 0.0808436 , 0.07993074, 0.07652351, 0.07026676,\n",
       "        0.04574679, 0.06513001, 0.06995908, 0.04558789, 0.07720467,\n",
       "        0.04001932, 0.08436005, 0.07726039, 0.07083762, 0.01042144,\n",
       "        0.05313129, 0.08743295, 0.07486392, 0.07433284, 0.0859919 ,\n",
       "        0.06180333, 0.06983042, 0.05927126, 0.06473683, 0.07207756,\n",
       "        0.08995846, 0.05757554, 0.00969078, 0.07893043, 0.08851286,\n",
       "        0.02280362, 0.07400306, 0.01479067, 0.08164162, 0.08026214,\n",
       "        0.02776322, 0.03083953, 0.02363632, 0.0228789 , 0.02292269,\n",
       "        0.0236438 , 0.015847  , 0.02363632, 0.02790223, 0.02425174,\n",
       "        0.02724828, 0.07454482, 0.02564565, 0.01873632, 0.0228789 ,\n",
       "        0.03315466, 0.07757675, 0.02652805, 0.02564565, 0.02724828,\n",
       "        0.06470868, 0.08166172, 0.07941085, 0.05471883, 0.08646095,\n",
       "        0.07946404, 0.08338521, 0.03514273, 0.06496305, 0.07366446,\n",
       "        0.07166116, 0.02735209, 0.07601521, 0.07197885, 0.06756481,\n",
       "        0.03659632, 0.04937103, 0.05671979, 0.0743068 , 0.07957611,\n",
       "        0.05203858, 0.04188691, 0.05718785, 0.04188691, 0.02927466,\n",
       "        0.06658232, 0.05823804, 0.04115122, 0.02817286, 0.08383446,\n",
       "        0.04776387, 0.03823573, 0.01829673, 0.03881255, 0.06658232,\n",
       "        0.07865094, 0.05923222, 0.05547257, 0.05920116, 0.06658232,\n",
       "        0.07043778, 0.08564106, 0.05718785, 0.04188691, 0.06329804,\n",
       "        0.06494049, 0.06279454, 0.06155963, 0.06782414, 0.06240863,\n",
       "        0.03048661, 0.09076209, 0.02653858, 0.08492954, 0.07285781,\n",
       "        0.07285781, 0.07942384, 0.06152132, 0.08093671, 0.05597797,\n",
       "        0.06551685, 0.06369543, 0.03253886, 0.02691394, 0.08565896,\n",
       "        0.02487828, 0.03381029, 0.06662762, 0.0246326 , 0.02829608,\n",
       "        0.02487828, 0.07479394, 0.03288704, 0.03315057, 0.02433977,\n",
       "        0.03315057, 0.06875441, 0.02575973, 0.0335765 , 0.036166  ,\n",
       "        0.07042643, 0.07286308, 0.09281036, 0.09563332, 0.0873269 ,\n",
       "        0.06265687, 0.08359601, 0.09081224, 0.05104288, 0.06566709,\n",
       "        0.08682168, 0.08440493, 0.08610539, 0.08309145, 0.07697146,\n",
       "        0.08309145, 0.06275418, 0.08468704, 0.05947889, 0.0714046 ,\n",
       "        0.08359601, 0.06240536, 0.08263427, 0.07589703, 0.05363962,\n",
       "        0.07795161, 0.08356614, 0.07589703, 0.07153532, 0.06989549,\n",
       "        0.07903906, 0.09082916, 0.07157935, 0.06866317, 0.10020699,\n",
       "        0.06597824, 0.09067357, 0.08235331, 0.07284446, 0.09496422,\n",
       "        0.04911445, 0.07710838, 0.09008303, 0.08477065, 0.06141121,\n",
       "        0.07710838, 0.0847599 , 0.08865924, 0.08833738, 0.08801646,\n",
       "        0.07148023, 0.07589703, 0.06316675, 0.06603147, 0.10020699,\n",
       "        0.06871487, 0.07589703, 0.06690189, 0.09361734, 0.06945168,\n",
       "        0.03049735, 0.07065913, 0.03516153, 0.03098481, 0.03315057,\n",
       "        0.03323337, 0.08506853, 0.03275732, 0.07283892, 0.0346263 ,\n",
       "        0.03062761, 0.0247036 , 0.07132678, 0.03937426, 0.03315057,\n",
       "        0.02714274, 0.06649124, 0.03315057, 0.02487828, 0.06420011,\n",
       "        0.07920795, 0.09086337, 0.1013687 , 0.08736884, 0.08072182,\n",
       "        0.0941416 , 0.10651751, 0.09081224, 0.0759295 , 0.10852898,\n",
       "        0.04852399, 0.08359601, 0.09563332, 0.06116279, 0.0765418 ,\n",
       "        0.09086337, 0.10232866, 0.07569923, 0.05918006, 0.09161353,\n",
       "        0.04902405, 0.0759811 , 0.07011089, 0.08359601, 0.06645094,\n",
       "        0.08429566, 0.09164144, 0.07710838, 0.08339707, 0.08318931,\n",
       "        0.0888965 , 0.08356614, 0.0907512 , 0.06327769, 0.06069244,\n",
       "        0.07589703, 0.05684149, 0.07725515, 0.08007842, 0.10020699,\n",
       "        0.06193122, 0.07710838, 0.06761626, 0.07710838, 0.05783996,\n",
       "        0.08338128, 0.09008303, 0.06835476, 0.07893873, 0.06327769,\n",
       "        0.06882167, 0.07783483, 0.07710838, 0.07053073, 0.07148023,\n",
       "        0.07589703, 0.06976948, 0.07252151, 0.0888965 , 0.08359601]),\n",
       " 'rank_test_score': array([463, 456, 455, 470, 391, 475, 348, 473, 479, 459, 466, 469, 468,\n",
       "        478, 449, 464, 467, 471, 453, 473, 293, 380, 275, 184, 366, 328,\n",
       "        149, 269, 325, 272, 268, 229, 339, 415, 410, 342, 179, 344, 278,\n",
       "        285, 312,  66, 199, 345, 434, 298, 142, 338, 363, 304, 314, 291,\n",
       "        311, 231, 360, 336, 365, 302, 276, 331, 177, 321, 260, 341, 178,\n",
       "        287, 355, 219, 326, 374, 265, 253, 383, 317, 256, 353, 233, 284,\n",
       "        392, 372, 472, 406, 443, 442, 309, 411, 419, 429, 399, 476, 403,\n",
       "        457, 422, 423, 453, 420, 413, 364, 430, 443, 319, 332, 263, 112,\n",
       "        264, 205,   5, 330, 185, 246, 228, 215, 283, 168, 255, 367, 343,\n",
       "        152, 280, 351, 340, 211,  59, 251, 248, 306, 164, 303, 217, 335,\n",
       "         57, 204, 270, 170, 207, 121, 305, 310, 324, 213, 111, 135, 257,\n",
       "        322,   2, 154, 294, 237, 292,  35, 393, 323, 267, 174,  80, 155,\n",
       "        362, 244, 158, 181, 431, 462, 431, 446, 405, 480, 443, 458, 286,\n",
       "        446, 450, 431, 428, 461, 349, 446, 477, 414, 460, 452, 245, 295,\n",
       "        281, 162, 346, 334, 109, 163, 148, 261, 226, 266, 218, 195, 191,\n",
       "        129, 376, 299, 273, 235, 202, 210, 193, 288, 390, 153,  95, 187,\n",
       "         49, 143, 296, 259, 282, 354, 249, 357, 194, 243, 247, 327, 223,\n",
       "        128, 189, 209, 221, 180,  93, 337, 227, 277, 230, 149, 347,   8,\n",
       "        120, 375, 102, 373, 139,  96, 451, 398, 424, 426, 465, 412, 439,\n",
       "        424, 435, 436, 437, 358, 440, 418, 426, 408, 361, 417, 440, 437,\n",
       "        225, 107, 161, 206, 113, 169,  39, 300, 216, 197, 140, 359, 200,\n",
       "        151,  62, 236, 333, 262,  84,  68, 224, 240, 124, 240, 289,  87,\n",
       "        190, 234, 315,  73, 252, 222, 301, 250,  87, 116, 214, 176, 196,\n",
       "         87, 254, 117, 124, 240, 192, 144, 182, 203, 232, 274, 316, 141,\n",
       "        279,  30,  69,  69, 307,  74, 122, 118, 318, 290, 403, 381, 186,\n",
       "        400, 378, 297, 421, 370, 400, 308, 407, 385, 396, 385, 352, 416,\n",
       "        371, 395, 172, 175,  85,   3,  65, 165,  23,  31, 212, 133, 131,\n",
       "         21, 114,  13,  82,  13,  10,   9, 156,  41,  23, 127,  48,  42,\n",
       "        238, 201,   6,  42,  75,  79, 119,  56, 167,  76,  36, 115, 138,\n",
       "         98,  40,  64, 220,  50,  16,  33, 103,  50,  61,  90, 173,  83,\n",
       "         71,  42, 126, 110,  36,  77,  42, 134, 105,  81, 377, 329, 368,\n",
       "        382, 385, 369, 258, 384, 350, 409, 397, 394, 313, 356, 385, 379,\n",
       "        320, 385, 400, 271,  58,  28,  67,   1,  15,  18, 132,  31, 183,\n",
       "         22, 145,  23,   3, 147, 100,  28,  63, 188, 159,  97, 239, 108,\n",
       "         34,  23, 137, 104,  20,  50, 101, 106,  11,   6,  94,  91,  99,\n",
       "         42, 160, 171,  86,  36, 198,  50, 136,  50, 166,  78,  16, 123,\n",
       "        157,  91, 146,  19,  50, 130,  71,  42, 208,  60,  11,  23]),\n",
       " 'split0_train_score': array([0.81428635, 0.81087219, 0.80062846, 0.80318287, 0.87593915,\n",
       "        0.80829765, 0.80162304, 0.81173957, 0.81011968, 0.81011968,\n",
       "        0.81830839, 0.81173957, 0.81914938, 0.80778208, 0.81173957,\n",
       "        0.81011968, 0.81830839, 0.81011968, 0.81011968, 0.81087219,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98241758,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 0.97529289, 0.98241758, 0.98241758,\n",
       "        0.98241758, 0.98241758, 0.97529289, 0.96231884, 0.96231884,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 1.        , 1.        ,\n",
       "        0.85589161, 0.85589161, 0.81685786, 0.81011968, 0.81011968,\n",
       "        0.80829765, 0.79278745, 0.80829765, 0.87911789, 0.81173957,\n",
       "        0.8513823 , 0.81011968, 0.86442259, 0.80318287, 0.80162304,\n",
       "        0.85397142, 0.83392602, 0.81685786, 0.84573529, 0.80318287,\n",
       "        1.        , 0.98241758, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 0.99169719, 0.98241758, 0.98241758,\n",
       "        0.98241758, 1.        , 0.98241758, 0.97411477, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.99452351, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.80984679, 0.81173957, 0.84761258, 0.83749175, 0.86101307,\n",
       "        0.83717859, 0.8199757 , 0.85589161, 0.85397142, 0.81173957,\n",
       "        0.85273088, 0.84761258, 0.80162304, 0.81173957, 0.79938354,\n",
       "        0.80495436, 0.83560226, 0.83201735, 0.80666166, 0.80162304,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98241758,\n",
       "        1.        , 1.        , 0.98241758, 0.98241758, 0.98241758,\n",
       "        0.98241758, 0.97698887, 0.95519521, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.99452351, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 0.99452351, 1.        , 1.        , 1.        ,\n",
       "        0.80558572, 0.80574675, 0.8430694 , 0.83532263, 0.85555036,\n",
       "        0.81685786, 0.80730658, 0.85367307, 0.84256196, 0.84068489,\n",
       "        0.84042019, 0.84256196, 0.81011968, 0.83929445, 0.79952334,\n",
       "        0.85367307, 0.84240093, 0.82093049, 0.81685786, 0.84693489,\n",
       "        0.99169719, 0.99169719, 1.        , 0.99169719, 0.99169719,\n",
       "        0.99169719, 1.        , 1.        , 0.99169719, 0.99169719,\n",
       "        0.99169719, 0.99169719, 0.98241758, 0.99169719, 0.99169719,\n",
       "        0.99169719, 0.99169719, 0.97411477, 0.97411477, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.80528983, 0.8121179 , 0.8121179 , 0.8206746 , 0.83899986,\n",
       "        0.84211523, 0.8121179 , 0.8206746 , 0.80666166, 0.80916489,\n",
       "        0.80780011, 0.8121179 , 0.79674717, 0.81550929, 0.82331296,\n",
       "        0.79805075, 0.81173957, 0.8121179 , 0.80956349, 0.80956349,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.81087219, 0.80439817, 0.83357256, 0.80695665, 0.8121179 ,\n",
       "        0.78785184, 0.8513823 , 0.81550929, 0.79660552, 0.80439817,\n",
       "        0.8121179 , 0.8206746 , 0.82988598, 0.80695665, 0.80695665,\n",
       "        0.8206746 , 0.81112332, 0.80666166, 0.8121179 , 0.81550929,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split1_train_score': array([0.83877931, 0.7939902 , 0.83195411, 0.79546485, 0.84974834,\n",
       "        0.83888634, 0.8192178 , 0.82201831, 0.7939902 , 0.79546485,\n",
       "        0.80066099, 0.84159057, 0.78552904, 0.83877931, 0.78360833,\n",
       "        0.78024276, 0.85223252, 0.81325908, 0.83888634, 0.80239708,\n",
       "        0.98241758, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.96231884, 0.96231884,\n",
       "        1.        , 1.        , 0.98241758, 0.96231884, 0.98241758,\n",
       "        0.95693637, 0.95693637, 0.95693637, 0.95693637, 0.95693637,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        0.80187341, 0.82887923, 0.77477274, 0.83603575, 0.85168419,\n",
       "        0.84857882, 0.79344877, 0.79063237, 0.82118847, 0.83888634,\n",
       "        0.82308123, 0.80239708, 0.82665823, 0.80354869, 0.84857882,\n",
       "        0.81208956, 0.82887923, 0.83919192, 0.81208956, 0.78659197,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99267399,\n",
       "        0.99267399, 1.        , 1.        , 1.        , 0.96231884,\n",
       "        0.98241758, 0.98241758, 0.99267399, 0.98241758, 0.98241758,\n",
       "        0.98241758, 0.97698887, 0.98241758, 0.97698887, 0.97698887,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 0.98241758,\n",
       "        0.83603575, 0.78418523, 0.80137538, 0.80239708, 0.80354869,\n",
       "        0.80918294, 0.80510131, 0.79344877, 0.83888634, 0.79344877,\n",
       "        0.83603575, 0.81208956, 0.80171152, 0.7986497 , 0.78818477,\n",
       "        0.79546485, 0.83603575, 0.83195411, 0.84003795, 0.78659197,\n",
       "        0.98241758, 1.        , 1.        , 1.        , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.98241758, 1.        , 0.97513935, 0.98241758, 0.98241758,\n",
       "        0.98241758, 0.9541621 , 0.98241758, 0.98241758, 0.95693637,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.86163811, 0.86003788, 0.86311276, 0.87973747, 0.83173256,\n",
       "        0.83195411, 0.87004499, 0.8683089 , 0.86311276, 0.8683089 ,\n",
       "        0.87973747, 0.86163811, 0.87131187, 0.82887923, 0.86311276,\n",
       "        0.87163089, 0.84047224, 0.86311276, 0.87862296, 0.86311276,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.83358634, 0.83888634, 0.83888634, 0.83888634, 0.83888634,\n",
       "        0.79171892, 0.82820818, 0.83888634, 0.82820818, 0.83888634,\n",
       "        0.82820818, 0.83888634, 0.82820818, 0.81893543, 0.82820818,\n",
       "        0.83888634, 0.87004499, 0.82820818, 0.83358634, 0.83888634,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.99267399, 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.99267399, 0.98241758, 0.97513935, 0.98241758, 0.97513935,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.83888634, 0.83358634, 0.83888634, 0.83888634, 0.81005808,\n",
       "        0.82820818, 0.83358634, 0.83888634, 0.82358883, 0.82820818,\n",
       "        0.83888634, 0.83888634, 0.83888634, 0.83888634, 0.83888634,\n",
       "        0.83888634, 0.82820818, 0.83358634, 0.83358634, 0.83888634,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.99267399, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.95959082, 1.        , 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split2_train_score': array([0.83595238, 0.81953526, 0.85540828, 0.85934352, 0.82331296,\n",
       "        0.86071429, 0.89324898, 0.86175749, 0.81605442, 0.80870863,\n",
       "        0.86071429, 0.85003292, 0.86298963, 0.86472728, 0.81953526,\n",
       "        0.86071429, 0.85273088, 0.85540828, 0.81800055, 0.82484127,\n",
       "        0.98241758, 0.98241758, 0.99267399, 0.97513935, 0.98512586,\n",
       "        1.        , 0.98241758, 0.99267399, 0.98241758, 0.96231884,\n",
       "        0.98241758, 0.98241758, 0.98241758, 0.97513935, 0.96231884,\n",
       "        0.98241758, 0.98241758, 0.97513935, 0.98241758, 0.96231884,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 0.98241758, 1.        ,\n",
       "        0.84145494, 0.85540828, 0.85980335, 0.87521639, 0.85678332,\n",
       "        0.86175749, 0.85559283, 0.86071429, 0.81696375, 0.91506892,\n",
       "        0.85892857, 0.85205861, 0.84679205, 0.81971981, 0.85205861,\n",
       "        0.85476651, 0.87521639, 0.84607294, 0.87414014, 0.86071429,\n",
       "        1.        , 0.99267399, 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 0.98241758,\n",
       "        1.        , 0.98241758, 0.98241758, 0.98241758, 0.97411477,\n",
       "        0.98241758, 0.97411477, 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98241758,\n",
       "        1.        , 0.95875458, 0.98241758, 0.98241758, 0.98241758,\n",
       "        0.85280181, 0.85980335, 0.88048934, 0.86071429, 0.86472728,\n",
       "        0.85892857, 0.86071429, 0.81097506, 0.88914502, 0.87003968,\n",
       "        0.88279582, 0.86071429, 0.89097219, 0.88048934, 0.85003292,\n",
       "        0.85540828, 0.84679205, 0.85314126, 0.88301826, 0.84145494,\n",
       "        0.98241758, 1.        , 1.        , 0.98241758, 0.96231884,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 0.98241758,\n",
       "        0.97513935, 1.        , 0.98241758, 0.98241758, 0.98241758,\n",
       "        0.98241758, 0.96231884, 0.97513935, 0.97513935, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 0.98241758, 0.98241758,\n",
       "        0.80331349, 0.80563795, 0.84145494, 0.84145494, 0.8095004 ,\n",
       "        0.80207403, 0.80747645, 0.84145494, 0.81373016, 0.84145494,\n",
       "        0.81373016, 0.84679205, 0.80999161, 0.84145494, 0.84145494,\n",
       "        0.85205861, 0.84505596, 0.84679205, 0.84145494, 0.80747645,\n",
       "        0.99267399, 0.99267399, 0.99267399, 0.99267399, 0.99267399,\n",
       "        0.99267399, 0.99267399, 0.99267399, 0.99267399, 0.98410128,\n",
       "        0.99267399, 0.99267399, 1.        , 0.99267399, 1.        ,\n",
       "        0.99267399, 0.97513935, 0.97513935, 0.96656664, 0.97513935,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.83604078, 0.84607294, 0.87432468, 0.86902948, 0.80100249,\n",
       "        0.84044284, 0.86902948, 0.84607294, 0.83860434, 0.87944614,\n",
       "        0.84044284, 0.86366722, 0.81384299, 0.86366722, 0.87944614,\n",
       "        0.87432468, 0.84607294, 0.87432468, 0.87432468, 0.86366722,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.99267399, 0.99267399, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.79419501, 0.83500333, 0.80112724, 0.87432468, 0.83350679,\n",
       "        0.86902948, 0.84607294, 0.86876477, 0.87432468, 0.8633135 ,\n",
       "        0.830012  , 0.83886905, 0.89059363, 0.83500333, 0.8719553 ,\n",
       "        0.86366722, 0.80747645, 0.86366722, 0.83350679, 0.85823153,\n",
       "        0.99267399, 1.        , 1.        , 1.        , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.99267399, 1.        , 0.99267399, 1.        , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split3_train_score': array([0.85860917, 0.84802551, 0.84802551, 0.8428501 , 0.8687608 ,\n",
       "        0.84802551, 0.84802551, 0.84802551, 0.89143488, 0.84802551,\n",
       "        0.87803197, 0.84802551, 0.83975335, 0.83975335, 0.84802551,\n",
       "        0.84802551, 0.83975335, 0.84802551, 0.84802551, 0.84802551,\n",
       "        0.97411477, 0.98241758, 0.99169719, 0.99169719, 0.99169719,\n",
       "        1.        , 1.        , 1.        , 0.97411477, 0.97411477,\n",
       "        0.99169719, 0.97411477, 0.98457821, 0.99169719, 0.97411477,\n",
       "        0.97411477, 0.95045177, 0.99169719, 0.97411477, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.99287476, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.8788582 , 0.85703344, 0.87285657, 0.84802551, 0.84239774,\n",
       "        0.88177293, 0.84802551, 0.84802551, 0.84969281, 0.85176646,\n",
       "        0.87803197, 0.87285657, 0.84802551, 0.84349729, 0.85176646,\n",
       "        0.84969281, 0.8788582 , 0.87803197, 0.87285657, 0.84802551,\n",
       "        1.        , 0.99169719, 1.        , 0.99169719, 1.        ,\n",
       "        0.99169719, 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        0.99169719, 0.99169719, 1.        , 0.99169719, 0.99169719,\n",
       "        0.99169719, 0.97411477, 0.97411477, 0.97411477, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.88507883, 0.85176646, 0.8428501 , 0.86453998, 0.87817444,\n",
       "        0.8428501 , 0.85176646, 0.85176646, 0.8428501 , 0.84802551,\n",
       "        0.85176646, 0.87995216, 0.84802551, 0.84969281, 0.84802551,\n",
       "        0.84802551, 0.85772424, 0.88703991, 0.87285657, 0.87803197,\n",
       "        0.99169719, 0.99169719, 1.        , 0.99169719, 1.        ,\n",
       "        1.        , 0.99169719, 1.        , 1.        , 0.99169719,\n",
       "        1.        , 0.99169719, 0.99169719, 1.        , 0.99169719,\n",
       "        0.99169719, 0.99169719, 0.97411477, 0.99169719, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99169719, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.88507883, 0.87995216, 0.8428501 , 0.84969281, 0.87803197,\n",
       "        0.84969281, 0.84802551, 0.84802551, 0.83975335, 0.87995216,\n",
       "        0.84802551, 0.87803197, 0.84137324, 0.87803197, 0.84969281,\n",
       "        0.84802551, 0.87058604, 0.87995216, 0.84802551, 0.84802551,\n",
       "        1.        , 0.99169719, 0.99169719, 1.        , 0.99169719,\n",
       "        0.99169719, 1.        , 0.99169719, 1.        , 0.99169719,\n",
       "        0.99169719, 0.99169719, 0.99169719, 0.99169719, 0.99169719,\n",
       "        0.99169719, 1.        , 0.99169719, 0.97411477, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.85860917, 0.85860917, 0.85860917, 0.85860917, 0.88177293,\n",
       "        0.85860917, 0.86393544, 0.85875164, 0.86140071, 0.85860917,\n",
       "        0.89419479, 0.85860917, 0.85343377, 0.85875164, 0.86382741,\n",
       "        0.89419479, 0.85875164, 0.86393544, 0.88886852, 0.85875164,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99169719,\n",
       "        0.99169719, 0.99169719, 1.        , 0.99169719, 1.        ,\n",
       "        0.99169719, 1.        , 1.        , 0.99169719, 1.        ,\n",
       "        1.        , 1.        , 0.99169719, 1.        , 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.88709663, 0.85875164, 0.85875164, 0.85343377, 0.85343377,\n",
       "        0.85343377, 0.86393544, 0.85860917, 0.88186186, 0.85860917,\n",
       "        0.86393544, 0.88886852, 0.86382741, 0.85343377, 0.85860917,\n",
       "        0.86393544, 0.88886852, 0.88369312, 0.85860917, 0.85860917,\n",
       "        0.99169719, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99169719, 1.        , 0.99169719,\n",
       "        0.99169719, 1.        , 0.99169719, 0.99169719, 1.        ,\n",
       "        0.99169719, 0.99169719, 0.99169719, 0.99169719, 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split4_train_score': array([0.89110012, 0.85962399, 0.86070381, 0.82590112, 0.8718076 ,\n",
       "        0.83067314, 0.85114253, 0.89086722, 0.81557026, 0.83297305,\n",
       "        0.83356643, 0.81594768, 0.80398719, 0.81350402, 0.81868307,\n",
       "        0.85918299, 0.87303609, 0.85918299, 0.83067314, 0.816845  ,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 0.98441948, 1.        , 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.87077293, 0.83449387, 0.86968623, 0.89496096, 0.86100855,\n",
       "        0.82763808, 0.88537763, 0.82553255, 0.85386177, 0.89216994,\n",
       "        0.85827002, 0.82852824, 0.89911592, 0.89496096, 0.88969284,\n",
       "        0.85953888, 0.88446466, 0.88791462, 0.87348988, 0.88791462,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 1.        ,\n",
       "        0.98441948, 0.98441948, 0.98441948, 1.        , 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 1.        , 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 0.98441948,\n",
       "        1.        , 0.98441948, 1.        , 0.98441948, 0.98441948,\n",
       "        0.88010277, 0.89800956, 0.87826036, 0.88791462, 0.84971379,\n",
       "        0.85556614, 0.88969284, 0.84873785, 0.87181492, 0.87303609,\n",
       "        0.88251959, 0.84684145, 0.8160084 , 0.88446466, 0.83722837,\n",
       "        0.88689845, 0.87919398, 0.88259738, 0.88537763, 0.88166629,\n",
       "        0.98441948, 0.98441948, 1.        , 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.97904429, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 0.98441948, 1.        ,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.82114607, 0.8495927 , 0.84064106, 0.83916215, 0.83475936,\n",
       "        0.83318681, 0.8421959 , 0.80704347, 0.85584617, 0.80836097,\n",
       "        0.85114253, 0.82114607, 0.82446399, 0.82114607, 0.81227165,\n",
       "        0.8495927 , 0.81730864, 0.83021911, 0.8495927 , 0.84777119,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.82606846, 0.85441931, 0.8552221 , 0.8552221 , 0.84633148,\n",
       "        0.8552221 , 0.86572575, 0.8552221 , 0.83176739, 0.86758175,\n",
       "        0.86389545, 0.85796445, 0.84873785, 0.8552221 , 0.8552221 ,\n",
       "        0.85796445, 0.82817019, 0.85796445, 0.86478338, 0.8552221 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.87188283, 0.85441931, 0.86758175, 0.85441931, 0.87733717,\n",
       "        0.82901614, 0.8552221 , 0.8571185 , 0.84672241, 0.8571185 ,\n",
       "        0.8552221 , 0.84672241, 0.88063623, 0.85796445, 0.86045426,\n",
       "        0.8552221 , 0.8910003 , 0.8659753 , 0.8459955 , 0.82653523,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'mean_train_score': array([0.84774547, 0.82640943, 0.83934403, 0.82534849, 0.85791377,\n",
       "        0.83731939, 0.84265157, 0.84688162, 0.82543389, 0.81905834,\n",
       "        0.83825641, 0.83346725, 0.82228172, 0.83290921, 0.81631835,\n",
       "        0.83165705, 0.84721225, 0.83719911, 0.82914104, 0.82059621,\n",
       "        0.98467388, 0.98985093, 0.99375813, 0.9902512 , 0.98873202,\n",
       "        0.9968839 , 0.99648352, 0.99541869, 0.97713765, 0.97663439,\n",
       "        0.99170685, 0.98819037, 0.98182515, 0.97919849, 0.97713765,\n",
       "        0.97606116, 0.97132856, 0.97669706, 0.97204141, 0.96802166,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99648352, 0.9968839 , 0.9968839 ,\n",
       "        1.        , 0.99296703, 0.99194236, 1.        , 0.99336741,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99336741, 1.        , 1.        ,\n",
       "        1.        , 0.99296703, 0.99336741, 0.99336741, 0.9968839 ,\n",
       "        0.84977022, 0.84634129, 0.83879535, 0.85287166, 0.8443987 ,\n",
       "        0.84560899, 0.83504644, 0.82664047, 0.84416494, 0.86192625,\n",
       "        0.85393882, 0.83319203, 0.85700286, 0.83298192, 0.84874395,\n",
       "        0.84601184, 0.8602689 , 0.85361386, 0.85566229, 0.83728585,\n",
       "        0.9968839 , 0.99024165, 0.99336741, 0.99522333, 0.9985348 ,\n",
       "        0.99375813, 0.99522333, 0.9968839 , 0.99296703, 0.9823147 ,\n",
       "        0.99170685, 0.98819037, 0.99024165, 0.98467388, 0.98301332,\n",
       "        0.98467388, 0.98192758, 0.9811574 , 0.9784111 , 0.98007166,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99336741, 1.        , 0.9968839 ,\n",
       "        0.9968839 , 0.99061155, 0.9968839 , 0.98985093, 0.99336741,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.9968839 , 0.99336741,\n",
       "        1.        , 0.98863481, 0.99296703, 0.99336741, 0.98985093,\n",
       "        0.85277319, 0.84110084, 0.85011755, 0.85061154, 0.85143545,\n",
       "        0.84074127, 0.84545012, 0.83216395, 0.85933356, 0.83925792,\n",
       "        0.8611697 , 0.84944201, 0.83166813, 0.84500722, 0.82457102,\n",
       "        0.83815029, 0.85106966, 0.85735   , 0.85759041, 0.83787364,\n",
       "        0.98819037, 0.99522333, 1.        , 0.99170685, 0.98788246,\n",
       "        0.9968839 , 0.99522333, 0.9968839 , 0.99336741, 0.98819037,\n",
       "        0.98839528, 0.99522333, 0.9821432 , 0.98633444, 0.98467388,\n",
       "        0.98467388, 0.9739173 , 0.97425728, 0.98321824, 0.97606116,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.9968839 , 0.9968839 , 1.        ,\n",
       "        0.9968839 , 0.9957886 , 0.99170685, 0.98985093, 0.99336741,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.9968839 , 1.        ,\n",
       "        1.        , 1.        , 0.98985093, 0.9968839 , 0.9968839 ,\n",
       "        1.        , 0.99227211, 0.99336741, 0.99336741, 0.99336741,\n",
       "        0.83535244, 0.84019349, 0.84622565, 0.849074  , 0.84191493,\n",
       "        0.82675313, 0.83500988, 0.84370118, 0.84300088, 0.84775237,\n",
       "        0.84661117, 0.85003403, 0.83145208, 0.84176133, 0.8332111 ,\n",
       "        0.85499616, 0.84316476, 0.84820131, 0.84691079, 0.84266416,\n",
       "        0.99687424, 0.99521368, 0.99687424, 0.99687424, 0.99374847,\n",
       "        0.99521368, 0.9985348 , 0.99687424, 0.99687424, 0.98998265,\n",
       "        0.99521368, 0.99521368, 0.99170685, 0.99209757, 0.99356277,\n",
       "        0.99209757, 0.9902512 , 0.98155768, 0.97632665, 0.97804119,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.83191892, 0.84202113, 0.84783204, 0.84848434, 0.84139862,\n",
       "        0.83762165, 0.84780335, 0.84392152, 0.83332846, 0.85073766,\n",
       "        0.84690828, 0.84624902, 0.82819399, 0.84241714, 0.85000336,\n",
       "        0.8526842 , 0.84295586, 0.84731013, 0.85422528, 0.84521816,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99687424,\n",
       "        0.99833944, 0.99833944, 1.        , 0.99833944, 1.        ,\n",
       "        0.99687424, 1.        , 1.        , 0.99170685, 0.9968839 ,\n",
       "        0.99541869, 0.99190221, 0.988786  , 0.99336741, 0.9902512 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.8405866 , 0.83723176, 0.83998391, 0.84560415, 0.83729074,\n",
       "        0.83350788, 0.85003982, 0.84777761, 0.84462066, 0.84232951,\n",
       "        0.84003476, 0.84680419, 0.86076592, 0.83844891, 0.84737235,\n",
       "        0.84847714, 0.84533535, 0.85071673, 0.83676314, 0.83955431,\n",
       "        0.99687424, 1.        , 1.        , 1.        , 0.9985348 ,\n",
       "        1.        , 1.        , 0.99833944, 1.        , 0.99833944,\n",
       "        0.99687424, 1.        , 0.99833944, 0.99522333, 0.9968839 ,\n",
       "        0.99375813, 0.9871415 , 0.99375813, 0.98819037, 0.99375813,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'std_train_score': array([0.02583265, 0.02412521, 0.02164798, 0.02386051, 0.01949103,\n",
       "        0.01761154, 0.03128436, 0.0283213 , 0.03395476, 0.01884879,\n",
       "        0.02799924, 0.01631797, 0.02704682, 0.02050182, 0.02055332,\n",
       "        0.03155064, 0.01796439, 0.02115891, 0.01371032, 0.01555779,\n",
       "        0.00844549, 0.00831886, 0.00584047, 0.00953463, 0.00643792,\n",
       "        0.00623221, 0.00703297, 0.0061884 , 0.00821602, 0.01430229,\n",
       "        0.00744243, 0.01024306, 0.00339653, 0.00995227, 0.00821602,\n",
       "        0.01020016, 0.01456215, 0.01165781, 0.01084316, 0.00993924,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00703297, 0.00623221, 0.00623221,\n",
       "        0.        , 0.00861359, 0.00745702, 0.        , 0.00814786,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00814786, 0.        , 0.        ,\n",
       "        0.        , 0.00861359, 0.00814786, 0.00814786, 0.00623221,\n",
       "        0.02715859, 0.01210806, 0.03777803, 0.0296832 , 0.01822855,\n",
       "        0.02566131, 0.03644181, 0.02550535, 0.02286533, 0.03712569,\n",
       "        0.01779249, 0.02619785, 0.02422483, 0.03429914, 0.02799051,\n",
       "        0.01724674, 0.02380623, 0.02603324, 0.02429825, 0.03731055,\n",
       "        0.00623221, 0.00629879, 0.00814786, 0.0062866 , 0.0029304 ,\n",
       "        0.00584047, 0.0062866 , 0.00623221, 0.00861359, 0.01197544,\n",
       "        0.00744243, 0.00682107, 0.00629879, 0.00359623, 0.00560858,\n",
       "        0.00359623, 0.00979074, 0.00360566, 0.0042683 , 0.00387036,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00814786, 0.        , 0.00623221,\n",
       "        0.00623221, 0.00648241, 0.00623221, 0.00831886, 0.00814786,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00623221, 0.00814786,\n",
       "        0.        , 0.01611273, 0.00861359, 0.00814786, 0.00831886,\n",
       "        0.02797634, 0.03950598, 0.02880747, 0.0289287 , 0.02561048,\n",
       "        0.01768402, 0.03002426, 0.02517852, 0.01877793, 0.0316894 ,\n",
       "        0.01851949, 0.02219572, 0.03414636, 0.03491012, 0.02575654,\n",
       "        0.03375132, 0.0162408 , 0.0237627 , 0.03017809, 0.03869685,\n",
       "        0.00682107, 0.0062866 , 0.        , 0.00744243, 0.01401513,\n",
       "        0.00623221, 0.0062866 , 0.00623221, 0.00814786, 0.00682107,\n",
       "        0.00996593, 0.0062866 , 0.0054788 , 0.00687663, 0.00359623,\n",
       "        0.00359623, 0.01386107, 0.01033462, 0.00528935, 0.01020016,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00623221, 0.00623221, 0.        ,\n",
       "        0.00623221, 0.00606737, 0.00744243, 0.00831886, 0.00814786,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00623221, 0.        ,\n",
       "        0.        , 0.        , 0.00831886, 0.00623221, 0.00623221,\n",
       "        0.        , 0.00752708, 0.00814786, 0.00814786, 0.00814786,\n",
       "        0.03249007, 0.02981127, 0.00849091, 0.01603762, 0.02322047,\n",
       "        0.01613472, 0.0243888 , 0.02035933, 0.0169529 , 0.02490256,\n",
       "        0.02116305, 0.01907562, 0.02302906, 0.01955851, 0.02369406,\n",
       "        0.00854262, 0.01692262, 0.02144533, 0.01973343, 0.01859777,\n",
       "        0.0038407 , 0.00392426, 0.0038407 , 0.0038407 , 0.00315614,\n",
       "        0.00392426, 0.0029304 , 0.0038407 , 0.0038407 , 0.00629361,\n",
       "        0.00392426, 0.00392426, 0.00744243, 0.00494074, 0.00588959,\n",
       "        0.00494074, 0.00953463, 0.00645493, 0.00644416, 0.00445173,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01717833, 0.01642757, 0.02110993, 0.01694412, 0.02568397,\n",
       "        0.02402276, 0.02319236, 0.01355018, 0.01763893, 0.02465202,\n",
       "        0.02981786, 0.01903975, 0.02123182, 0.02077389, 0.02131959,\n",
       "        0.03284666, 0.02090989, 0.02333701, 0.02876197, 0.01967206,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.0038407 ,\n",
       "        0.00332112, 0.00332112, 0.        , 0.00332112, 0.        ,\n",
       "        0.0038407 , 0.        , 0.        , 0.00744243, 0.00623221,\n",
       "        0.0061884 , 0.00745243, 0.00842191, 0.00814786, 0.00953463,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.03515865, 0.01926041, 0.02309121, 0.02237247, 0.02551047,\n",
       "        0.02753747, 0.01008166, 0.01879008, 0.03169052, 0.02263744,\n",
       "        0.01834679, 0.02270607, 0.02335029, 0.01794005, 0.02283491,\n",
       "        0.01661419, 0.0370878 , 0.02728472, 0.01543094, 0.0170882 ,\n",
       "        0.0038407 , 0.        , 0.        , 0.        , 0.0029304 ,\n",
       "        0.        , 0.        , 0.00332112, 0.        , 0.00332112,\n",
       "        0.0038407 , 0.        , 0.00332112, 0.0062866 , 0.00623221,\n",
       "        0.00584047, 0.0149525 , 0.00584047, 0.00682107, 0.00584047,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ])}"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search\n",
    "\n",
    "# Usando um grid completo de todos os parâmetros\n",
    "param_grid = {\"max_depth\": range(3,10,2),\n",
    "              \"min_samples_split\": range(1,6),\n",
    "              \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"n_estimators\": range(100,500,100),\n",
    "              \"bootstrap\": [True, False]}\n",
    "\n",
    "# Executando o Grid Search\n",
    "grid_search = GridSearchCV(modelo4_multi, param_grid = param_grid, return_train_score = True, scoring='f1_macro')\n",
    "start = time.time()\n",
    "grid_search.fit(X_falha_treino, y_falha_treino)\n",
    "\n",
    "print(\"GridSearchCV executou em %.2f segundos para todas as combinações de candidatos a parâmetros do modelo.\"\n",
    "      % (time.time() - start))\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "dd6baf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'log_loss',\n",
       " 'max_depth': 5,\n",
       " 'min_samples_split': 1,\n",
       " 'n_estimators': 400}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "3d84f432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7765662507427213"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00bfc7",
   "metadata": {},
   "source": [
    "A primeira tentativa de otimização dos hiperparâmetros, chegamos a 77,66% de F1 Score. Vamos fazer uma segunda otimização de hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "9bf3a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV executou em 1874.75 segundos para todas as combinações de candidatos a parâmetros do modelo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.6378046 , 0.82475739, 0.65942569, 0.76480265, 0.59600387,\n",
       "        0.77120061, 0.57159724, 0.82260065, 0.60000119, 0.76899819,\n",
       "        0.6056035 , 0.74199829, 0.59379764, 0.75039768, 0.55179825,\n",
       "        0.70119791, 0.55579958, 0.82980151, 0.60359893, 0.72480063,\n",
       "        0.57900023, 0.7455946 , 0.57980156, 0.69960375, 0.56540251,\n",
       "        0.820401  , 0.61619601, 0.77401891, 0.62479825, 0.76859727,\n",
       "        0.62899857, 0.78559723, 0.65479922, 0.7969934 , 0.64279652,\n",
       "        0.76799974, 0.58220296, 0.71059899, 0.56259632, 0.73879638,\n",
       "        0.62019911, 0.71140018, 0.60019755, 0.75760322, 0.62459993,\n",
       "        0.75619812, 0.6152019 , 0.71999979, 0.58059745, 0.71460128,\n",
       "        0.57879653, 0.72479887, 0.56259775, 0.73180995, 0.59039946,\n",
       "        0.79100089, 0.60880055, 0.82379794, 0.56859837, 0.72360229,\n",
       "        0.57660017, 0.72619705, 0.57999973, 0.70640016, 0.57000008,\n",
       "        0.71459737, 0.5891993 , 0.75540128, 0.62739849, 0.76220107,\n",
       "        0.58379998, 0.72700338, 0.61979446, 0.80100155, 0.62180252,\n",
       "        0.72960191, 0.57839293, 0.71300144, 0.62879686, 0.74000187,\n",
       "        0.58000131, 0.72419906, 0.55039797, 0.68819871, 0.56599994,\n",
       "        0.73359632, 0.58080001, 0.74480186, 0.56599674, 0.71959863,\n",
       "        0.57359734, 0.69660053, 0.54560037, 0.73219738, 0.56060095,\n",
       "        0.70460119, 0.56520319, 0.71339726, 0.56260247, 0.69499784,\n",
       "        0.62899852, 0.72660017, 0.56979651, 0.78040423, 0.63180013,\n",
       "        0.79199753, 0.58450651, 0.71539555, 0.62259855, 0.74319658,\n",
       "        0.62519922, 0.73719501, 0.63919816, 0.77319202, 0.5843997 ,\n",
       "        0.77079792, 0.62319713, 0.75639615, 0.59519992, 0.80359793,\n",
       "        0.62580051, 0.87579989, 0.64939857, 0.78380208, 0.70579853,\n",
       "        0.71139746, 0.64690733, 0.7821979 , 0.69762578, 0.84120078,\n",
       "        0.60100088, 0.82700176, 0.67880077, 0.91159768, 0.71759982,\n",
       "        0.97710419, 0.75759988, 0.9385972 , 0.69760308, 0.91519966,\n",
       "        0.68999619, 0.9499929 , 0.79380112, 0.82900238, 0.65740447,\n",
       "        0.88220053, 0.68619318, 0.82079916, 0.65180054, 0.81139965,\n",
       "        0.66559973, 0.8171958 , 0.65760126, 0.92860699, 0.66799974,\n",
       "        0.92420387, 0.75719786, 0.82439823, 0.66920295, 0.91700268,\n",
       "        0.69619656, 0.80400052, 0.66479506, 0.83820381, 0.70028939,\n",
       "        0.84528089, 0.71038718, 0.92813888, 0.59830508, 0.71439905,\n",
       "        0.57198853, 0.70860157, 0.58219905, 0.76472478, 0.62939553,\n",
       "        0.82507648, 0.69720764, 0.81601777, 0.60439696, 0.7423986 ,\n",
       "        0.66180329, 0.8526011 , 0.72760243, 1.0690022 , 0.68040299,\n",
       "        0.77899737, 0.66559658, 0.852001  , 0.64659982, 1.06866574,\n",
       "        0.62599773, 0.93416967, 0.78761206, 0.9200613 , 0.62208061,\n",
       "        0.77632322, 0.64310889, 0.90453014, 0.83276916, 0.92917037,\n",
       "        0.62941704, 0.80290408, 0.65049062, 0.84499683, 0.63786197,\n",
       "        0.78512611, 0.77272749, 0.83290162, 0.76015415, 0.94519877,\n",
       "        0.78948317, 0.84610066, 0.63087821, 0.79432425, 0.62809854,\n",
       "        1.00566163, 0.78734694, 0.82640572, 0.63100162, 0.84201741,\n",
       "        0.72680564, 0.9134088 , 0.69873848, 0.94839935, 0.80138912,\n",
       "        0.96875749, 0.64282346, 0.79453816, 0.64636402, 1.02929029,\n",
       "        0.66650958, 0.86722875, 0.79846859, 1.01271992, 0.80640798,\n",
       "        0.92916689, 0.63498139, 0.88940248, 0.6290391 , 0.87159748,\n",
       "        0.66420298, 0.87259998, 0.71500196, 0.79459715, 0.65959864,\n",
       "        0.83627996, 0.63300071, 0.79777212, 0.65619903, 0.81799541,\n",
       "        0.70011425, 0.81659851, 0.47760267, 0.63280358, 0.46339951,\n",
       "        0.59379864, 0.53240018, 0.81240101, 0.54623008, 0.65672898,\n",
       "        0.51742773, 0.62269697, 0.62669182, 0.61934977, 0.56005821,\n",
       "        0.5905633 , 0.53120084, 0.55780072, 0.43239827, 0.53159814,\n",
       "        0.43539491, 0.57659774, 0.56938648, 0.57426925, 0.54101477,\n",
       "        0.67170005, 0.51819715, 0.59160271, 0.48980403, 0.61240144,\n",
       "        0.44059463, 0.61799655, 0.55379925, 0.66940012, 0.45540156,\n",
       "        0.58339992, 0.49760141, 0.55399842, 0.47499933, 0.63919663,\n",
       "        0.43240018, 0.70820255, 0.51099935, 0.60179791, 0.51940041,\n",
       "        0.6360014 , 0.4905931 , 0.62159824, 0.49020104, 0.6811975 ,\n",
       "        0.48320117, 0.81280079, 0.63480229, 0.57019749, 0.43519893,\n",
       "        0.55300231, 0.45319657, 0.59339681, 0.48419833, 0.69519901,\n",
       "        0.61159725, 0.65099773, 0.4913959 , 0.57519884, 0.53579535,\n",
       "        0.61780095, 0.49220109, 0.57920394, 0.52340097, 0.6597959 ,\n",
       "        0.47040005, 0.58700309, 0.47800059, 0.62480006, 0.5127996 ,\n",
       "        0.67599831, 0.49080296, 0.7803997 , 0.57439771, 0.60181856,\n",
       "        0.51686721, 0.61840572, 0.49259925, 0.64620547, 0.50439782,\n",
       "        0.58379798, 0.46379728, 0.59840293, 0.43900156, 0.53760142,\n",
       "        0.45440459, 0.58079925, 0.51019425, 0.62719512, 0.47979703,\n",
       "        0.59120522, 0.46920033, 0.60920129, 0.55879722, 0.67479887,\n",
       "        0.54400105, 0.6790019 , 0.49860373, 0.58400126, 0.56779947,\n",
       "        0.74359832, 0.6185957 , 0.71340094, 0.50700502, 0.66979856,\n",
       "        0.53219914, 0.62519655, 0.50759487, 0.64240026, 0.53839931,\n",
       "        0.6743969 , 0.63160553, 0.64699988, 0.51039891, 0.61219764,\n",
       "        0.48580532, 0.78279781, 0.49459853, 0.63799887, 0.47919598,\n",
       "        0.61059928, 0.47240229, 0.65440006, 0.50320287, 0.63720365,\n",
       "        0.49720106, 0.62219758, 0.49720001, 0.72219906, 0.55159554,\n",
       "        0.63179755, 0.61399884, 0.61339855, 0.51659942, 0.5840023 ,\n",
       "        0.55880318, 0.67280011, 0.49939828, 0.6011981 , 0.47399945,\n",
       "        0.65019999, 0.46459899, 0.56999884, 0.45840111, 0.58339853,\n",
       "        0.47320318, 0.66260033, 0.60300198, 0.69610758, 0.53332543,\n",
       "        0.7571981 , 0.63120275, 0.69718099, 0.51700053, 0.6575964 ,\n",
       "        0.51780009, 0.59240026, 0.47799778, 0.6569984 , 0.56020107,\n",
       "        0.68879614, 0.59280243, 0.717802  , 0.52720146, 0.74000177,\n",
       "        0.53379598, 0.80479226, 0.48320303, 0.55380001, 0.49759798,\n",
       "        0.59540401, 0.47079945, 0.62800083, 0.46779857, 0.65419679,\n",
       "        0.53100214, 0.61959834, 0.47800126, 0.63859754, 0.46239471,\n",
       "        0.55839834, 0.66259885, 0.79167695, 0.69475675, 0.6078074 ,\n",
       "        0.47479901, 0.61691608, 0.52052846, 0.59252014, 0.47773256,\n",
       "        0.73139997, 0.60520172, 0.68579745, 0.50079918, 0.69700422,\n",
       "        0.55359654, 0.69240341, 0.47439718, 0.66400027, 0.61720052,\n",
       "        0.73957911, 0.54188237, 0.6751018 , 0.52446952, 0.59445605,\n",
       "        0.50255671, 0.57882838, 0.46747427, 0.59428091, 0.45700011,\n",
       "        0.58962903, 0.46625714, 0.59929686, 0.45686665, 0.58428211,\n",
       "        0.50311618, 0.56604986, 0.45942874, 0.65052066, 0.57528653,\n",
       "        0.75949268, 0.47874269, 0.60219898, 0.53460031, 0.72419705,\n",
       "        0.48980031, 0.6208015 , 0.52379804, 0.66259761, 0.4846004 ,\n",
       "        0.57399964, 0.48039584, 0.69259577, 0.50420051, 0.69419804,\n",
       "        0.53299923, 0.69259977, 0.53539677, 0.65698314, 0.54939904,\n",
       "        0.69400086, 0.52599769, 0.67300076, 0.58910937, 0.64939861,\n",
       "        0.56619897, 0.72359924, 0.78219852, 0.69408808]),\n",
       " 'std_fit_time': array([0.03813382, 0.05840711, 0.04085613, 0.03250442, 0.03139378,\n",
       "        0.03560433, 0.01580587, 0.06306686, 0.0193187 , 0.03159112,\n",
       "        0.05819481, 0.03048666, 0.02367816, 0.0443415 , 0.00980623,\n",
       "        0.01058878, 0.00968623, 0.10866229, 0.02438625, 0.00936571,\n",
       "        0.01843811, 0.01270901, 0.0281178 , 0.01423183, 0.01908696,\n",
       "        0.05605974, 0.03486682, 0.03912877, 0.01292468, 0.01971475,\n",
       "        0.04096959, 0.03491039, 0.12993085, 0.05672092, 0.03247035,\n",
       "        0.03961299, 0.01292361, 0.01807298, 0.01791563, 0.02363148,\n",
       "        0.02956598, 0.01202561, 0.01519825, 0.03184291, 0.01469455,\n",
       "        0.03174361, 0.04316408, 0.01908767, 0.01600833, 0.02682297,\n",
       "        0.02148162, 0.02428714, 0.01227186, 0.02524348, 0.01565243,\n",
       "        0.01972512, 0.02990451, 0.04087392, 0.00520455, 0.02098131,\n",
       "        0.02059487, 0.02214651, 0.0286585 , 0.02876454, 0.02350304,\n",
       "        0.01613004, 0.01551115, 0.03352595, 0.03268676, 0.0232042 ,\n",
       "        0.0088405 , 0.01069658, 0.04297957, 0.03921738, 0.02373193,\n",
       "        0.02782729, 0.01177059, 0.01460185, 0.04825976, 0.00892309,\n",
       "        0.01006097, 0.03015628, 0.01465191, 0.00893036, 0.04834763,\n",
       "        0.02981559, 0.04039737, 0.03919748, 0.01896606, 0.02760132,\n",
       "        0.0199891 , 0.0136627 , 0.00458669, 0.03955483, 0.01861797,\n",
       "        0.01937978, 0.01013027, 0.01139522, 0.00708506, 0.01440529,\n",
       "        0.05133416, 0.01635361, 0.01123178, 0.04797423, 0.04349638,\n",
       "        0.06051438, 0.01343956, 0.0114276 , 0.05844742, 0.0130587 ,\n",
       "        0.02886558, 0.02919622, 0.0332838 , 0.0506349 , 0.01913795,\n",
       "        0.03129957, 0.03730357, 0.00775897, 0.02269339, 0.094428  ,\n",
       "        0.01358795, 0.10239265, 0.04164431, 0.03734034, 0.05986134,\n",
       "        0.01112565, 0.03888541, 0.0127176 , 0.02864273, 0.0598871 ,\n",
       "        0.01115393, 0.04417093, 0.04426671, 0.07005492, 0.05790628,\n",
       "        0.16327833, 0.05419237, 0.04296249, 0.04042992, 0.11792769,\n",
       "        0.02590539, 0.07110646, 0.09944625, 0.01588695, 0.03423948,\n",
       "        0.0533202 , 0.03636276, 0.02193831, 0.01213705, 0.00895775,\n",
       "        0.01875601, 0.00913213, 0.02096487, 0.10087516, 0.01411117,\n",
       "        0.07910515, 0.07294435, 0.03152644, 0.01779899, 0.060838  ,\n",
       "        0.05975046, 0.00447196, 0.00194094, 0.04640139, 0.05188521,\n",
       "        0.07148213, 0.04511663, 0.07807085, 0.01540616, 0.00463432,\n",
       "        0.01097875, 0.0176701 , 0.01821404, 0.05008086, 0.02619523,\n",
       "        0.03972367, 0.02731775, 0.05011922, 0.00722676, 0.031911  ,\n",
       "        0.03624864, 0.04588618, 0.08025089, 0.11056984, 0.02993419,\n",
       "        0.04219086, 0.0482681 , 0.04873624, 0.02441819, 0.14096555,\n",
       "        0.01917231, 0.12120687, 0.02023319, 0.09437455, 0.00581696,\n",
       "        0.00633461, 0.03934665, 0.15347441, 0.03904817, 0.11135536,\n",
       "        0.00392721, 0.03195869, 0.01899292, 0.04857203, 0.02048937,\n",
       "        0.00702015, 0.06953429, 0.0899017 , 0.01564348, 0.01278429,\n",
       "        0.15061865, 0.11852318, 0.00708042, 0.01047675, 0.00531827,\n",
       "        0.07518587, 0.04829032, 0.04544592, 0.03992088, 0.04384767,\n",
       "        0.11698569, 0.08743814, 0.04854036, 0.12071059, 0.05696768,\n",
       "        0.1032865 , 0.00639367, 0.01118308, 0.04007135, 0.11598204,\n",
       "        0.02337403, 0.04769602, 0.11855549, 0.06373944, 0.0682831 ,\n",
       "        0.08365824, 0.00495967, 0.0602334 , 0.04114209, 0.04862751,\n",
       "        0.04542209, 0.06412676, 0.01688739, 0.03632481, 0.0559707 ,\n",
       "        0.07949886, 0.02383024, 0.01947013, 0.02987658, 0.01944168,\n",
       "        0.02440182, 0.05292623, 0.04650457, 0.08198095, 0.03041469,\n",
       "        0.05031634, 0.08654324, 0.2113074 , 0.10694181, 0.04306536,\n",
       "        0.02322267, 0.05395231, 0.06024516, 0.07656507, 0.14963255,\n",
       "        0.03468324, 0.07878204, 0.03566983, 0.01996481, 0.00653027,\n",
       "        0.01417975, 0.03089634, 0.08673074, 0.00971401, 0.1002208 ,\n",
       "        0.06841676, 0.0656875 , 0.03262273, 0.04094966, 0.07862427,\n",
       "        0.00757683, 0.04722114, 0.17742365, 0.11378944, 0.02153575,\n",
       "        0.04804959, 0.05139211, 0.00623201, 0.02368769, 0.05963043,\n",
       "        0.00445409, 0.10440356, 0.08596214, 0.04895524, 0.04311181,\n",
       "        0.03152698, 0.02323401, 0.02224749, 0.01380219, 0.11632812,\n",
       "        0.03346461, 0.08487676, 0.12058026, 0.02482307, 0.0099857 ,\n",
       "        0.01048699, 0.0160437 , 0.04406832, 0.0226407 , 0.08138452,\n",
       "        0.05326672, 0.03420354, 0.02444932, 0.01565367, 0.04519505,\n",
       "        0.05038159, 0.04727286, 0.02164445, 0.02525473, 0.05448176,\n",
       "        0.01012745, 0.01257263, 0.0224872 , 0.05574006, 0.02754805,\n",
       "        0.04981286, 0.02542647, 0.1789894 , 0.08334006, 0.02190392,\n",
       "        0.04188717, 0.02444541, 0.01201086, 0.04545091, 0.02911339,\n",
       "        0.01543161, 0.0162254 , 0.0385263 , 0.01067615, 0.01379508,\n",
       "        0.00778538, 0.01704054, 0.01882902, 0.02260744, 0.01471372,\n",
       "        0.0203215 , 0.01179084, 0.04554019, 0.05110296, 0.04664753,\n",
       "        0.02547092, 0.0681081 , 0.03485308, 0.01680282, 0.05139988,\n",
       "        0.15028048, 0.16542039, 0.06017874, 0.02170622, 0.03476568,\n",
       "        0.04890503, 0.03813776, 0.04297011, 0.03794777, 0.03830273,\n",
       "        0.05280362, 0.09489851, 0.01729581, 0.02788165, 0.00888519,\n",
       "        0.00825836, 0.07665328, 0.03308478, 0.0308799 , 0.01357027,\n",
       "        0.02439918, 0.00840346, 0.05438842, 0.01238165, 0.02894321,\n",
       "        0.01100041, 0.01284433, 0.01295251, 0.06764763, 0.03709501,\n",
       "        0.03602057, 0.14669219, 0.02595375, 0.02595957, 0.00734932,\n",
       "        0.04269021, 0.05637485, 0.02170072, 0.01116001, 0.00969232,\n",
       "        0.05704001, 0.00515983, 0.00874098, 0.01070995, 0.01474553,\n",
       "        0.02155689, 0.03184036, 0.04890992, 0.04012932, 0.0263268 ,\n",
       "        0.07146011, 0.04048371, 0.05988612, 0.02020605, 0.04399536,\n",
       "        0.0407166 , 0.00595404, 0.00912255, 0.07850496, 0.05359838,\n",
       "        0.05288873, 0.04900808, 0.03052446, 0.03382349, 0.11292383,\n",
       "        0.0589531 , 0.13067208, 0.04329189, 0.02295757, 0.05120746,\n",
       "        0.03520591, 0.02164803, 0.02886797, 0.01739247, 0.09843873,\n",
       "        0.0502574 , 0.04863577, 0.04963748, 0.10647442, 0.01762586,\n",
       "        0.02438557, 0.09843394, 0.16738954, 0.03264018, 0.01567909,\n",
       "        0.00661133, 0.03847478, 0.03548201, 0.00427861, 0.01891876,\n",
       "        0.1237674 , 0.07227635, 0.10270856, 0.04541971, 0.06591175,\n",
       "        0.03007275, 0.08264243, 0.01000964, 0.10984598, 0.05987054,\n",
       "        0.03892328, 0.02129488, 0.03043923, 0.02411953, 0.0382947 ,\n",
       "        0.0407185 , 0.0114705 , 0.0152645 , 0.01752068, 0.00340485,\n",
       "        0.01384995, 0.00725297, 0.01885187, 0.00976877, 0.00918706,\n",
       "        0.03913695, 0.01105785, 0.01622058, 0.0458334 , 0.02402955,\n",
       "        0.05263927, 0.01779854, 0.01010965, 0.05490532, 0.07374768,\n",
       "        0.02778867, 0.02650045, 0.03851736, 0.03675568, 0.03404446,\n",
       "        0.00718367, 0.00412772, 0.11594623, 0.01768   , 0.07284119,\n",
       "        0.01953792, 0.0278035 , 0.04476633, 0.01877288, 0.04115323,\n",
       "        0.057082  , 0.0197726 , 0.07852813, 0.07320047, 0.02234625,\n",
       "        0.03822346, 0.06820153, 0.12278144, 0.04024471]),\n",
       " 'mean_score_time': array([0.03819251, 0.04880271, 0.03780031, 0.04539762, 0.03699651,\n",
       "        0.04840078, 0.03439913, 0.04460135, 0.03620043, 0.04339852,\n",
       "        0.03539739, 0.04499998, 0.03640127, 0.0445982 , 0.03619895,\n",
       "        0.04660206, 0.03679557, 0.0489994 , 0.03639932, 0.04499793,\n",
       "        0.03660054, 0.04420156, 0.0360002 , 0.0449976 , 0.0351995 ,\n",
       "        0.04839988, 0.03719773, 0.04540138, 0.03580136, 0.04759812,\n",
       "        0.03699856, 0.04900174, 0.03899674, 0.04560332, 0.03640318,\n",
       "        0.04979997, 0.03919954, 0.04480081, 0.0366015 , 0.04600463,\n",
       "        0.03559823, 0.04440522, 0.03720422, 0.04479804, 0.04060302,\n",
       "        0.04479952, 0.036799  , 0.0447998 , 0.03620329, 0.0458004 ,\n",
       "        0.03599949, 0.04500222, 0.03519998, 0.04490361, 0.0358016 ,\n",
       "        0.05019741, 0.03620024, 0.04619966, 0.03599782, 0.04419756,\n",
       "        0.03720026, 0.04460201, 0.03640199, 0.04640079, 0.03679996,\n",
       "        0.04499927, 0.03560162, 0.0457983 , 0.03619981, 0.04419718,\n",
       "        0.03660178, 0.0463975 , 0.0368011 , 0.04799924, 0.0363986 ,\n",
       "        0.04619918, 0.03700304, 0.04519615, 0.03720255, 0.04700036,\n",
       "        0.03579731, 0.0452004 , 0.0349998 , 0.04520235, 0.03459964,\n",
       "        0.04320297, 0.03740125, 0.0449985 , 0.03679967, 0.04560103,\n",
       "        0.03660278, 0.04340057, 0.03539953, 0.04479976, 0.03559871,\n",
       "        0.0444025 , 0.03639946, 0.04459901, 0.03699603, 0.10000362,\n",
       "        0.03680143, 0.0438005 , 0.0360003 , 0.05419602, 0.03780074,\n",
       "        0.04440012, 0.03560033, 0.04280076, 0.03679724, 0.04519882,\n",
       "        0.03620005, 0.04560094, 0.03699942, 0.0484005 , 0.03540211,\n",
       "        0.04400196, 0.03819938, 0.04480214, 0.0364006 , 0.04780293,\n",
       "        0.0372025 , 0.04919863, 0.03860173, 0.04799938, 0.04160023,\n",
       "        0.04439988, 0.03660088, 0.04600043, 0.04200215, 0.04680014,\n",
       "        0.03820071, 0.04679809, 0.03759985, 0.04680257, 0.04499936,\n",
       "        0.05159941, 0.04640083, 0.04679785, 0.03879862, 0.05059814,\n",
       "        0.03800497, 0.05380278, 0.04139705, 0.04959927, 0.0392036 ,\n",
       "        0.04719996, 0.03640027, 0.04699931, 0.03740172, 0.04620104,\n",
       "        0.03759913, 0.04560256, 0.0365983 , 0.04799433, 0.03839889,\n",
       "        0.05339785, 0.04159818, 0.04640098, 0.04059753, 0.04999919,\n",
       "        0.0441999 , 0.04640055, 0.03679919, 0.04899964, 0.04140496,\n",
       "        0.04930167, 0.03765545, 0.0497992 , 0.03679566, 0.04440098,\n",
       "        0.03560205, 0.04479928, 0.0368031 , 0.04460316, 0.03560429,\n",
       "        0.04985533, 0.04090986, 0.04520268, 0.0362021 , 0.0446012 ,\n",
       "        0.03639922, 0.04979925, 0.0484005 , 0.06899595, 0.03880353,\n",
       "        0.05179949, 0.03860297, 0.04899912, 0.03580227, 0.05500255,\n",
       "        0.03780222, 0.05100365, 0.04440327, 0.04780011, 0.03679948,\n",
       "        0.04724402, 0.04320593, 0.0532002 , 0.04403114, 0.04839997,\n",
       "        0.03920484, 0.04522438, 0.0396174 , 0.05100012, 0.03838282,\n",
       "        0.04680862, 0.04459572, 0.04741201, 0.04200182, 0.05060878,\n",
       "        0.04760423, 0.04600425, 0.08643999, 0.04721761, 0.03680558,\n",
       "        0.05840192, 0.03900752, 0.05019889, 0.03779874, 0.04939933,\n",
       "        0.04019847, 0.04859858, 0.03880177, 0.04931316, 0.04479942,\n",
       "        0.05339518, 0.03820524, 0.04801712, 0.03920679, 0.0574245 ,\n",
       "        0.03940277, 0.04880471, 0.05037322, 0.06399813, 0.05759921,\n",
       "        0.06139627, 0.04020181, 0.04979782, 0.03859897, 0.04600186,\n",
       "        0.03839812, 0.0559967 , 0.04119787, 0.04519739, 0.03660145,\n",
       "        0.04939799, 0.03840227, 0.04600205, 0.04020114, 0.04520135,\n",
       "        0.03859735, 0.04519973, 0.03819623, 0.04719625, 0.03619537,\n",
       "        0.04919853, 0.04000502, 0.06219974, 0.04038372, 0.04740939,\n",
       "        0.03564115, 0.05120897, 0.05079803, 0.05200953, 0.03880625,\n",
       "        0.04721441, 0.0429987 , 0.04419909, 0.03519993, 0.04400191,\n",
       "        0.03700495, 0.04940166, 0.04040098, 0.04980159, 0.04460444,\n",
       "        0.048     , 0.04139857, 0.045401  , 0.03680024, 0.04740081,\n",
       "        0.03620048, 0.04639964, 0.04139748, 0.04979858, 0.03660078,\n",
       "        0.05139852, 0.03799672, 0.0450016 , 0.03740077, 0.04659801,\n",
       "        0.03660264, 0.05539994, 0.03759708, 0.04480095, 0.03720169,\n",
       "        0.04740143, 0.03679962, 0.04859977, 0.03799791, 0.04780488,\n",
       "        0.04260149, 0.06320047, 0.06760135, 0.04480362, 0.03539939,\n",
       "        0.04519725, 0.03680129, 0.04340048, 0.03779993, 0.04940448,\n",
       "        0.0430028 , 0.04599948, 0.03800397, 0.04559903, 0.08739972,\n",
       "        0.04599791, 0.03699775, 0.04600091, 0.04099898, 0.05060349,\n",
       "        0.03799887, 0.04540129, 0.03759894, 0.04559951, 0.04079847,\n",
       "        0.05940003, 0.03859944, 0.06140261, 0.04359674, 0.04720569,\n",
       "        0.03799124, 0.04719501, 0.03719954, 0.04659534, 0.03579946,\n",
       "        0.04480052, 0.03719859, 0.04919882, 0.03680253, 0.04439859,\n",
       "        0.03639989, 0.04699965, 0.03740087, 0.04620109, 0.0359992 ,\n",
       "        0.04479671, 0.03619885, 0.0442039 , 0.0385963 , 0.04960179,\n",
       "        0.03919802, 0.0459991 , 0.03619852, 0.04599934, 0.04220028,\n",
       "        0.05219769, 0.04440093, 0.04660263, 0.03759942, 0.04780297,\n",
       "        0.03759832, 0.04820094, 0.03719721, 0.04639759, 0.03879929,\n",
       "        0.05360255, 0.03779697, 0.04780016, 0.03619766, 0.04540148,\n",
       "        0.03659554, 0.0488019 , 0.03720179, 0.04539857, 0.03799949,\n",
       "        0.04519944, 0.03699956, 0.04740047, 0.03659954, 0.04579644,\n",
       "        0.036198  , 0.04680467, 0.03679886, 0.05360093, 0.03660216,\n",
       "        0.04720402, 0.03960061, 0.04559956, 0.04400239, 0.04699941,\n",
       "        0.03759713, 0.0580009 , 0.03659873, 0.04480243, 0.03539929,\n",
       "        0.04519978, 0.03660011, 0.04440093, 0.03620048, 0.04679933,\n",
       "        0.03759699, 0.0492012 , 0.0383997 , 0.0509974 , 0.03699827,\n",
       "        0.04620113, 0.04659815, 0.04839854, 0.03759952, 0.04679856,\n",
       "        0.0379992 , 0.04659867, 0.0372014 , 0.04799867, 0.04239678,\n",
       "        0.04619961, 0.04120007, 0.05140176, 0.03700085, 0.04599724,\n",
       "        0.04440179, 0.05720329, 0.03679705, 0.04640188, 0.0386014 ,\n",
       "        0.04799786, 0.0364007 , 0.04539986, 0.03820677, 0.04620109,\n",
       "        0.03719687, 0.04620018, 0.03600016, 0.04780126, 0.03640227,\n",
       "        0.04940333, 0.04359937, 0.05240431, 0.05219755, 0.04459729,\n",
       "        0.03740096, 0.04540005, 0.03879771, 0.04500494, 0.03660178,\n",
       "        0.05019984, 0.04800053, 0.05360193, 0.03719845, 0.04980135,\n",
       "        0.03700171, 0.04979496, 0.0372025 , 0.0497993 , 0.04079933,\n",
       "        0.0482275 , 0.03699331, 0.0534059 , 0.03844175, 0.0454319 ,\n",
       "        0.03540235, 0.04660716, 0.08361726, 0.04339952, 0.03480058,\n",
       "        0.04399943, 0.03580637, 0.04460812, 0.03699965, 0.04560542,\n",
       "        0.03720293, 0.04519792, 0.03722744, 0.05420389, 0.03720217,\n",
       "        0.05137854, 0.03660302, 0.04420123, 0.03860078, 0.05760422,\n",
       "        0.0376008 , 0.04719958, 0.03840137, 0.04560022, 0.03840113,\n",
       "        0.04619918, 0.03620057, 0.04619913, 0.0369988 , 0.05200362,\n",
       "        0.03639984, 0.0463995 , 0.03719759, 0.04580011, 0.03760018,\n",
       "        0.04599514, 0.03740506, 0.04699883, 0.04199891, 0.0455976 ,\n",
       "        0.03839736, 0.06600041, 0.05417156, 0.04700308]),\n",
       " 'std_score_time': array([0.0039725 , 0.00278819, 0.00312416, 0.00149908, 0.00141546,\n",
       "        0.00534907, 0.00048834, 0.00149711, 0.00146743, 0.0019596 ,\n",
       "        0.00101629, 0.00167047, 0.00049398, 0.00162168, 0.00193594,\n",
       "        0.00265501, 0.00132741, 0.00501707, 0.00135667, 0.00167563,\n",
       "        0.00135795, 0.00116349, 0.00209774, 0.00141915, 0.00097776,\n",
       "        0.00338577, 0.00147011, 0.00206018, 0.00040268, 0.00241386,\n",
       "        0.00141448, 0.00596491, 0.00252813, 0.00185433, 0.0013575 ,\n",
       "        0.00754718, 0.00160259, 0.00160006, 0.00101963, 0.00283052,\n",
       "        0.00079761, 0.00079758, 0.00116398, 0.00098054, 0.00773802,\n",
       "        0.00160113, 0.00159825, 0.00097699, 0.00097887, 0.00160121,\n",
       "        0.0015496 , 0.00141662, 0.00039967, 0.00284226, 0.00075042,\n",
       "        0.00560011, 0.00172054, 0.00116511, 0.00089234, 0.00116623,\n",
       "        0.00159749, 0.00185567, 0.00101881, 0.00332332, 0.00263801,\n",
       "        0.00209844, 0.00080174, 0.00331415, 0.00147338, 0.00039585,\n",
       "        0.00185768, 0.0017424 , 0.00147092, 0.00126423, 0.00101622,\n",
       "        0.00097673, 0.0014159 , 0.00193683, 0.00146966, 0.00424075,\n",
       "        0.00074736, 0.00204048, 0.0010958 , 0.00248269, 0.00048967,\n",
       "        0.00074914, 0.00079952, 0.00227801, 0.00203785, 0.00162547,\n",
       "        0.00294332, 0.00135907, 0.00195946, 0.0020404 , 0.00135517,\n",
       "        0.00205905, 0.0017441 , 0.00205782, 0.00167041, 0.09394053,\n",
       "        0.00193756, 0.00116821, 0.00089447, 0.01641887, 0.00256247,\n",
       "        0.00080096, 0.00135491, 0.00074708, 0.00132608, 0.00074797,\n",
       "        0.0019389 , 0.0013579 , 0.0025244 , 0.00688449, 0.00102047,\n",
       "        0.00109498, 0.00074688, 0.00132611, 0.00135721, 0.00183526,\n",
       "        0.00074989, 0.00636864, 0.00215522, 0.00340715, 0.01020905,\n",
       "        0.00224341, 0.00119879, 0.00167032, 0.00867336, 0.00278826,\n",
       "        0.00193668, 0.00097901, 0.00080531, 0.0014686 , 0.01322182,\n",
       "        0.00872757, 0.00862016, 0.00194005, 0.00426405, 0.00873412,\n",
       "        0.00155109, 0.0087049 , 0.00574772, 0.00427608, 0.00342964,\n",
       "        0.00147299, 0.00048827, 0.0045602 , 0.00185613, 0.00159994,\n",
       "        0.00326163, 0.00079996, 0.0013543 , 0.0024432 , 0.00287255,\n",
       "        0.01070582, 0.00589049, 0.00174416, 0.005819  , 0.00569403,\n",
       "        0.01343493, 0.00135863, 0.00074438, 0.00701324, 0.00683272,\n",
       "        0.00360851, 0.00127301, 0.00278469, 0.00116685, 0.00174343,\n",
       "        0.00080053, 0.00204001, 0.0019412 , 0.00120321, 0.00081144,\n",
       "        0.006026  , 0.00379606, 0.00213417, 0.0007495 , 0.00102021,\n",
       "        0.00102379, 0.00354361, 0.00854883, 0.01172962, 0.00348183,\n",
       "        0.0126259 , 0.00300107, 0.00260936, 0.00074488, 0.00536039,\n",
       "        0.00248471, 0.0023743 , 0.00534712, 0.00286112, 0.00116766,\n",
       "        0.00192594, 0.00770151, 0.0070566 , 0.00188915, 0.00233505,\n",
       "        0.00639964, 0.00212955, 0.00164312, 0.00551441, 0.00257323,\n",
       "        0.00318355, 0.00206073, 0.00206882, 0.00218819, 0.0017502 ,\n",
       "        0.00661891, 0.00109594, 0.09937855, 0.00147745, 0.00146981,\n",
       "        0.00804207, 0.00274821, 0.00396924, 0.00172264, 0.00403155,\n",
       "        0.00312419, 0.00344104, 0.00278543, 0.00633919, 0.00640006,\n",
       "        0.00546016, 0.00116507, 0.00108468, 0.0023104 , 0.0096429 ,\n",
       "        0.00196042, 0.00160066, 0.01273397, 0.01251021, 0.02801104,\n",
       "        0.01749806, 0.00324409, 0.01159892, 0.00431599, 0.0020976 ,\n",
       "        0.00393043, 0.01311467, 0.00646023, 0.00146661, 0.0010235 ,\n",
       "        0.00679663, 0.00250024, 0.00189702, 0.00746625, 0.00116249,\n",
       "        0.00174921, 0.00240025, 0.00193488, 0.00231287, 0.00116784,\n",
       "        0.00849462, 0.00328528, 0.02241895, 0.00406006, 0.00232888,\n",
       "        0.00096104, 0.008076  , 0.01925172, 0.00597264, 0.00247842,\n",
       "        0.00364928, 0.01108068, 0.00146964, 0.00074838, 0.00109715,\n",
       "        0.00189741, 0.00854724, 0.00571551, 0.00519074, 0.01143423,\n",
       "        0.00384868, 0.00659098, 0.00135893, 0.00116669, 0.00233725,\n",
       "        0.00147105, 0.00149608, 0.00796445, 0.00869864, 0.00162108,\n",
       "        0.00933234, 0.00126528, 0.0022815 , 0.00206027, 0.00119962,\n",
       "        0.00185365, 0.01116538, 0.00135653, 0.00147149, 0.00132701,\n",
       "        0.00185655, 0.00194003, 0.00135495, 0.00228025, 0.0028558 ,\n",
       "        0.00783555, 0.01404517, 0.0308363 , 0.00098395, 0.00048924,\n",
       "        0.00160116, 0.00213655, 0.00048905, 0.00231678, 0.0061561 ,\n",
       "        0.00918458, 0.00357741, 0.00296889, 0.00162759, 0.09981588,\n",
       "        0.00219071, 0.0026051 , 0.00218693, 0.00399913, 0.00567762,\n",
       "        0.00404834, 0.00162172, 0.00162443, 0.00205716, 0.0049994 ,\n",
       "        0.02411781, 0.00265285, 0.01639982, 0.0082602 , 0.00320036,\n",
       "        0.00165907, 0.00231055, 0.00193892, 0.00135357, 0.00039754,\n",
       "        0.00116843, 0.00097917, 0.00507705, 0.00146906, 0.00173977,\n",
       "        0.00079904, 0.00167309, 0.00206152, 0.00386851, 0.00252783,\n",
       "        0.00222603, 0.00097753, 0.00116775, 0.00080189, 0.00338386,\n",
       "        0.0030544 , 0.0010993 , 0.00074824, 0.00155096, 0.00846773,\n",
       "        0.00818359, 0.01290451, 0.00232938, 0.00119944, 0.00549352,\n",
       "        0.00195772, 0.00461482, 0.00172141, 0.00135563, 0.0039182 ,\n",
       "        0.01294279, 0.00147386, 0.00292611, 0.00160003, 0.00149793,\n",
       "        0.00079582, 0.00203965, 0.00213732, 0.001745  , 0.00141455,\n",
       "        0.00193809, 0.00126324, 0.00205801, 0.00079973, 0.00074885,\n",
       "        0.00147083, 0.00204032, 0.001719  , 0.0111485 , 0.00120029,\n",
       "        0.00160191, 0.00431983, 0.00215376, 0.01253821, 0.00089448,\n",
       "        0.0016226 , 0.02355283, 0.0010177 , 0.00116794, 0.00048768,\n",
       "        0.00171979, 0.00162467, 0.001201  , 0.00116272, 0.00146908,\n",
       "        0.00149479, 0.00292075, 0.00102409, 0.01253863, 0.00089337,\n",
       "        0.0018342 , 0.00747301, 0.00150124, 0.00185438, 0.00325156,\n",
       "        0.00362963, 0.0021554 , 0.00147059, 0.00209622, 0.00646515,\n",
       "        0.00213508, 0.00256154, 0.00320341, 0.00109559, 0.0017914 ,\n",
       "        0.00628446, 0.00652027, 0.00146995, 0.00280302, 0.00313644,\n",
       "        0.00210204, 0.00102054, 0.00300973, 0.00325215, 0.00147058,\n",
       "        0.00193814, 0.00172063, 0.00089571, 0.00767875, 0.00102243,\n",
       "        0.00900427, 0.0103063 , 0.00872942, 0.00725208, 0.00119596,\n",
       "        0.00162238, 0.00174248, 0.00664747, 0.001794  , 0.00135866,\n",
       "        0.00652685, 0.0088796 , 0.01221016, 0.0013265 , 0.00604863,\n",
       "        0.00253047, 0.00881722, 0.00222592, 0.01161688, 0.00556334,\n",
       "        0.00327987, 0.00125526, 0.01682659, 0.00149149, 0.00118457,\n",
       "        0.00049083, 0.00049442, 0.09522235, 0.00048735, 0.00040082,\n",
       "        0.00109215, 0.00075292, 0.00185625, 0.00181823, 0.00162724,\n",
       "        0.0022276 , 0.00193804, 0.00261696, 0.01174864, 0.00160221,\n",
       "        0.0033167 , 0.00205939, 0.00146921, 0.00338368, 0.01465032,\n",
       "        0.00241818, 0.00172365, 0.00195992, 0.00149667, 0.00631432,\n",
       "        0.00278776, 0.00159922, 0.00159895, 0.00141469, 0.0073471 ,\n",
       "        0.00048629, 0.00162757, 0.00074853, 0.00193772, 0.00174189,\n",
       "        0.00141816, 0.00102094, 0.00126019, 0.00460673, 0.00185549,\n",
       "        0.00349524, 0.02343273, 0.01459265, 0.00252987]),\n",
       " 'param_bootstrap': masked_array(data=[True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.65863636, 0.65863636, 0.65863636, 0.63652174, 0.65863636,\n",
       "        0.65863636, 0.65613445, 0.80974747, 0.80724556, 0.80974747,\n",
       "        0.80974747, 0.80724556, 0.83077498, 0.67966387, 0.83077498,\n",
       "        0.83077498, 0.65863636, 0.80974747, 0.83077498, 0.83077498,\n",
       "        0.82888889, 0.83077498, 0.82888889, 0.82888889, 0.7997619 ,\n",
       "        0.70181818, 0.77077498, 0.7997619 , 0.67515152, 0.77077498,\n",
       "        0.76888889, 0.73730994, 0.83077498, 0.78601307, 0.82888889,\n",
       "        0.73934641, 0.73934641, 0.77077498, 0.77077498, 0.73934641,\n",
       "        0.7397619 , 0.7997619 , 0.73934641, 0.73934641, 0.755     ,\n",
       "        0.78601307, 0.73934641, 0.78601307, 0.70833333, 0.80920635,\n",
       "        0.7397619 , 0.61378788, 0.76181818, 0.7397619 , 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.755     , 0.85292929, 0.78601307, 0.73934641, 0.62521645,\n",
       "        0.77077498, 0.73934641, 0.73934641, 0.78601307, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.63688312, 0.755     , 0.73934641,\n",
       "        0.76253968, 0.7397619 , 0.66823529, 0.73934641, 0.70833333,\n",
       "        0.76253968, 0.80920635, 0.73934641, 0.755     , 0.67777778,\n",
       "        0.67777778, 0.65863636, 0.67777778, 0.65613445, 0.67777778,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.67966387, 0.67966387, 0.82888889, 0.82888889,\n",
       "        0.83077498, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.83077498, 0.77077498,\n",
       "        0.83077498, 0.83077498, 0.83077498, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.76888889, 0.82888889, 0.82888889, 0.83077498,\n",
       "        0.83077498, 0.67966387, 0.64156863, 0.7397619 , 0.83077498,\n",
       "        0.83077498, 0.78601307, 0.78601307, 0.73934641, 0.78601307,\n",
       "        0.73730994, 0.78397661, 0.73934641, 0.78601307, 0.83077498,\n",
       "        0.64156863, 0.76888889, 0.83077498, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.82888889, 0.78601307, 0.82888889, 0.73934641,\n",
       "        0.73934641, 0.70833333, 0.78601307, 0.77077498, 0.73934641,\n",
       "        0.78601307, 0.78601307, 0.78601307, 0.78601307, 0.78397661,\n",
       "        0.82888889, 0.66823529, 0.83077498, 0.78601307, 0.64156863,\n",
       "        0.83077498, 0.6529972 , 0.78601307, 0.78601307, 0.78601307,\n",
       "        0.78601307, 0.83077498, 0.73730994, 0.65613445, 0.65613445,\n",
       "        0.70603175, 0.65613445, 0.65863636, 0.67966387, 0.82888889,\n",
       "        0.80724556, 0.82888889, 0.82888889, 0.80724556, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.67966387, 0.82888889, 0.67777778,\n",
       "        0.67777778, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.77077498, 0.82888889, 0.7997619 ,\n",
       "        0.82888889, 0.77077498, 0.6529972 , 0.76888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.73934641,\n",
       "        0.73934641, 0.77077498, 0.78601307, 0.7397619 , 0.83077498,\n",
       "        0.73934641, 0.78397661, 0.78397661, 0.83077498, 0.78397661,\n",
       "        0.78601307, 0.77077498, 0.6529972 , 0.78601307, 0.83077498,\n",
       "        0.7997619 , 0.65016043, 0.78601307, 0.73730994, 0.73934641,\n",
       "        0.78601307, 0.78601307, 0.82888889, 0.69142857, 0.78601307,\n",
       "        0.77077498, 0.77077498, 0.78601307, 0.83077498, 0.78601307,\n",
       "        0.78601307, 0.78601307, 0.78601307, 0.82888889, 0.78601307,\n",
       "        0.76253968, 0.77077498, 0.73934641, 0.78601307, 0.83077498,\n",
       "        0.77077498, 0.78397661, 0.83077498, 0.78601307, 0.78601307,\n",
       "        0.73934641, 0.73730994, 0.65863636, 0.65863636, 0.65863636,\n",
       "        0.65863636, 0.65863636, 0.65863636, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.83077498, 0.80974747, 0.80974747, 0.80974747, 0.83077498,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.77077498, 0.73934641, 0.78601307, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.73934641, 0.73730994,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.70833333, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.68430184, 0.67777778, 0.70603175, 0.67777778,\n",
       "        0.70603175, 0.70603175, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.73730994, 0.77077498, 0.83077498, 0.76888889, 0.82888889,\n",
       "        0.83077498, 0.78601307, 0.78397661, 0.78601307, 0.78601307,\n",
       "        0.78601307, 0.73934641, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.78601307, 0.78601307, 0.78601307, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.78601307, 0.73934641,\n",
       "        0.78601307, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.78601307, 0.73934641, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.65613445, 0.70603175, 0.67777778, 0.70603175, 0.70603175,\n",
       "        0.67777778, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.77077498,\n",
       "        0.83077498, 0.77077498, 0.76888889, 0.76888889, 0.77077498,\n",
       "        0.78601307, 0.78601307, 0.73730994, 0.78601307, 0.78601307,\n",
       "        0.78601307, 0.73934641, 0.73934641, 0.78601307, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.78601307, 0.78601307,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.78601307, 0.78601307, 0.73934641,\n",
       "        0.78601307, 0.78601307, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.73934641]),\n",
       " 'split1_test_score': array([0.646     , 0.646     , 0.646     , 0.66833333, 0.62208791,\n",
       "        0.66833333, 0.69812253, 0.85288443, 0.83310415, 0.79761905,\n",
       "        0.69812253, 0.69812253, 0.63878788, 0.68953964, 0.68953964,\n",
       "        0.66833333, 0.63878788, 0.646     , 0.82273292, 0.84642857,\n",
       "        0.84642857, 0.82273292, 0.84642857, 0.87481538, 0.63463768,\n",
       "        0.64285714, 0.63463768, 0.79354978, 0.69166667, 0.66175889,\n",
       "        0.84642857, 0.84642857, 0.84642857, 0.84642857, 0.84642857,\n",
       "        0.84642857, 0.66833333, 0.69166667, 0.69166667, 0.66797101,\n",
       "        0.61025641, 0.66797101, 0.84642857, 0.82273292, 0.84642857,\n",
       "        0.82273292, 0.84642857, 0.84642857, 0.69166667, 0.66797101,\n",
       "        0.66797101, 0.66797101, 0.66384615, 0.66384615, 0.84642857,\n",
       "        0.82273292, 0.66797101, 0.84642857, 0.84642857, 0.81860806,\n",
       "        0.66797101, 0.64285714, 0.82273292, 0.69166667, 0.64285714,\n",
       "        0.64285714, 0.82273292, 0.82273292, 0.82273292, 0.84642857,\n",
       "        0.84642857, 0.84642857, 0.66797101, 0.66797101, 0.66797101,\n",
       "        0.66797101, 0.66797101, 0.66797101, 0.84642857, 0.82273292,\n",
       "        0.84642857, 0.84642857, 0.84642857, 0.84642857, 0.66833333,\n",
       "        0.62208791, 0.646     , 0.66833333, 0.646     , 0.66833333,\n",
       "        0.82435786, 0.82435786, 0.79354978, 0.87481538, 0.82435786,\n",
       "        0.84430155, 0.84430155, 0.68953964, 0.68953964, 0.68953964,\n",
       "        0.68953964, 0.66175889, 0.87481538, 0.84642857, 0.84642857,\n",
       "        0.84642857, 0.84642857, 0.84642857, 0.66175889, 0.68953964,\n",
       "        0.68953964, 0.68953964, 0.66833333, 0.69812253, 0.87481538,\n",
       "        0.81860806, 0.84642857, 0.84642857, 0.81860806, 0.81860806,\n",
       "        0.63878788, 0.68953964, 0.60952381, 0.69166667, 0.69166667,\n",
       "        0.67515152, 0.84642857, 0.81860806, 0.81860806, 0.81860806,\n",
       "        0.84642857, 0.84642857, 0.63878788, 0.66175889, 0.82273292,\n",
       "        0.69166667, 0.66384615, 0.63800764, 0.82273292, 0.84642857,\n",
       "        0.84642857, 0.81860806, 0.87481538, 0.81860806, 0.69166667,\n",
       "        0.69812253, 0.69166667, 0.81860806, 0.69166667, 0.63463768,\n",
       "        0.84642857, 0.81860806, 0.81860806, 0.81860806, 0.81860806,\n",
       "        0.84642857, 0.63463768, 0.69166667, 0.64285714, 0.81860806,\n",
       "        0.84642857, 0.66797101, 0.84642857, 0.81860806, 0.84642857,\n",
       "        0.81860806, 0.84642857, 0.81860806, 0.66833333, 0.66833333,\n",
       "        0.63878788, 0.646     , 0.646     , 0.66833333, 0.8165208 ,\n",
       "        0.84430155, 0.82309524, 0.82435786, 0.84642857, 0.84430155,\n",
       "        0.66175889, 0.68953964, 0.66833333, 0.68953964, 0.66833333,\n",
       "        0.68953964, 0.8165208 , 0.87481538, 0.84642857, 0.87481538,\n",
       "        0.87481538, 0.87481538, 0.66175889, 0.66384615, 0.68953964,\n",
       "        0.68953964, 0.68953964, 0.68953964, 0.84642857, 0.87481538,\n",
       "        0.87481538, 0.84642857, 0.87481538, 0.87481538, 0.66833333,\n",
       "        0.72005348, 0.63463768, 0.63463768, 0.69166667, 0.60952381,\n",
       "        0.84642857, 0.81860806, 0.81860806, 0.81860806, 0.81860806,\n",
       "        0.81860806, 0.78939959, 0.69166667, 0.66384615, 0.66175889,\n",
       "        0.69166667, 0.66384615, 0.81860806, 0.81860806, 0.79276955,\n",
       "        0.84642857, 0.81860806, 0.81860806, 0.76428571, 0.69166667,\n",
       "        0.66797101, 0.69166667, 0.63878788, 0.63878788, 0.81860806,\n",
       "        0.79276955, 0.79276955, 0.84642857, 0.81860806, 0.81860806,\n",
       "        0.66384615, 0.66797101, 0.61212121, 0.60952381, 0.66384615,\n",
       "        0.66797101, 0.84642857, 0.84642857, 0.81860806, 0.81860806,\n",
       "        0.81860806, 0.84642857, 0.63878788, 0.61431169, 0.63878788,\n",
       "        0.61431169, 0.61431169, 0.61431169, 0.74534161, 0.71904762,\n",
       "        0.71904762, 0.71603759, 0.75340022, 0.74299034, 0.7915208 ,\n",
       "        0.63463768, 0.73717216, 0.66175889, 0.63463768, 0.63463768,\n",
       "        0.77142857, 0.77142857, 0.77142857, 0.79908425, 0.77142857,\n",
       "        0.82137862, 0.69166667, 0.63463768, 0.63463768, 0.78939959,\n",
       "        0.81860806, 0.63463768, 0.84908425, 0.84908425, 0.84908425,\n",
       "        0.87857143, 0.84908425, 0.88717949, 0.81860806, 0.81860806,\n",
       "        0.66384615, 0.81860806, 0.81860806, 0.81860806, 0.90842883,\n",
       "        0.87857143, 0.91714286, 0.87857143, 0.87857143, 0.87857143,\n",
       "        0.66384615, 0.66384615, 0.81      , 0.66384615, 0.81860806,\n",
       "        0.66384615, 0.87857143, 0.87857143, 0.87857143, 0.87857143,\n",
       "        0.87857143, 0.87857143, 0.69166667, 0.69166667, 0.81860806,\n",
       "        0.66384615, 0.69166667, 0.66384615, 0.87857143, 0.87857143,\n",
       "        0.89809524, 0.87857143, 0.87857143, 0.87857143, 0.66384615,\n",
       "        0.66384615, 0.81860806, 0.66384615, 0.81860806, 0.69166667,\n",
       "        0.87857143, 0.87857143, 0.87857143, 0.91714286, 0.87857143,\n",
       "        0.84908425, 0.62208791, 0.646     , 0.82309524, 0.646     ,\n",
       "        0.646     , 0.66833333, 0.8007619 , 0.82435786, 0.82435786,\n",
       "        0.80542125, 0.8007619 , 0.8007619 , 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.82991342,\n",
       "        0.8007619 , 0.8007619 , 0.82991342, 0.8007619 , 0.8007619 ,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.87481538, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.82309524, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.82309524, 0.82309524, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.82309524,\n",
       "        0.85288443, 0.82309524, 0.82309524, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.93773292, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.82309524, 0.85288443, 0.85288443,\n",
       "        0.82309524, 0.85288443, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.82309524, 0.85288443, 0.85288443, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.93773292, 0.85288443, 0.85288443,\n",
       "        0.646     , 0.646     , 0.646     , 0.8007619 , 0.646     ,\n",
       "        0.646     , 0.7847619 , 0.82309524, 0.80542125, 0.8007619 ,\n",
       "        0.8007619 , 0.80542125, 0.8007619 , 0.82309524, 0.8007619 ,\n",
       "        0.8007619 , 0.8007619 , 0.82309524, 0.82991342, 0.82309524,\n",
       "        0.8007619 , 0.8007619 , 0.8007619 , 0.8007619 , 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.82991342, 0.82991342,\n",
       "        0.82991342, 0.82309524, 0.82309524, 0.85288443, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.85288443, 0.93773292, 0.82991342,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.82309524, 0.82309524, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.93773292, 0.93773292, 0.85288443,\n",
       "        0.82309524, 0.82309524, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.82309524, 0.85288443, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.93773292, 0.85288443, 0.82309524, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.93773292, 0.85288443, 0.85288443, 0.93773292]),\n",
       " 'split2_test_score': array([0.59823529, 0.58823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.58823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.628815  , 0.628815  , 0.628815  ,\n",
       "        0.628815  , 0.628815  , 0.66338681, 0.67214834, 0.81338681,\n",
       "        0.67214834, 0.66338681, 0.81338681, 0.67214834, 0.66338681,\n",
       "        0.66338681, 0.66338681, 0.66338681, 0.66338681, 0.66338681,\n",
       "        0.75338681, 0.63672014, 0.63672014, 0.79292929, 0.66338681,\n",
       "        0.693863  , 0.63672014, 0.63845174, 0.63845174, 0.63672014,\n",
       "        0.66338681, 0.75338681, 0.843863  , 0.67204482, 0.67515152,\n",
       "        0.64548167, 0.67214834, 0.63672014, 0.63672014, 0.63672014,\n",
       "        0.67204482, 0.6129972 , 0.66338681, 0.66338681, 0.75338681,\n",
       "        0.63672014, 0.75338681, 0.63672014, 0.66338681, 0.75338681,\n",
       "        0.693863  , 0.63672014, 0.63672014, 0.75338681, 0.75338681,\n",
       "        0.81338681, 0.75338681, 0.67204482, 0.75338681, 0.63672014,\n",
       "        0.66338681, 0.75338681, 0.63672014, 0.693863  , 0.693863  ,\n",
       "        0.63672014, 0.693863  , 0.63672014, 0.75338681, 0.81338681,\n",
       "        0.70187166, 0.75338681, 0.67515152, 0.66338681, 0.59823529,\n",
       "        0.59823529, 0.59823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.67214834, 0.63823529, 0.63823529, 0.67214834, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.778815  , 0.628815  , 0.628815  ,\n",
       "        0.63823529, 0.63823529, 0.81338681, 0.843863  , 0.81338681,\n",
       "        0.843863  , 0.67214834, 0.67214834, 0.63672014, 0.75338681,\n",
       "        0.81338681, 0.81338681, 0.81338681, 0.66338681, 0.78871148,\n",
       "        0.75338681, 0.79292929, 0.78871148, 0.67214834, 0.81338681,\n",
       "        0.76368984, 0.75338681, 0.843863  , 0.75338681, 0.81338681,\n",
       "        0.81338681, 0.81338681, 0.843863  , 0.78871148, 0.75338681,\n",
       "        0.843863  , 0.64548167, 0.63672014, 0.81338681, 0.64548167,\n",
       "        0.75338681, 0.63672014, 0.81338681, 0.79292929, 0.78871148,\n",
       "        0.75338681, 0.78871148, 0.75338681, 0.843863  , 0.81338681,\n",
       "        0.78871148, 0.75338681, 0.75338681, 0.81338681, 0.75338681,\n",
       "        0.64548167, 0.75338681, 0.75338681, 0.75338681, 0.843863  ,\n",
       "        0.81338681, 0.78871148, 0.78871148, 0.63672014, 0.78871148,\n",
       "        0.78845174, 0.843863  , 0.843863  , 0.78871148, 0.78871148,\n",
       "        0.78871148, 0.81338681, 0.81338681, 0.59823529, 0.59823529,\n",
       "        0.63823529, 0.59823529, 0.63823529, 0.59823529, 0.63823529,\n",
       "        0.67214834, 0.63823529, 0.63823529, 0.63823529, 0.67214834,\n",
       "        0.628815  , 0.628815  , 0.628815  , 0.63823529, 0.628815  ,\n",
       "        0.628815  , 0.81338681, 0.843863  , 0.67214834, 0.81338681,\n",
       "        0.67214834, 0.70187166, 0.81338681, 0.81338681, 0.81338681,\n",
       "        0.81338681, 0.66338681, 0.843863  , 0.85292929, 0.843863  ,\n",
       "        0.843863  , 0.843863  , 0.81338681, 0.81338681, 0.81338681,\n",
       "        0.75338681, 0.78871148, 0.75338681, 0.81338681, 0.81338681,\n",
       "        0.843863  , 0.75338681, 0.75338681, 0.75338681, 0.78871148,\n",
       "        0.75338681, 0.81338681, 0.75338681, 0.63672014, 0.78871148,\n",
       "        0.81338681, 0.66338681, 0.78871148, 0.843863  , 0.843863  ,\n",
       "        0.78871148, 0.68005348, 0.70187166, 0.67204482, 0.75338681,\n",
       "        0.75338681, 0.75338681, 0.693863  , 0.75338681, 0.75338681,\n",
       "        0.79292929, 0.843863  , 0.843863  , 0.75338681, 0.75338681,\n",
       "        0.81338681, 0.75338681, 0.75338681, 0.75338681, 0.81338681,\n",
       "        0.63672014, 0.78871148, 0.78871148, 0.75338681, 0.78871148,\n",
       "        0.75338681, 0.88340548, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.59823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.67214834, 0.63823529,\n",
       "        0.71169082, 0.67214834, 0.71169082, 0.67214834, 0.71169082,\n",
       "        0.67214834, 0.61475703, 0.843863  , 0.843863  , 0.843863  ,\n",
       "        0.64548167, 0.70187166, 0.69466089, 0.66141414, 0.66141414,\n",
       "        0.74141414, 0.70187166, 0.70187166, 0.76368984, 0.81823529,\n",
       "        0.78871148, 0.72966387, 0.76368984, 0.81823529, 0.66141414,\n",
       "        0.66141414, 0.69466089, 0.66141414, 0.69466089, 0.66141414,\n",
       "        0.76368984, 0.76368984, 0.76368984, 0.76368984, 0.76368984,\n",
       "        0.72966387, 0.69466089, 0.69466089, 0.69466089, 0.66141414,\n",
       "        0.69466089, 0.69466089, 0.76368984, 0.76368984, 0.76368984,\n",
       "        0.76368984, 0.65511841, 0.76368984, 0.66141414, 0.69466089,\n",
       "        0.69466089, 0.69466089, 0.66141414, 0.69466089, 0.76368984,\n",
       "        0.65511841, 0.76368984, 0.76368984, 0.78871148, 0.76368984,\n",
       "        0.69466089, 0.69466089, 0.66141414, 0.69466089, 0.66141414,\n",
       "        0.69466089, 0.63823529, 0.67777778, 0.67777778, 0.67777778,\n",
       "        0.63823529, 0.67777778, 0.71169082, 0.71169082, 0.71169082,\n",
       "        0.71169082, 0.71169082, 0.71169082, 0.85292929, 0.71169082,\n",
       "        0.63823529, 0.81835749, 0.71169082, 0.85292929, 0.74141414,\n",
       "        0.74141414, 0.74141414, 0.74141414, 0.74141414, 0.74141414,\n",
       "        0.843863  , 0.88340548, 0.88340548, 0.843863  , 0.88340548,\n",
       "        0.88340548, 0.88340548, 0.71959596, 0.71959596, 0.82825397,\n",
       "        0.71959596, 0.74141414, 0.843863  , 0.88340548, 0.82825397,\n",
       "        0.82825397, 0.81823529, 0.843863  , 0.74141414, 0.71959596,\n",
       "        0.71959596, 0.82825397, 0.88340548, 0.82825397, 0.88340548,\n",
       "        0.82825397, 0.67204482, 0.82825397, 0.88340548, 0.82825397,\n",
       "        0.82825397, 0.71959596, 0.71959596, 0.71959596, 0.71959596,\n",
       "        0.71959596, 0.88340548, 0.82825397, 0.80323232, 0.82825397,\n",
       "        0.88340548, 0.82825397, 0.71959596, 0.71959596, 0.71959596,\n",
       "        0.82825397, 0.82825397, 0.71959596, 0.80323232, 0.80323232,\n",
       "        0.80323232, 0.82825397, 0.82825397, 0.73229437, 0.71959596,\n",
       "        0.82825397, 0.71959596, 0.82825397, 0.71959596, 0.71959596,\n",
       "        0.67777778, 0.67777778, 0.67214834, 0.67777778, 0.67777778,\n",
       "        0.67777778, 0.71169082, 0.71169082, 0.67777778, 0.71169082,\n",
       "        0.67777778, 0.71169082, 0.67777778, 0.85292929, 0.67214834,\n",
       "        0.67214834, 0.70181818, 0.70181818, 0.74141414, 0.74141414,\n",
       "        0.74141414, 0.74141414, 0.88340548, 0.88340548, 0.79292929,\n",
       "        0.88340548, 0.88340548, 0.88340548, 0.88340548, 0.88340548,\n",
       "        0.74141414, 0.74141414, 0.74141414, 0.88340548, 0.71959596,\n",
       "        0.88340548, 0.71047619, 0.88340548, 0.82825397, 0.82825397,\n",
       "        0.82825397, 0.88340548, 0.82825397, 0.82825397, 0.82825397,\n",
       "        0.71959596, 0.71959596, 0.71959596, 0.82825397, 0.88340548,\n",
       "        0.88340548, 0.82825397, 0.80323232, 0.82825397, 0.71959596,\n",
       "        0.82825397, 0.82825397, 0.82825397, 0.82825397, 0.71959596,\n",
       "        0.82825397, 0.82825397, 0.88340548, 0.80323232, 0.82825397,\n",
       "        0.82825397, 0.82825397, 0.82825397, 0.71959596, 0.71959596,\n",
       "        0.71959596, 0.71959596, 0.82825397, 0.82825397, 0.78871148,\n",
       "        0.80323232, 0.76368984, 0.82825397, 0.71959596, 0.82825397,\n",
       "        0.82825397, 0.71959596, 0.82825397, 0.71959596]),\n",
       " 'split3_test_score': array([0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.64571429, 0.60545455, 0.60545455, 0.68      , 0.68      ,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.6738756 , 0.68      , 0.68      , 0.61341615,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.64571429, 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.64571429, 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.60545455, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.64571429, 0.64571429,\n",
       "        0.68      , 0.64571429, 0.64571429, 0.64571429, 0.64571429,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.6738756 ,\n",
       "        0.6738756 , 0.68      , 0.6738756 , 0.6738756 , 0.68      ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.68      , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.61341615, 0.60545455,\n",
       "        0.61341615, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.64571429,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.64571429, 0.64571429, 0.64571429, 0.64571429, 0.64571429,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.6738756 , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.6738756 , 0.6738756 , 0.68      ,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.61341615, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.61341615, 0.61341615, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.64571429, 0.64571429,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.64571429, 0.64571429, 0.64571429, 0.64571429, 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.6738756 ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.6738756 ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.6738756 , 0.64080201, 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      ]),\n",
       " 'split4_test_score': array([0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.61206349, 0.66555556, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61239316, 0.61239316, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61239316, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.66555556, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61984127, 0.61239316,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.66555556, 0.6557265 ,\n",
       "        0.61984127, 0.61239316, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.66555556, 0.66555556, 0.61206349,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61239316, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.60247073, 0.61984127, 0.60247073, 0.64495425,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.61239316,\n",
       "        0.61239316, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.61239316, 0.61239316, 0.61984127, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61984127, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61239316, 0.61984127,\n",
       "        0.61984127, 0.61239316, 0.61239316, 0.61239316, 0.61984127,\n",
       "        0.6557265 , 0.66555556, 0.66555556, 0.6557265 , 0.61984127,\n",
       "        0.61239316, 0.61984127, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.60247073, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61239316, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61239316, 0.61984127, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.61984127, 0.61984127, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.61239316, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61239316, 0.61239316, 0.65461538, 0.65461538, 0.65461538,\n",
       "        0.70017094, 0.66239316, 0.65461538, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66555556,\n",
       "        0.66555556, 0.65461538, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66555556, 0.66555556, 0.61206349, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66555556, 0.66555556,\n",
       "        0.61206349, 0.66555556, 0.66555556, 0.66555556, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.61984127, 0.66239316, 0.66239316, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.61984127, 0.66239316, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.64495425, 0.64495425, 0.64495425,\n",
       "        0.64495425, 0.66239316, 0.64495425, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.66555556, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66239316, 0.66239316,\n",
       "        0.61984127, 0.66239316, 0.66239316, 0.66239316, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66239316, 0.61984127, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.61984127, 0.66239316,\n",
       "        0.61984127, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.61984127, 0.70017094, 0.70017094, 0.66239316,\n",
       "        0.61984127, 0.64495425, 0.64495425, 0.64495425, 0.64495425,\n",
       "        0.61784314, 0.64495425, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.61984127, 0.66239316, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.61984127, 0.66239316, 0.66239316,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.61984127, 0.66239316]),\n",
       " 'mean_test_score': array([0.62563349, 0.62363349, 0.62563349, 0.62567724, 0.62085108,\n",
       "        0.62810016, 0.64355762, 0.7052326 , 0.70077617, 0.69417953,\n",
       "        0.67428022, 0.67377984, 0.66473474, 0.64466287, 0.68402794,\n",
       "        0.67714022, 0.63944987, 0.66888602, 0.7250995 , 0.75808633,\n",
       "        0.72260427, 0.72185757, 0.75621949, 0.73513878, 0.68866839,\n",
       "        0.67072354, 0.666871  , 0.72045081, 0.66829497, 0.67915239,\n",
       "        0.73221949, 0.70257036, 0.72126337, 0.74355282, 0.72770911,\n",
       "        0.71589585, 0.67799109, 0.68928979, 0.68928979, 0.67791862,\n",
       "        0.66264928, 0.7041922 , 0.74440623, 0.70530346, 0.71379465,\n",
       "        0.70932416, 0.7100633 , 0.71231099, 0.67645514, 0.69189061,\n",
       "        0.68506666, 0.63891947, 0.68692134, 0.68251008, 0.72631099,\n",
       "        0.69823853, 0.69061948, 0.70297766, 0.70980061, 0.72074689,\n",
       "        0.68333506, 0.69561243, 0.71820434, 0.70599109, 0.67143738,\n",
       "        0.70537204, 0.72157186, 0.70530346, 0.73090519, 0.70297766,\n",
       "        0.70831099, 0.72631099, 0.65742597, 0.69247791, 0.67864878,\n",
       "        0.68255728, 0.68028744, 0.6636964 , 0.72631099, 0.72736924,\n",
       "        0.72064661, 0.7390581 , 0.71066393, 0.71144171, 0.64466362,\n",
       "        0.62467936, 0.62563349, 0.63392844, 0.62513311, 0.63392844,\n",
       "        0.71013818, 0.69988146, 0.69719396, 0.71675558, 0.70837817,\n",
       "        0.70734431, 0.6774993 , 0.68380572, 0.67450787, 0.67450787,\n",
       "        0.67676915, 0.67997864, 0.76338647, 0.76380435, 0.75770911,\n",
       "        0.76380435, 0.72946141, 0.72946141, 0.69496191, 0.6958514 ,\n",
       "        0.72899425, 0.72670854, 0.73161013, 0.69119076, 0.75696178,\n",
       "        0.73865538, 0.7416176 , 0.75277404, 0.72389731, 0.75252222,\n",
       "        0.71576165, 0.68448632, 0.6881022 , 0.69693133, 0.7362768 ,\n",
       "        0.72234129, 0.74764432, 0.74966508, 0.72781182, 0.73008022,\n",
       "        0.74399894, 0.71514562, 0.66293914, 0.71220001, 0.72890902,\n",
       "        0.67729267, 0.67385929, 0.71640214, 0.72948036, 0.74419888,\n",
       "        0.72780061, 0.74572032, 0.74132168, 0.75675062, 0.70884823,\n",
       "        0.71238138, 0.69978847, 0.7407127 , 0.72231099, 0.68544243,\n",
       "        0.7140633 , 0.73156984, 0.73008022, 0.73008022, 0.74776817,\n",
       "        0.75770911, 0.687428  , 0.72219888, 0.67308633, 0.71888875,\n",
       "        0.75309931, 0.6929345 , 0.75373956, 0.73714516, 0.73585212,\n",
       "        0.73714516, 0.7565967 , 0.73233959, 0.62959978, 0.62959978,\n",
       "        0.64167015, 0.62513311, 0.63363349, 0.63430566, 0.70178816,\n",
       "        0.70979825, 0.70310305, 0.69988146, 0.70344105, 0.71412692,\n",
       "        0.67809458, 0.67450787, 0.6404216 , 0.67639193, 0.64004439,\n",
       "        0.64428565, 0.74487041, 0.76948171, 0.72260427, 0.76338647,\n",
       "        0.73513878, 0.74108344, 0.70915239, 0.73033548, 0.72050593,\n",
       "        0.73547418, 0.6778514 , 0.69148199, 0.7536176 , 0.76948171,\n",
       "        0.76948171, 0.76231472, 0.76338647, 0.76338647, 0.71332442,\n",
       "        0.71166845, 0.70793594, 0.70391862, 0.71807419, 0.71984823,\n",
       "        0.74440623, 0.73116255, 0.72967293, 0.7390326 , 0.73673786,\n",
       "        0.73156984, 0.73468053, 0.6818641 , 0.67728413, 0.72536018,\n",
       "        0.73007419, 0.66458979, 0.73863478, 0.73843483, 0.73367442,\n",
       "        0.74270926, 0.71541355, 0.72835235, 0.68552007, 0.71532442,\n",
       "        0.70753767, 0.70313394, 0.68370104, 0.70455819, 0.73008022,\n",
       "        0.73282102, 0.74300776, 0.75373956, 0.73865538, 0.73008022,\n",
       "        0.71706564, 0.70753767, 0.68093914, 0.68975299, 0.72156984,\n",
       "        0.68420434, 0.74230197, 0.75315126, 0.73156984, 0.73863478,\n",
       "        0.72074689, 0.75190743, 0.63914589, 0.63425066, 0.63914589,\n",
       "        0.64336177, 0.62780621, 0.63425066, 0.6960627 , 0.6908039 ,\n",
       "        0.6908039 , 0.6902019 , 0.69767442, 0.69559245, 0.70593102,\n",
       "        0.67493161, 0.68904497, 0.67615035, 0.67750872, 0.67493161,\n",
       "        0.72402315, 0.71611465, 0.73088029, 0.72164578, 0.72402315,\n",
       "        0.7329618 , 0.6776937 , 0.70582339, 0.70445831, 0.73677577,\n",
       "        0.7029412 , 0.68675845, 0.72509694, 0.71844759, 0.7180403 ,\n",
       "        0.74034503, 0.72653909, 0.73415814, 0.73343997, 0.74434906,\n",
       "        0.69679351, 0.72663478, 0.73343997, 0.73814645, 0.72909163,\n",
       "        0.72312015, 0.73870866, 0.72312015, 0.7297695 , 0.72434503,\n",
       "        0.70126271, 0.70126271, 0.73049348, 0.70126271, 0.73221509,\n",
       "        0.6956824 , 0.72125912, 0.7297695 , 0.7297695 , 0.72312015,\n",
       "        0.7297695 , 0.7297695 , 0.70682681, 0.70682681, 0.73221509,\n",
       "        0.70126271, 0.68511253, 0.70126271, 0.72312015, 0.7297695 ,\n",
       "        0.73367426, 0.72125912, 0.72312015, 0.7297695 , 0.70126271,\n",
       "        0.67954842, 0.73221509, 0.70126271, 0.73721942, 0.70682681,\n",
       "        0.7297695 , 0.7297695 , 0.72312015, 0.73748378, 0.72312015,\n",
       "        0.71536168, 0.64249455, 0.65451313, 0.69558297, 0.65451313,\n",
       "        0.65225543, 0.66463059, 0.71835008, 0.72466159, 0.72306927,\n",
       "        0.72087427, 0.72183786, 0.71835008, 0.7551847 , 0.72693701,\n",
       "        0.70310305, 0.74827034, 0.72693701, 0.7551847 , 0.74166478,\n",
       "        0.74269162, 0.73583448, 0.74852192, 0.74269162, 0.74269162,\n",
       "        0.7431076 , 0.75770911, 0.76970911, 0.74942339, 0.76933189,\n",
       "        0.76970911, 0.77732542, 0.73977003, 0.74017733, 0.76190893,\n",
       "        0.74017733, 0.73520763, 0.75970537, 0.75142339, 0.75320807,\n",
       "        0.74635093, 0.73838936, 0.75037204, 0.73520763, 0.74017733,\n",
       "        0.72233361, 0.76190893, 0.77293923, 0.76190893, 0.74913768,\n",
       "        0.75320807, 0.71478352, 0.74725023, 0.75828054, 0.76254141,\n",
       "        0.75257559, 0.72233361, 0.75714702, 0.73084399, 0.73084399,\n",
       "        0.73084399, 0.77357171, 0.74602535, 0.74697886, 0.75320807,\n",
       "        0.75142339, 0.75320807, 0.74017733, 0.74017733, 0.73084399,\n",
       "        0.76190893, 0.75257559, 0.73084399, 0.74697886, 0.74697886,\n",
       "        0.75035436, 0.75198319, 0.74406522, 0.71205831, 0.74017733,\n",
       "        0.74406522, 0.73084399, 0.77887862, 0.73084399, 0.73084399,\n",
       "        0.64955199, 0.65102107, 0.66031032, 0.69803938, 0.65953145,\n",
       "        0.64537027, 0.7167424 , 0.72281675, 0.71249934, 0.71835008,\n",
       "        0.70773757, 0.72087427, 0.71568773, 0.7551847 , 0.71456185,\n",
       "        0.71456185, 0.72049582, 0.72496248, 0.74166478, 0.74030114,\n",
       "        0.73583448, 0.74269162, 0.77108989, 0.77108989, 0.73961387,\n",
       "        0.76970911, 0.75770911, 0.75733189, 0.75733189, 0.75770911,\n",
       "        0.74454096, 0.74454096, 0.73480034, 0.76834503, 0.73558312,\n",
       "        0.76834503, 0.72369468, 0.75828054, 0.74654141, 0.74725023,\n",
       "        0.74725023, 0.75142339, 0.75257559, 0.76954529, 0.74798139,\n",
       "        0.73084399, 0.73166695, 0.74017733, 0.74039309, 0.7630135 ,\n",
       "        0.76423838, 0.74725023, 0.7422459 , 0.75658357, 0.73895245,\n",
       "        0.75257559, 0.75257559, 0.77036825, 0.76954529, 0.74017733,\n",
       "        0.74602535, 0.74602535, 0.75387064, 0.74820374, 0.74406522,\n",
       "        0.74725023, 0.76190893, 0.76190893, 0.74017733, 0.73084399,\n",
       "        0.74017733, 0.75714702, 0.75198319, 0.73941063, 0.73811686,\n",
       "        0.74697886, 0.73907037, 0.75320807, 0.73084399, 0.75257559,\n",
       "        0.76954529, 0.74017733, 0.74406522, 0.74781369]),\n",
       " 'std_test_score': array([0.02321803, 0.02578119, 0.02321803, 0.02503287, 0.02087489,\n",
       "        0.03072917, 0.03218118, 0.10436513, 0.09838136, 0.090093  ,\n",
       "        0.07472569, 0.07381978, 0.08374054, 0.03359629, 0.07890316,\n",
       "        0.07904315, 0.02153532, 0.07324602, 0.08557649, 0.09094537,\n",
       "        0.09554788, 0.08853494, 0.09293228, 0.0985903 , 0.05746232,\n",
       "        0.01954483, 0.05386384, 0.06251363, 0.01506473, 0.04992858,\n",
       "        0.07990199, 0.083522  , 0.09834843, 0.08493073, 0.0920659 ,\n",
       "        0.07561635, 0.03382223, 0.04445134, 0.04445134, 0.03384323,\n",
       "        0.04653061, 0.06413423, 0.09154389, 0.07117208, 0.08025867,\n",
       "        0.08133157, 0.07916286, 0.08962262, 0.0243222 , 0.06035533,\n",
       "        0.027784  , 0.02897979, 0.03794806, 0.02927651, 0.0780426 ,\n",
       "        0.07573017, 0.05110603, 0.0836952 , 0.07832644, 0.06984118,\n",
       "        0.04364708, 0.08018377, 0.07265726, 0.03427737, 0.0446969 ,\n",
       "        0.07458163, 0.07100689, 0.07117208, 0.07564532, 0.0836952 ,\n",
       "        0.08007445, 0.0780426 , 0.01753633, 0.03283909, 0.04114636,\n",
       "        0.04244193, 0.03879955, 0.01439344, 0.0780426 , 0.08039771,\n",
       "        0.07911964, 0.08596194, 0.07888381, 0.08151963, 0.03232729,\n",
       "        0.02799445, 0.02321803, 0.03283191, 0.02251778, 0.03283191,\n",
       "        0.09767629, 0.10425242, 0.09434371, 0.11401362, 0.09747822,\n",
       "        0.10615529, 0.08704508, 0.05576517, 0.08234572, 0.08234572,\n",
       "        0.08208812, 0.07747334, 0.09671146, 0.09510067, 0.09064652,\n",
       "        0.09510067, 0.09090097, 0.09090097, 0.0693229 , 0.05874517,\n",
       "        0.07745722, 0.08164162, 0.07423188, 0.07340401, 0.09684444,\n",
       "        0.08262461, 0.0812258 , 0.08811734, 0.08417357, 0.08609185,\n",
       "        0.07106291, 0.04241504, 0.08145595, 0.04749514, 0.07075945,\n",
       "        0.08503576, 0.08765979, 0.08562126, 0.07442205, 0.07465538,\n",
       "        0.09157003, 0.08619062, 0.04302857, 0.07456329, 0.08067896,\n",
       "        0.04600246, 0.05189578, 0.08863632, 0.07612735, 0.0821791 ,\n",
       "        0.07589555, 0.08505665, 0.08982913, 0.0931113 , 0.06466548,\n",
       "        0.04693622, 0.03024975, 0.05935499, 0.05967638, 0.05373329,\n",
       "        0.08820193, 0.07233038, 0.07465538, 0.07465538, 0.08772967,\n",
       "        0.09064652, 0.05281496, 0.07668473, 0.0597856 , 0.07092858,\n",
       "        0.08842496, 0.0781121 , 0.09292082, 0.07811785, 0.09044814,\n",
       "        0.07811785, 0.09322984, 0.07894578, 0.02780766, 0.02780766,\n",
       "        0.03449809, 0.02251778, 0.01888666, 0.03334044, 0.09935049,\n",
       "        0.09796377, 0.10089181, 0.10425242, 0.10204203, 0.10254503,\n",
       "        0.07857233, 0.08234572, 0.02863557, 0.08138084, 0.02812399,\n",
       "        0.03320955, 0.09203434, 0.10055467, 0.09554788, 0.09671146,\n",
       "        0.0985903 , 0.09545936, 0.07174209, 0.07451339, 0.07435238,\n",
       "        0.07053037, 0.05172736, 0.08096632, 0.0916794 , 0.10055467,\n",
       "        0.10055467, 0.09737512, 0.09671146, 0.09671146, 0.05675598,\n",
       "        0.03378192, 0.06070022, 0.05662204, 0.05377743, 0.0869037 ,\n",
       "        0.09154389, 0.07202776, 0.07435407, 0.08303897, 0.07786691,\n",
       "        0.07233038, 0.0730338 , 0.03903246, 0.05823239, 0.07043427,\n",
       "        0.06315471, 0.00947221, 0.0757602 , 0.08584316, 0.08155491,\n",
       "        0.08445574, 0.07583014, 0.08335394, 0.04639977, 0.0463251 ,\n",
       "        0.04513855, 0.054231  , 0.05775808, 0.07800091, 0.07465538,\n",
       "        0.07393126, 0.08429427, 0.09292082, 0.08262461, 0.07465538,\n",
       "        0.0603409 , 0.04513855, 0.05853172, 0.07033457, 0.08457529,\n",
       "        0.04555919, 0.08425058, 0.08844579, 0.07233038, 0.0757602 ,\n",
       "        0.06984118, 0.10106827, 0.01873632, 0.02122282, 0.01873632,\n",
       "        0.03396392, 0.02721358, 0.02122282, 0.08094281, 0.07838407,\n",
       "        0.07838407, 0.07817612, 0.08198156, 0.08066149, 0.0880371 ,\n",
       "        0.08021515, 0.07435012, 0.07014699, 0.07026867, 0.08021515,\n",
       "        0.06830332, 0.07148833, 0.06147743, 0.07645163, 0.06830332,\n",
       "        0.07550192, 0.0528969 , 0.07807783, 0.09267948, 0.07430755,\n",
       "        0.0673474 , 0.05464549, 0.06703395, 0.0712402 , 0.07112528,\n",
       "        0.07592983, 0.06643856, 0.0807071 , 0.05597797, 0.0653419 ,\n",
       "        0.06132926, 0.05391719, 0.05597797, 0.06697397, 0.09417508,\n",
       "        0.08288403, 0.09279023, 0.08288403, 0.07890506, 0.08218958,\n",
       "        0.04188691, 0.04188691, 0.05463365, 0.04188691, 0.05718785,\n",
       "        0.03233696, 0.08753577, 0.07890506, 0.07890506, 0.08288403,\n",
       "        0.07890506, 0.07890506, 0.03823573, 0.03823573, 0.05718785,\n",
       "        0.04188691, 0.02964004, 0.04188691, 0.08288403, 0.07890506,\n",
       "        0.08630799, 0.08753577, 0.08288403, 0.07890506, 0.04188691,\n",
       "        0.03048661, 0.05718785, 0.04188691, 0.0607095 , 0.03823573,\n",
       "        0.07890506, 0.07890506, 0.08288403, 0.09359408, 0.08288403,\n",
       "        0.07711129, 0.02810814, 0.02714274, 0.07170824, 0.02714274,\n",
       "        0.03315057, 0.03287332, 0.08624063, 0.08910637, 0.09112828,\n",
       "        0.08510086, 0.08351124, 0.08624063, 0.10005064, 0.08763414,\n",
       "        0.10089181, 0.09407275, 0.08763414, 0.10005064, 0.0785957 ,\n",
       "        0.06508739, 0.07269264, 0.07105964, 0.06508739, 0.06508739,\n",
       "        0.08010376, 0.09086337, 0.09563332, 0.08064078, 0.09539513,\n",
       "        0.09563332, 0.09328924, 0.0703075 , 0.07056782, 0.07729383,\n",
       "        0.07056782, 0.06671328, 0.07349831, 0.09082916, 0.07589703,\n",
       "        0.08338128, 0.07408753, 0.07252151, 0.06671328, 0.07056782,\n",
       "        0.07698995, 0.07729383, 0.08902032, 0.07729383, 0.09496422,\n",
       "        0.07589703, 0.06042882, 0.06866317, 0.08359601, 0.07648563,\n",
       "        0.07663443, 0.07698995, 0.09978432, 0.06687796, 0.06687796,\n",
       "        0.06687796, 0.08824048, 0.06989549, 0.07268086, 0.07589703,\n",
       "        0.09082916, 0.07589703, 0.07056782, 0.07056782, 0.06687796,\n",
       "        0.07729383, 0.07663443, 0.06687796, 0.07268086, 0.07268086,\n",
       "        0.06693085, 0.07710838, 0.08773548, 0.07264826, 0.07056782,\n",
       "        0.08773548, 0.06687796, 0.1010637 , 0.06687796, 0.06687796,\n",
       "        0.02433977, 0.03692098, 0.03239772, 0.06261441, 0.03349868,\n",
       "        0.02948181, 0.0813128 , 0.09084857, 0.08877784, 0.08624063,\n",
       "        0.09078138, 0.08510086, 0.08503396, 0.10005064, 0.08556407,\n",
       "        0.08556407, 0.0834188 , 0.08806557, 0.0785957 , 0.07709762,\n",
       "        0.07269264, 0.06508739, 0.08596312, 0.08596312, 0.07083001,\n",
       "        0.09563332, 0.09086337, 0.09081224, 0.09081224, 0.09086337,\n",
       "        0.06983096, 0.06983096, 0.06669298, 0.08529054, 0.06347161,\n",
       "        0.08529054, 0.05583826, 0.08359601, 0.09566904, 0.06866317,\n",
       "        0.06866317, 0.09082916, 0.07663443, 0.10212316, 0.07096185,\n",
       "        0.06687796, 0.08120018, 0.07056782, 0.0763217 , 0.09008303,\n",
       "        0.0888965 , 0.06866317, 0.06327769, 0.07011089, 0.07164662,\n",
       "        0.07663443, 0.07663443, 0.11174188, 0.10212316, 0.07056782,\n",
       "        0.06989549, 0.06989549, 0.10115042, 0.07148023, 0.08773548,\n",
       "        0.06866317, 0.07729383, 0.07729383, 0.07056782, 0.06687796,\n",
       "        0.07056782, 0.09978432, 0.07710838, 0.07755577, 0.06192695,\n",
       "        0.07268086, 0.06813901, 0.07589703, 0.06687796, 0.07663443,\n",
       "        0.10212316, 0.07056782, 0.08773548, 0.09882572]),\n",
       " 'rank_test_score': array([497, 503, 497, 496, 504, 494, 476, 354, 375, 392, 449, 451, 458,\n",
       "        474, 417, 439, 482, 455, 254,  39, 272, 279,  53, 181, 406, 454,\n",
       "        457, 294, 456, 428, 193, 366, 284, 121, 243, 309, 431, 403, 403,\n",
       "        432, 463, 357, 112, 352, 321, 335, 332, 325, 441, 395, 414, 485,\n",
       "        409, 422, 250, 379, 400, 363, 333, 289, 420, 388, 301, 348, 453,\n",
       "        351, 282, 352, 203, 363, 339, 250, 466, 394, 429, 421, 425, 461,\n",
       "        250, 244, 291, 155, 330, 329, 473, 502, 497, 489, 500, 489, 331,\n",
       "        376, 382, 305, 338, 344, 436, 418, 446, 446, 440, 426,  23,  21,\n",
       "         40,  21, 235, 235, 391, 386, 238, 247, 198, 397,  49, 159, 133,\n",
       "         68, 261,  75, 310, 415, 407, 383, 173, 274,  93,  84, 241, 218,\n",
       "        120, 315, 462, 326, 239, 437, 450, 307, 234, 115, 242, 108, 134,\n",
       "         50, 337, 324, 378, 136, 277, 412, 320, 199, 218, 218,  92,  40,\n",
       "        408, 278, 452, 296,  67, 393,  58, 169, 174, 169,  51, 192, 492,\n",
       "        492, 479, 500, 491, 486, 367, 334, 361, 376, 359, 319, 430, 446,\n",
       "        480, 442, 481, 475, 109,  14, 272,  23, 181, 135, 336, 217, 292,\n",
       "        178, 433, 396,  60,  14,  14,  29,  23,  23, 322, 328, 340, 358,\n",
       "        302, 295, 112, 202, 233, 156, 172, 199, 184, 423, 438, 253, 223,\n",
       "        460, 161, 163, 186, 124, 312, 240, 411, 314, 342, 360, 419, 355,\n",
       "        218, 191, 123,  58, 159, 218, 304, 342, 424, 402, 283, 416, 129,\n",
       "         66, 199, 161, 289,  78, 483, 487, 483, 477, 495, 487, 385, 398,\n",
       "        398, 401, 381, 389, 349, 444, 405, 443, 435, 444, 259, 308, 204,\n",
       "        281, 259, 190, 434, 350, 356, 171, 365, 410, 255, 297, 303, 138,\n",
       "        249, 185, 188, 114, 384, 248, 188, 165, 237, 263, 158, 263, 224,\n",
       "        258, 368, 368, 216, 368, 194, 387, 285, 224, 224, 263, 224, 224,\n",
       "        345, 345, 194, 368, 413, 368, 263, 224, 187, 285, 263, 224, 368,\n",
       "        427, 194, 368, 168, 345, 224, 224, 263, 167, 263, 313, 478, 467,\n",
       "        390, 467, 469, 459, 298, 257, 270, 287, 280, 298,  54, 245, 361,\n",
       "         88, 245,  54, 131, 125, 175,  87, 125, 125, 122,  40,   8,  85,\n",
       "         17,   8,   2, 151, 140,  30, 140, 179,  36,  79,  61, 104, 164,\n",
       "         82, 179, 140, 275,  30,   4,  30,  86,  61, 316,  94,  37,  28,\n",
       "         69, 275,  47, 205, 205, 205,   3, 105,  99,  61,  79,  61, 140,\n",
       "        140, 205,  30,  69, 205,  99,  99,  83,  76, 116, 327, 140, 116,\n",
       "        205,   1, 205, 205, 471, 470, 464, 380, 465, 472, 306, 271, 323,\n",
       "        298, 341, 287, 311,  54, 317, 317, 293, 256, 131, 139, 175, 125,\n",
       "          5,   5, 152,   8,  40,  45,  45,  40, 110, 110, 183,  18, 177,\n",
       "         18, 262,  37, 103,  94,  94,  79,  69,  11,  90, 205, 197, 140,\n",
       "        137,  27,  20,  94, 130,  52, 157,  69,  69,   7,  11, 140, 105,\n",
       "        105,  57,  89, 116,  94,  30,  30, 140, 205, 140,  47,  76, 153,\n",
       "        166,  99, 154,  61, 205,  69,  11, 140, 116,  91]),\n",
       " 'split0_train_score': array([0.81173957, 0.80062846, 0.81011968, 0.81590623, 0.80318287,\n",
       "        0.81011968, 0.83690976, 0.86703964, 0.83969887, 0.86703964,\n",
       "        0.85945408, 0.87094332, 0.91879611, 0.94488003, 0.95401603,\n",
       "        0.94488003, 0.93737813, 0.94770283, 0.95401603, 0.96699634,\n",
       "        0.95401603, 0.96699634, 0.95401603, 0.96699634, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.81685786,\n",
       "        0.86101307, 0.80829765, 0.80318287, 0.79427258, 0.8199757 ,\n",
       "        0.87335762, 0.86813187, 0.86497438, 0.86497438, 0.87012286,\n",
       "        0.85455978, 0.95401603, 0.95401603, 0.95401603, 0.95401603,\n",
       "        0.95401603, 0.95401603, 0.96699634, 0.97411477, 0.97411477,\n",
       "        0.97411477, 0.95401603, 0.97411477, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.79646179, 0.83899986,\n",
       "        0.84761258, 0.80829765, 0.84411465, 0.80162304, 0.86824539,\n",
       "        0.86998039, 0.85455978, 0.8629415 , 0.86497438, 0.87868132,\n",
       "        0.95401603, 0.95401603, 0.95401603, 0.95401603, 0.96231884,\n",
       "        0.95401603, 0.97411477, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.95401603, 0.95401603, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.85367307, 0.85367307, 0.80829765,\n",
       "        0.84205731, 0.84693489, 0.85367307, 0.86482804, 0.83008765,\n",
       "        0.87628788, 0.87377326, 0.83894546, 0.87913133, 0.94488003,\n",
       "        0.94488003, 0.94488003, 0.92962921, 0.95401603, 0.94488003,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.97411477, 1.        , 1.        , 1.        , 0.99169719,\n",
       "        1.        , 0.99169719, 0.99169719, 0.99169719, 0.99169719,\n",
       "        0.99169719, 0.99169719, 0.99169719, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.81173957, 0.81550929, 0.79805075, 0.81550929,\n",
       "        0.83392602, 0.8206746 , 0.89381349, 0.86204931, 0.90155083,\n",
       "        0.89321814, 0.90155083, 0.89321814, 0.95401603, 0.9411947 ,\n",
       "        0.94869505, 0.95401603, 0.94869505, 0.95401603, 0.94869505,\n",
       "        0.95401603, 0.95401603, 0.97411477, 0.97411477, 0.95401603,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.8206746 , 0.81550929, 0.81550929, 0.8121179 , 0.80695665,\n",
       "        0.8206746 , 0.89321814, 0.86204931, 0.86204931, 0.86204931,\n",
       "        0.89321814, 0.89321814, 0.95401603, 0.95401603, 0.9411947 ,\n",
       "        0.95401603, 0.95401603, 0.95401603, 0.95401603, 0.97411477,\n",
       "        0.96879274, 0.95401603, 0.94869505, 0.95401603, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split1_train_score': array([0.79546485, 0.82887923, 0.83888634, 0.80239708, 0.79238997,\n",
       "        0.83888634, 0.8549542 , 0.83785777, 0.81059719, 0.86297551,\n",
       "        0.83769955, 0.83769955, 0.93511547, 0.92774368, 0.94434548,\n",
       "        0.91996396, 0.91259216, 0.94434548, 0.95693637, 0.94436926,\n",
       "        0.95693637, 0.94981428, 0.95148936, 0.94436926, 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.80239708,\n",
       "        0.82993804, 0.83888634, 0.83888634, 0.80510131, 0.84003795,\n",
       "        0.83730159, 0.85768879, 0.84949245, 0.88773145, 0.84708645,\n",
       "        0.83092859, 0.94434548, 0.91826156, 0.94434548, 0.94434548,\n",
       "        0.93697368, 0.92935079, 0.95148936, 0.97149711, 0.95148936,\n",
       "        0.97529289, 0.95148936, 0.95148936, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.79662058, 0.80239708,\n",
       "        0.84857882, 0.81208956, 0.79344877, 0.80137538, 0.81298452,\n",
       "        0.80079365, 0.84949245, 0.84949245, 0.86521061, 0.83730159,\n",
       "        0.90496927, 0.93697368, 0.94434548, 0.91090326, 0.92935079,\n",
       "        0.92935079, 0.95693637, 0.94981428, 0.95519521, 0.96231884,\n",
       "        0.95148936, 0.95693637, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.87004499, 0.87004499, 0.87004499,\n",
       "        0.86311276, 0.87004499, 0.87004499, 0.87009617, 0.87922394,\n",
       "        0.87163089, 0.87163089, 0.88737287, 0.88066129, 0.90496927,\n",
       "        0.89584151, 0.92774368, 0.92935079, 0.92935079, 0.93697368,\n",
       "        0.94436926, 0.94436926, 0.96437502, 0.94436926, 0.94436926,\n",
       "        0.94436926, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.82820818, 0.82820818, 0.82820818, 0.83888634,\n",
       "        0.83358634, 0.83888634, 0.83786465, 0.82887923, 0.82887923,\n",
       "        0.82887923, 0.82887923, 0.82887923, 0.93074353, 0.93347252,\n",
       "        0.92017539, 0.93876521, 0.91636003, 0.92549283, 0.9411781 ,\n",
       "        0.9411781 , 0.9411781 , 0.9411781 , 0.9411781 , 0.9411781 ,\n",
       "        1.        , 1.        , 0.99267399, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.82820818, 0.83358634, 0.83888634, 0.83888634, 0.83888634,\n",
       "        0.82820818, 0.82887923, 0.81967366, 0.82609994, 0.82887923,\n",
       "        0.82887923, 0.82887923, 0.93347252, 0.92160752, 0.93347252,\n",
       "        0.91636003, 0.92433972, 0.92549283, 0.93577797, 0.94651413,\n",
       "        0.93577797, 0.9411781 , 0.9411781 , 0.9411781 , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split2_train_score': array([0.87003968, 0.82142018, 0.86071429, 0.85540828, 0.85892857,\n",
       "        0.86071429, 0.89746791, 0.90654611, 0.88214627, 0.86720856,\n",
       "        0.90654611, 0.86720856, 0.94762971, 0.94762971, 0.94233535,\n",
       "        0.94762971, 0.93876521, 0.94762971, 0.98241758, 0.98241758,\n",
       "        0.97513935, 0.98241758, 0.97513935, 0.97513935, 0.98241758,\n",
       "        0.98241758, 0.98241758, 0.98241758, 0.97513935, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.88914502,\n",
       "        0.85980335, 0.85476651, 0.88814252, 0.88814252, 0.88048934,\n",
       "        0.88253311, 0.91506892, 0.89138073, 0.89138073, 0.8965074 ,\n",
       "        0.8965074 , 0.94762971, 0.94762971, 0.94651413, 0.95401603,\n",
       "        0.94651413, 0.94762971, 0.97411477, 0.97411477, 0.96564127,\n",
       "        0.98241758, 0.97411477, 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.85892857, 0.88383262,\n",
       "        0.88914502, 0.88914502, 0.85476651, 0.88383262, 0.91447421,\n",
       "        0.89944378, 0.91321915, 0.8965074 , 0.8965074 , 0.90916073,\n",
       "        0.95401603, 0.94762971, 0.94651413, 0.94762971, 0.94762971,\n",
       "        0.94762971, 0.97411477, 0.97411477, 0.98241758, 0.98241758,\n",
       "        0.96564127, 0.98241758, 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.80563795, 0.84145494, 0.84145494,\n",
       "        0.8095004 , 0.8041549 , 0.81280395, 0.88048934, 0.90741575,\n",
       "        0.88048934, 0.87521639, 0.88048934, 0.88048934, 0.93876521,\n",
       "        0.93876521, 0.93876521, 0.93876521, 0.93876521, 0.93876521,\n",
       "        0.96656664, 0.96656664, 0.96656664, 0.96656664, 0.96656664,\n",
       "        0.96656664, 0.99267399, 0.99267399, 0.99267399, 0.99267399,\n",
       "        0.99267399, 0.99267399, 0.99169719, 0.99169719, 0.99169719,\n",
       "        0.99169719, 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.87432468, 0.86876477, 0.84607294, 0.83350679,\n",
       "        0.87432468, 0.86366722, 0.8646152 , 0.8646152 , 0.8646152 ,\n",
       "        0.8646152 , 0.85963486, 0.8646152 , 0.94969155, 0.95780134,\n",
       "        0.95877295, 0.95877295, 0.93074353, 0.95877295, 0.96564127,\n",
       "        0.96564127, 0.96564127, 0.96564127, 0.96656664, 0.96564127,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.83886905, 0.86366722, 0.86876477, 0.86366722, 0.84607294,\n",
       "        0.87432468, 0.85467865, 0.86962469, 0.85963486, 0.86962469,\n",
       "        0.85963486, 0.8646152 , 0.95780134, 0.95780134, 0.95877295,\n",
       "        0.95780134, 0.95877295, 0.93876521, 0.96564127, 0.96656664,\n",
       "        0.96564127, 0.96564127, 0.96564127, 0.96656664, 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split3_train_score': array([0.83975335, 0.84802551, 0.84802551, 0.84802551, 0.84802551,\n",
       "        0.84802551, 0.89925186, 0.89517114, 0.89507134, 0.89517114,\n",
       "        0.87258201, 0.88177293, 0.95946276, 0.97411477, 0.95045177,\n",
       "        0.94333333, 0.93579976, 0.97411477, 0.96699634, 0.96699634,\n",
       "        0.96699634, 0.96699634, 0.96699634, 0.94333333, 1.        ,\n",
       "        0.99169719, 0.99169719, 0.99169719, 0.98241758, 0.99169719,\n",
       "        0.99169719, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.84802551,\n",
       "        0.87285657, 0.88703991, 0.84802551, 0.8428501 , 0.85176646,\n",
       "        0.84794083, 0.88709663, 0.88630631, 0.88630631, 0.87789988,\n",
       "        0.88630631, 0.96656664, 0.96656664, 0.96656664, 0.97411477,\n",
       "        0.95045177, 0.96656664, 0.96699634, 0.96699634, 0.96699634,\n",
       "        0.97411477, 0.96699634, 0.96699634, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99169719, 0.99169719, 1.        ,\n",
       "        1.        , 1.        , 0.99169719, 1.        , 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.84802551, 0.87995216,\n",
       "        0.88177293, 0.8428501 , 0.84802551, 0.8428501 , 0.87789988,\n",
       "        0.88709663, 0.87789988, 0.89442802, 0.88630631, 0.89517114,\n",
       "        0.97411477, 0.97411477, 0.96656664, 0.96656664, 0.96656664,\n",
       "        0.95045177, 0.97411477, 0.97411477, 0.97411477, 0.96699634,\n",
       "        0.96699634, 0.97411477, 1.        , 0.99169719, 0.99169719,\n",
       "        1.        , 1.        , 0.99169719, 0.99169719, 0.99169719,\n",
       "        1.        , 0.99169719, 1.        , 0.99169719, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.84802551, 0.84802551, 0.8428501 ,\n",
       "        0.84969281, 0.84802551, 0.85481948, 0.87285657, 0.87654718,\n",
       "        0.87275139, 0.88709663, 0.88177293, 0.90476471, 0.95743064,\n",
       "        0.94963695, 0.96656664, 0.94963695, 0.95877295, 0.95743064,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.96879274, 0.97411477,\n",
       "        0.97411477, 1.        , 0.99169719, 0.99169719, 0.99169719,\n",
       "        0.99169719, 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.86393544, 0.86393544, 0.88886852, 0.86382741,\n",
       "        0.86393544, 0.85860917, 0.87985762, 0.87985762, 0.87985762,\n",
       "        0.88901099, 0.88386251, 0.88901099, 0.96656664, 0.96656664,\n",
       "        0.96656664, 0.96656664, 0.96656664, 0.96656664, 0.97411477,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.99169719, 1.        , 0.99169719, 0.99169719, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.85343377, 0.86393544, 0.85860917, 0.88886852, 0.88901099,\n",
       "        0.85860917, 0.86763271, 0.88901099, 0.87985762, 0.87985762,\n",
       "        0.87479161, 0.87985762, 0.96656664, 0.96656664, 0.96656664,\n",
       "        0.96656664, 0.96656664, 0.96656664, 0.97411477, 0.97411477,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.97411477, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split4_train_score': array([0.87932186, 0.83976199, 0.86171998, 0.81447254, 0.84676924,\n",
       "        0.8540424 , 0.89774506, 0.90626253, 0.88227216, 0.90573427,\n",
       "        0.91347267, 0.90808858, 0.98441948, 0.98441948, 0.96954694,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.97724316,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 0.98441948, 1.        , 0.98441948,\n",
       "        0.98441948, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.88459659,\n",
       "        0.88985806, 0.87493249, 0.85112383, 0.88689845, 0.90328105,\n",
       "        0.90056047, 0.88665248, 0.90289453, 0.89101957, 0.90875457,\n",
       "        0.93781311, 0.96954694, 0.96954694, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.96756548, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 1.        ,\n",
       "        1.        , 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.88791462, 0.83297305,\n",
       "        0.88007554, 0.89216994, 0.89216994, 0.85905995, 0.91559654,\n",
       "        0.91808858, 0.93781311, 0.91808858, 0.91808858, 0.91808858,\n",
       "        0.97709402, 0.98441948, 0.98441948, 0.98441948, 0.97709402,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.97904429,\n",
       "        0.97904429, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 1.        , 0.98441948,\n",
       "        0.98441948, 0.98441948, 1.        , 0.98441948, 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.83625796, 0.8495927 , 0.81227165,\n",
       "        0.8018497 , 0.81591391, 0.85297384, 0.88201511, 0.87278852,\n",
       "        0.88201511, 0.88201511, 0.87278852, 0.89605275, 0.96861898,\n",
       "        0.95918369, 0.98441948, 0.97709402, 0.95918369, 0.96756548,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.97621435, 0.97904429,\n",
       "        0.98441948, 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.98441948, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.8552221 , 0.85796445, 0.8571185 , 0.85441931,\n",
       "        0.85441931, 0.85441931, 0.89173011, 0.90657345, 0.88505531,\n",
       "        0.92085719, 0.89286759, 0.87061096, 0.97171997, 0.97171997,\n",
       "        0.97171997, 0.97904429, 0.97171997, 0.98441948, 0.97904429,\n",
       "        0.97904429, 0.97904429, 0.97904429, 0.97904429, 0.97904429,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.8552221 , 0.85441931, 0.8552221 , 0.86478338, 0.86478338,\n",
       "        0.872899  , 0.93917273, 0.9156507 , 0.92375068, 0.87722329,\n",
       "        0.85562962, 0.91588621, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.97904429, 0.97709402, 0.97904429, 0.97904429, 0.97904429,\n",
       "        0.97904429, 0.97904429, 0.97904429, 0.97904429, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'mean_train_score': array([0.83926386, 0.82774307, 0.84389316, 0.82724193, 0.82985923,\n",
       "        0.84235764, 0.87726576, 0.88257544, 0.86195717, 0.87962582,\n",
       "        0.87795088, 0.87314259, 0.94908471, 0.95575753, 0.95213911,\n",
       "        0.9480453 , 0.94179095, 0.95964246, 0.96895716, 0.96760454,\n",
       "        0.96750151, 0.9701288 , 0.96641211, 0.96285155, 0.99336741,\n",
       "        0.99170685, 0.99170685, 0.99170685, 0.9848788 , 0.99170685,\n",
       "        0.99833944, 1.        , 0.9968839 , 1.        , 0.9968839 ,\n",
       "        0.9968839 , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.84820441,\n",
       "        0.86269382, 0.85278458, 0.84587221, 0.84345299, 0.8591101 ,\n",
       "        0.86833872, 0.88292774, 0.87900968, 0.88428249, 0.88007423,\n",
       "        0.88122304, 0.95642096, 0.95120418, 0.95917235, 0.96218236,\n",
       "        0.95447502, 0.95302573, 0.96880326, 0.97422849, 0.96853224,\n",
       "        0.9780719 , 0.9662072 , 0.97188751, 0.9968839 , 0.9968839 ,\n",
       "        0.9968839 , 0.9968839 , 0.98819037, 0.99170685, 0.99648352,\n",
       "        1.        , 0.9968839 , 0.99522333, 0.9968839 , 0.99522333,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.9968839 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.83759021, 0.84763095,\n",
       "        0.86943698, 0.84891046, 0.84650508, 0.83774822, 0.87784011,\n",
       "        0.87508061, 0.88659687, 0.88429159, 0.88621745, 0.88768067,\n",
       "        0.95284203, 0.95943074, 0.95917235, 0.95270702, 0.956592  ,\n",
       "        0.95317356, 0.97274003, 0.97131562, 0.97405236, 0.97297836,\n",
       "        0.96343746, 0.97038085, 0.99336741, 0.99522333, 0.99522333,\n",
       "        0.9968839 , 0.99336741, 0.98819037, 0.99833944, 0.99522333,\n",
       "        0.9968839 , 0.99522333, 1.        , 0.99522333, 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.9968839 , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.84272789, 0.85255824, 0.83498387,\n",
       "        0.8332426 , 0.83701484, 0.84886307, 0.87405705, 0.87321261,\n",
       "        0.87663492, 0.87794645, 0.87227382, 0.88821988, 0.94293283,\n",
       "        0.93766148, 0.95247501, 0.94489524, 0.94801774, 0.94912301,\n",
       "        0.96871699, 0.96871699, 0.97271814, 0.96601155, 0.96764195,\n",
       "        0.96871699, 0.9985348 , 0.99687424, 0.99687424, 0.99521368,\n",
       "        0.99375813, 0.99521368, 0.99667888, 0.99667888, 0.99667888,\n",
       "        0.99667888, 0.99667888, 0.99833944, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.84668599, 0.84687643, 0.84366378, 0.84122983,\n",
       "        0.85203836, 0.84725133, 0.87357621, 0.86839496, 0.87199164,\n",
       "        0.87931615, 0.873359  , 0.8692669 , 0.95454754, 0.95415103,\n",
       "        0.953186  , 0.95943302, 0.94681704, 0.95785359, 0.9617347 ,\n",
       "        0.96279889, 0.96279889, 0.96681864, 0.96700372, 0.96279889,\n",
       "        0.99833944, 1.        , 0.99687424, 0.99833944, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.83928154, 0.84622352, 0.84739833, 0.85366467, 0.84914206,\n",
       "        0.85094313, 0.87671629, 0.87120187, 0.87027848, 0.86352683,\n",
       "        0.86243069, 0.87649128, 0.9592552 , 0.9568822 , 0.95688526,\n",
       "        0.95475767, 0.95615787, 0.952777  , 0.96171887, 0.96807092,\n",
       "        0.96467421, 0.96279889, 0.9617347 , 0.96298397, 0.9985348 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'std_train_score': array([0.03233566, 0.01632363, 0.01888671, 0.02065973, 0.02674533,\n",
       "        0.01764631, 0.02621957, 0.0265908 , 0.0317809 , 0.01739621,\n",
       "        0.02853283, 0.02276824, 0.02223018, 0.02062839, 0.00965668,\n",
       "        0.02070766, 0.02338496, 0.01639638, 0.0125845 , 0.01306099,\n",
       "        0.01129858, 0.01255024, 0.01246694, 0.01646755, 0.00814786,\n",
       "        0.00744243, 0.00744243, 0.00744243, 0.00819553, 0.00744243,\n",
       "        0.00332112, 0.        , 0.00623221, 0.        , 0.00623221,\n",
       "        0.00623221, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.03487542,\n",
       "        0.01962294, 0.02771329, 0.02716681, 0.03943245, 0.02952058,\n",
       "        0.02299691, 0.0195971 , 0.01920951, 0.00984447, 0.02137551,\n",
       "        0.03661242, 0.01004105, 0.01832136, 0.01482395, 0.01475401,\n",
       "        0.01601712, 0.01403824, 0.01075527, 0.00572095, 0.0108143 ,\n",
       "        0.00443209, 0.01232947, 0.0119407 , 0.00623221, 0.00623221,\n",
       "        0.00623221, 0.00623221, 0.00682107, 0.00744243, 0.00703297,\n",
       "        0.        , 0.00623221, 0.0062866 , 0.00623221, 0.0062866 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00623221,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.03596338, 0.03062899,\n",
       "        0.01769253, 0.03614446, 0.03154846, 0.03234811, 0.03759233,\n",
       "        0.04032914, 0.03408988, 0.02474274, 0.02007095, 0.02849976,\n",
       "        0.0258273 , 0.01739517, 0.01482395, 0.02438366, 0.0165833 ,\n",
       "        0.01779148, 0.00885251, 0.01146756, 0.01032525, 0.00743769,\n",
       "        0.00992588, 0.0126844 , 0.00814786, 0.0062866 , 0.0062866 ,\n",
       "        0.00623221, 0.00814786, 0.00682107, 0.00332112, 0.0062866 ,\n",
       "        0.00623221, 0.0062866 , 0.        , 0.0062866 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00623221, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.02150167, 0.00958804, 0.02263198,\n",
       "        0.02362093, 0.02381452, 0.01910208, 0.00643443, 0.02480747,\n",
       "        0.00409989, 0.00574517, 0.01730175, 0.01034003, 0.02159795,\n",
       "        0.02194927, 0.02037795, 0.01772853, 0.0119197 , 0.01167934,\n",
       "        0.01343336, 0.01343336, 0.00704657, 0.0113683 , 0.01230108,\n",
       "        0.01343336, 0.0029304 , 0.0038407 , 0.0038407 , 0.00392426,\n",
       "        0.00584047, 0.00392426, 0.00406753, 0.00406753, 0.00406753,\n",
       "        0.00406753, 0.00406753, 0.00332112, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.02322494, 0.02109718, 0.0301653 , 0.01680058,\n",
       "        0.01620138, 0.01565987, 0.02066003, 0.02531812, 0.02457385,\n",
       "        0.03089632, 0.02627733, 0.02288274, 0.01435162, 0.01463972,\n",
       "        0.01824079, 0.01335985, 0.02097483, 0.01920653, 0.01456541,\n",
       "        0.0137451 , 0.0137451 , 0.01352595, 0.0135149 , 0.0137451 ,\n",
       "        0.00332112, 0.        , 0.0038407 , 0.00332112, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01358966, 0.01891152, 0.01861604, 0.02610564, 0.02730711,\n",
       "        0.02245256, 0.03749579, 0.03173098, 0.03188663, 0.01840712,\n",
       "        0.02135617, 0.02915829, 0.01661931, 0.02052049, 0.01816467,\n",
       "        0.02104036, 0.01772279, 0.01909144, 0.01550147, 0.01149286,\n",
       "        0.01515595, 0.0137451 , 0.01456541, 0.01378829, 0.0029304 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ])}"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search 2\n",
    "\n",
    "# Usando um grid completo de todos os parâmetros\n",
    "param_grid = {\"max_depth\": range(3,10,1),\n",
    "              \"min_samples_split\": range(1,4),\n",
    "              \"max_features\": ['sqrt', 'log2'],\n",
    "              \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"n_estimators\": range(400,600,100),\n",
    "              \"bootstrap\": [True, False]}\n",
    "\n",
    "# Executando o Grid Search\n",
    "grid_search = GridSearchCV(modelo4_multi, param_grid = param_grid, return_train_score = True, scoring = 'f1_macro')\n",
    "start = time.time()\n",
    "grid_search.fit(X_falha_treino, y_falha_treino)\n",
    "\n",
    "print(\"GridSearchCV executou em %.2f segundos para todas as combinações de candidatos a parâmetros do modelo.\"\n",
    "      % (time.time() - start))\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "7ee1211b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'entropy',\n",
       " 'max_depth': 9,\n",
       " 'max_features': 'log2',\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "fd3983a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7788786243594428"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dfbc3",
   "metadata": {},
   "source": [
    "A segunda tentativa de otimização não resultou em uma melhora expressiva da métrica, vamos ver como fica a matriz de confusão e a acurácia do modelo em teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "6913febb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusão\n",
      " [[19  0  0  0  0]\n",
      " [ 0 11  1  1  0]\n",
      " [ 0  1  9  5  1]\n",
      " [ 0  0  0  3  0]\n",
      " [ 0  0  0  1  6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'XGBoost Classifier Grid Search 2',\n",
       " 'Versão': 'Otimizada',\n",
       " 'Precision': 0.7947619047619047,\n",
       " 'Recall': 0.8531593406593407,\n",
       " 'F1 Score': 0.7781978021978022,\n",
       " 'Acurácia': 0.8275862068965517}"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes_multi = grid_search.predict(X_falha_val)\n",
    "\n",
    "# Avaliação do modelo\n",
    "# Matriz de confusão\n",
    "print('\\nMatriz de confusão\\n', confusion_matrix(y_falha_val, previsoes_multi))\n",
    "\n",
    "# Dicionário de métricas e metadados\n",
    "dict_model =   {'Modelo': 'XGBoost Classifier Grid Search 2',\n",
    "                'Versão': 'Otimizada',\n",
    "                'Precision':precision_score(y_falha_val, previsoes_multi, average = 'macro', zero_division=0),\n",
    "                'Recall':recall_score(y_falha_val, previsoes_multi, average = 'macro', zero_division=0),\n",
    "                'F1 Score':f1_score(y_falha_val, previsoes_multi, average = 'macro', zero_division=0),\n",
    "                'Acurácia':accuracy_score(y_falha_val, previsoes_multi)}\n",
    "\n",
    "dict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "7229dcc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 3, 4, 1, 0, 2, 2, 0, 3, 4, 1, 1, 0, 4, 2, 4, 0, 2, 0, 0, 3,\n",
       "       0, 1, 0, 1, 2, 0, 0, 1, 3, 1, 3, 0, 0, 1, 0, 3, 0, 4, 3, 0, 3, 2,\n",
       "       2, 0, 1, 4, 1, 2, 1, 2, 0, 4, 0, 3, 2, 1])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7797489",
   "metadata": {},
   "source": [
    "No nosso conjunto de dados de validação, o modelo acertou todas 48 das 58 previsões, atingindo 82,75% de acurácia. Vamos usar esse como o segundo modelo do nosso pipeline para fazer as previsões completas e ver como ficam as métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243d43a",
   "metadata": {},
   "source": [
    "# Avaliação do pipeline completo\n",
    "\n",
    "Ao longo da modelagem, construímos 2 modelos preditivos. Um de classificação binária, para prever se havia ou não falha nas máquinas, e outro de classificação multiclasse, para prever se, dado que o modelo previu que havia uma falha, que tipo de falha ela seria. O segundo modelo ainda pode classificar como \"No Failure\", sendo assim um segundo filtro. \n",
    "\n",
    "Avaliamos cada modelo e suas previsões separadamente, mas é interessante prever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "70b6872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "2f99132a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>-1.078106</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>0.307330</td>\n",
       "      <td>-0.059381</td>\n",
       "      <td>-0.499925</td>\n",
       "      <td>0.232524</td>\n",
       "      <td>0.686992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>-0.691228</td>\n",
       "      <td>0.402605</td>\n",
       "      <td>0.174797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>0.457986</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>0.142480</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>-0.777961</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>2.785976</td>\n",
       "      <td>-2.338711</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532</th>\n",
       "      <td>-0.747267</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>1.030495</td>\n",
       "      <td>-1.068101</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>0.940470</td>\n",
       "      <td>-1.408264</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>-1.902302</td>\n",
       "      <td>-2.141734</td>\n",
       "      <td>0.659143</td>\n",
       "      <td>-0.898019</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>-1.098799</td>\n",
       "      <td>-0.663935</td>\n",
       "      <td>-0.460539</td>\n",
       "      <td>0.112466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "5089           0.206892               0.948209              0.698529   \n",
       "2028           0.307330              -0.059381             -0.499925   \n",
       "2438           1.060613               0.881037             -0.691228   \n",
       "4076           0.457986               0.612346             -0.342382   \n",
       "2330           0.809518               0.343655              0.360936   \n",
       "...                 ...                    ...                   ...   \n",
       "6252          -1.249456              -1.335661              2.785976   \n",
       "6532          -0.747267              -0.529589              1.030495   \n",
       "6002          -1.400113              -1.335661              0.940470   \n",
       "664           -1.902302              -2.141734              0.659143   \n",
       "454           -1.098799              -0.663935             -0.460539   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  prediction  \n",
       "5089  -1.078106       0.524390  0.0  1.0  0.0           1  \n",
       "2028   0.232524       0.686992  0.0  0.0  1.0           1  \n",
       "2438   0.402605       0.174797  0.0  1.0  0.0           1  \n",
       "4076   0.142480       0.406504  0.0  1.0  0.0           1  \n",
       "2330  -0.777961       0.008130  1.0  0.0  0.0           1  \n",
       "...         ...            ...  ...  ...  ...         ...  \n",
       "6252  -2.338711       0.520325  0.0  0.0  1.0           1  \n",
       "6532  -1.068101       0.691057  0.0  0.0  1.0           1  \n",
       "6002  -1.408264       0.813008  0.0  1.0  0.0           1  \n",
       "664   -0.898019       0.552846  0.0  1.0  0.0           1  \n",
       "454    0.112466       0.000000  0.0  1.0  0.0           1  \n",
       "\n",
       "[1667 rows x 9 columns]"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_bin = modelo3.predict(X_val) # Previsoes do modelo binario\n",
    "\n",
    "validacao_com_prev = X_val.copy()\n",
    "validacao_com_prev['prediction'] = prev_bin\n",
    "\n",
    "prev_falha = X_val[prev_bin == 0] # Registros em que o primeiro modelo previu falha\n",
    "\n",
    "prev_multi = grid_search.predict(prev_falha)# Previsões do modelo multiclasse\n",
    "prev_multi = np.where(prev_multi == 0, prev_multi, prev_multi+1) # Convertendo de volta para o encoding original\n",
    "\n",
    "validacao_com_prev_slice =  prev_falha.copy()\n",
    "validacao_com_prev_slice['prediction'] = prev_multi\n",
    "\n",
    "validacao_com_prev.loc[prev_falha.index] = validacao_com_prev_slice\n",
    "\n",
    "validacao_com_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "c3084130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  18,    1,    0,    0,    0,    0],\n",
       "       [  15, 1558,    7,    4,   12,   13],\n",
       "       [   0,    1,   11,    1,    0,    0],\n",
       "       [   0,    9,    1,    6,    0,    0],\n",
       "       [   0,    3,    0,    0,    0,    0],\n",
       "       [   0,    6,    0,    0,    0,    1]], dtype=int64)"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de confusão\n",
    "confusion_matrix(y_val,validacao_com_prev['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "cb7e3b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9562087582483503"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, validacao_com_prev['prediction'], average = 'weighted', zero_division=0)\n",
    "accuracy_score(y_val, validacao_com_prev['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b6317",
   "metadata": {},
   "source": [
    "Atingimos um F1 Score de 98,47%. Podemos então partir para previsão em novos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e1e29",
   "metadata": {},
   "source": [
    "# Previsões para o conjunto sem labels\n",
    "\n",
    "O objetivo final da modelagem era utilizar o modelo para prever o tipo de falha em 3333 registros que estão armazenados na variável _df\\_teste_. Para isso, contudo, precisamos aplicar todas as transformações que foram feitas na etapa de pré-processamento, como encoding, padronização e normalização. O LabelEncoding da variável target não é necessário visto que esse conjunto de dados não possui labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "4ab15d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste = pd.read_csv(\"desafio_manutencao_preditiva_teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "2a41e892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>1.435606</td>\n",
       "      <td>-1.338230</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-0.010416</td>\n",
       "      <td>0.732764</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.066971</td>\n",
       "      <td>-0.438033</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.295297</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>0.743541</td>\n",
       "      <td>-0.417789</td>\n",
       "      <td>0.276423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.064991</td>\n",
       "      <td>2.090145</td>\n",
       "      <td>0.068356</td>\n",
       "      <td>-0.918029</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>1.963354</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-0.702481</td>\n",
       "      <td>1.192985</td>\n",
       "      <td>0.369919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-0.319876</td>\n",
       "      <td>-0.157664</td>\n",
       "      <td>0.735772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.485591</td>\n",
       "      <td>-0.162333</td>\n",
       "      <td>-0.357760</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>0.056235</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>0.839192</td>\n",
       "      <td>-1.238182</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3333 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "0             -1.249456              -0.932625              1.435606   \n",
       "1              0.357548               0.343655             -0.010416   \n",
       "2             -1.400113              -1.066971             -0.438033   \n",
       "3             -0.295297              -0.596762              0.743541   \n",
       "4              2.064991               2.090145              0.068356   \n",
       "...                 ...                    ...                   ...   \n",
       "3328           1.261488               1.284073             -1.304521   \n",
       "3329           0.357548               0.679519             -0.702481   \n",
       "3330           0.407767               0.813864             -0.319876   \n",
       "3331           1.412145               1.485591             -0.162333   \n",
       "3332           0.056235              -0.529589              0.839192   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "0     -1.338230       0.284553  0.0  1.0  0.0  \n",
       "1      0.732764       0.780488  0.0  1.0  0.0  \n",
       "2      0.202509       0.166667  0.0  1.0  0.0  \n",
       "3     -0.417789       0.276423  0.0  1.0  0.0  \n",
       "4     -0.918029       0.036585  0.0  1.0  0.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "3328   1.963354       0.699187  0.0  1.0  0.0  \n",
       "3329   1.192985       0.369919  0.0  1.0  0.0  \n",
       "3330  -0.157664       0.735772  0.0  1.0  0.0  \n",
       "3331  -0.357760       0.211382  0.0  1.0  0.0  \n",
       "3332  -1.238182       0.386179  0.0  0.0  1.0  \n",
       "\n",
       "[3333 rows x 8 columns]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teste = df_teste.drop(columns = ['udi', 'product_id'])\n",
    "\n",
    "# One Hot Encoding\n",
    "cat_encoded_teste = onehot.transform(df_teste[[categorical]])\n",
    "df_teste[onehot.categories_[0]] = cat_encoded_teste.toarray()\n",
    "df_teste.drop(columns='type', inplace = True)\n",
    "\n",
    "# Scaling\n",
    "df_teste[scale] = scaler.transform(df_teste[scale])\n",
    "\n",
    "# Normalizing\n",
    "df_teste[normalize] = normalizer.transform(df_teste[normalize])\n",
    "\n",
    "df_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "ad18ea63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previsões do Modelo de classificação binária\n",
    "previsoes_finais_bin = modelo3.predict(df_teste)\n",
    "previsoes_finais_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "4481c126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>1.435606</td>\n",
       "      <td>-1.338230</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-0.010416</td>\n",
       "      <td>0.732764</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.066971</td>\n",
       "      <td>-0.438033</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.295297</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>0.743541</td>\n",
       "      <td>-0.417789</td>\n",
       "      <td>0.276423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.064991</td>\n",
       "      <td>2.090145</td>\n",
       "      <td>0.068356</td>\n",
       "      <td>-0.918029</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>1.963354</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-0.702481</td>\n",
       "      <td>1.192985</td>\n",
       "      <td>0.369919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-0.319876</td>\n",
       "      <td>-0.157664</td>\n",
       "      <td>0.735772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.485591</td>\n",
       "      <td>-0.162333</td>\n",
       "      <td>-0.357760</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>0.056235</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>0.839192</td>\n",
       "      <td>-1.238182</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3333 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "0             -1.249456              -0.932625              1.435606   \n",
       "1              0.357548               0.343655             -0.010416   \n",
       "2             -1.400113              -1.066971             -0.438033   \n",
       "3             -0.295297              -0.596762              0.743541   \n",
       "4              2.064991               2.090145              0.068356   \n",
       "...                 ...                    ...                   ...   \n",
       "3328           1.261488               1.284073             -1.304521   \n",
       "3329           0.357548               0.679519             -0.702481   \n",
       "3330           0.407767               0.813864             -0.319876   \n",
       "3331           1.412145               1.485591             -0.162333   \n",
       "3332           0.056235              -0.529589              0.839192   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  prediction  \n",
       "0     -1.338230       0.284553  0.0  1.0  0.0           1  \n",
       "1      0.732764       0.780488  0.0  1.0  0.0           1  \n",
       "2      0.202509       0.166667  0.0  1.0  0.0           1  \n",
       "3     -0.417789       0.276423  0.0  1.0  0.0           1  \n",
       "4     -0.918029       0.036585  0.0  1.0  0.0           1  \n",
       "...         ...            ...  ...  ...  ...         ...  \n",
       "3328   1.963354       0.699187  0.0  1.0  0.0           1  \n",
       "3329   1.192985       0.369919  0.0  1.0  0.0           1  \n",
       "3330  -0.157664       0.735772  0.0  1.0  0.0           1  \n",
       "3331  -0.357760       0.211382  0.0  1.0  0.0           1  \n",
       "3332  -1.238182       0.386179  0.0  0.0  1.0           1  \n",
       "\n",
       "[3333 rows x 9 columns]"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvando os resultados intermediários num novo DataFrame\n",
    "df_final = df_teste.copy()\n",
    "df_final['prediction'] = previsoes_finais_bin\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "84e0fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-0.948143</td>\n",
       "      <td>-1.604352</td>\n",
       "      <td>2.318973</td>\n",
       "      <td>-1.978538</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.960175</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.719360</td>\n",
       "      <td>0.892841</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>0.203393</td>\n",
       "      <td>-0.597875</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>1.351246</td>\n",
       "      <td>-1.484570</td>\n",
       "      <td>2.723719</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>1.491871</td>\n",
       "      <td>-1.468293</td>\n",
       "      <td>0.849593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>-0.847705</td>\n",
       "      <td>-0.462417</td>\n",
       "      <td>-1.130098</td>\n",
       "      <td>1.603182</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>-0.496173</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>0.902845</td>\n",
       "      <td>0.918699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>-1.550769</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-0.775626</td>\n",
       "      <td>1.703230</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-1.248255</td>\n",
       "      <td>2.113426</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-0.769999</td>\n",
       "      <td>1.293033</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "118           -0.948143              -1.604352              2.318973   \n",
       "139            0.960175              -0.260899             -0.719360   \n",
       "143            1.060613               0.679519              0.203393   \n",
       "160            0.407767               1.351246             -1.484570   \n",
       "175           -0.596610               0.410828              1.491871   \n",
       "...                 ...                    ...                   ...   \n",
       "3307          -0.847705              -0.462417             -1.130098   \n",
       "3308          -0.496173               0.276483             -0.972555   \n",
       "3309          -1.550769              -1.470007             -0.775626   \n",
       "3310          -1.500550              -1.470007             -1.248255   \n",
       "3311           1.713458               1.216900             -0.769999   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "118   -1.978538       0.821138  0.0  1.0  0.0  \n",
       "139    0.892841       0.906504  0.0  1.0  0.0  \n",
       "143   -0.597875       0.873984  0.0  0.0  1.0  \n",
       "160    2.723719       0.646341  0.0  0.0  1.0  \n",
       "175   -1.468293       0.849593  0.0  1.0  0.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "3307   1.603182       0.837398  0.0  1.0  0.0  \n",
       "3308   0.902845       0.918699  0.0  1.0  0.0  \n",
       "3309   1.703230       0.821138  0.0  1.0  0.0  \n",
       "3310   2.113426       0.813008  0.0  1.0  0.0  \n",
       "3311   1.293033       0.845528  0.0  1.0  0.0  \n",
       "\n",
       "[205 rows x 8 columns]"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conjunto de dados cujo modelo de classificação binária previu como falha\n",
    "df_teste2 = df_teste[previsoes_finais_bin == 0]\n",
    "df_teste2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "976cfddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Previsões do Modelo de classificação multiclasse\n",
    "previsoes_finais = grid_search.predict(df_teste2)\n",
    "previsoes_finais = np.where(previsoes_finais == 0, previsoes_finais, previsoes_finais+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "58b0f08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-0.948143</td>\n",
       "      <td>-1.604352</td>\n",
       "      <td>2.318973</td>\n",
       "      <td>-1.978538</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.960175</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.719360</td>\n",
       "      <td>0.892841</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>0.203393</td>\n",
       "      <td>-0.597875</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>1.351246</td>\n",
       "      <td>-1.484570</td>\n",
       "      <td>2.723719</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>1.491871</td>\n",
       "      <td>-1.468293</td>\n",
       "      <td>0.849593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3307</th>\n",
       "      <td>-0.847705</td>\n",
       "      <td>-0.462417</td>\n",
       "      <td>-1.130098</td>\n",
       "      <td>1.603182</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>-0.496173</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>0.902845</td>\n",
       "      <td>0.918699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>-1.550769</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-0.775626</td>\n",
       "      <td>1.703230</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-1.248255</td>\n",
       "      <td>2.113426</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-0.769999</td>\n",
       "      <td>1.293033</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>205 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "118           -0.948143              -1.604352              2.318973   \n",
       "139            0.960175              -0.260899             -0.719360   \n",
       "143            1.060613               0.679519              0.203393   \n",
       "160            0.407767               1.351246             -1.484570   \n",
       "175           -0.596610               0.410828              1.491871   \n",
       "...                 ...                    ...                   ...   \n",
       "3307          -0.847705              -0.462417             -1.130098   \n",
       "3308          -0.496173               0.276483             -0.972555   \n",
       "3309          -1.550769              -1.470007             -0.775626   \n",
       "3310          -1.500550              -1.470007             -1.248255   \n",
       "3311           1.713458               1.216900             -0.769999   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  prediction  \n",
       "118   -1.978538       0.821138  0.0  1.0  0.0           5  \n",
       "139    0.892841       0.906504  0.0  1.0  0.0           2  \n",
       "143   -0.597875       0.873984  0.0  0.0  1.0           5  \n",
       "160    2.723719       0.646341  0.0  0.0  1.0           3  \n",
       "175   -1.468293       0.849593  0.0  1.0  0.0           5  \n",
       "...         ...            ...  ...  ...  ...         ...  \n",
       "3307   1.603182       0.837398  0.0  1.0  0.0           2  \n",
       "3308   0.902845       0.918699  0.0  1.0  0.0           2  \n",
       "3309   1.703230       0.821138  0.0  1.0  0.0           2  \n",
       "3310   2.113426       0.813008  0.0  1.0  0.0           2  \n",
       "3311   1.293033       0.845528  0.0  1.0  0.0           2  \n",
       "\n",
       "[205 rows x 9 columns]"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_slice = df_teste2.copy()\n",
    "df_final_slice['prediction'] = previsoes_finais\n",
    "\n",
    "df_final_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "7bb54c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.loc[df_final_slice.index] = df_final_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "204932ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>1.435606</td>\n",
       "      <td>-1.338230</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-0.010416</td>\n",
       "      <td>0.732764</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.066971</td>\n",
       "      <td>-0.438033</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.295297</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>0.743541</td>\n",
       "      <td>-0.417789</td>\n",
       "      <td>0.276423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.064991</td>\n",
       "      <td>2.090145</td>\n",
       "      <td>0.068356</td>\n",
       "      <td>-0.918029</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>1.963354</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-0.702481</td>\n",
       "      <td>1.192985</td>\n",
       "      <td>0.369919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-0.319876</td>\n",
       "      <td>-0.157664</td>\n",
       "      <td>0.735772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.485591</td>\n",
       "      <td>-0.162333</td>\n",
       "      <td>-0.357760</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>0.056235</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>0.839192</td>\n",
       "      <td>-1.238182</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3333 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "0             -1.249456              -0.932625              1.435606   \n",
       "1              0.357548               0.343655             -0.010416   \n",
       "2             -1.400113              -1.066971             -0.438033   \n",
       "3             -0.295297              -0.596762              0.743541   \n",
       "4              2.064991               2.090145              0.068356   \n",
       "...                 ...                    ...                   ...   \n",
       "3328           1.261488               1.284073             -1.304521   \n",
       "3329           0.357548               0.679519             -0.702481   \n",
       "3330           0.407767               0.813864             -0.319876   \n",
       "3331           1.412145               1.485591             -0.162333   \n",
       "3332           0.056235              -0.529589              0.839192   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  prediction  \n",
       "0     -1.338230       0.284553  0.0  1.0  0.0           1  \n",
       "1      0.732764       0.780488  0.0  1.0  0.0           1  \n",
       "2      0.202509       0.166667  0.0  1.0  0.0           1  \n",
       "3     -0.417789       0.276423  0.0  1.0  0.0           1  \n",
       "4     -0.918029       0.036585  0.0  1.0  0.0           1  \n",
       "...         ...            ...  ...  ...  ...         ...  \n",
       "3328   1.963354       0.699187  0.0  1.0  0.0           1  \n",
       "3329   1.192985       0.369919  0.0  1.0  0.0           1  \n",
       "3330  -0.157664       0.735772  0.0  1.0  0.0           1  \n",
       "3331  -0.357760       0.211382  0.0  1.0  0.0           1  \n",
       "3332  -1.238182       0.386179  0.0  0.0  1.0           1  \n",
       "\n",
       "[3333 rows x 9 columns]"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "8fe9cacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['prediction'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0920cf",
   "metadata": {},
   "source": [
    "# Convertendo os resultados de volta para os labels\n",
    "\n",
    "Quando fizemos o LabelEncoding da variável target, os labels tornaram-se valores numéricos. Para leitura humana, o resultado final tem mais valor sendo transformado de volta para os labels originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "7533a414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       No Failure\n",
       "1       No Failure\n",
       "2       No Failure\n",
       "3       No Failure\n",
       "4       No Failure\n",
       "           ...    \n",
       "3328    No Failure\n",
       "3329    No Failure\n",
       "3330    No Failure\n",
       "3331    No Failure\n",
       "3332    No Failure\n",
       "Length: 3333, dtype: object"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes_finais_transformadas = pd.Series(le.inverse_transform(df_final['prediction']))\n",
    "previsoes_finais_transformadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "9d63f9e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Failure                  3128\n",
       "Tool Wear Failure             57\n",
       "Heat Dissipation Failure      53\n",
       "Overstrain Failure            40\n",
       "Power Failure                 30\n",
       "Random Failures               25\n",
       "dtype: int64"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantidade de previsões de cada tipo\n",
    "previsoes_finais_transformadas.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e35c8",
   "metadata": {},
   "source": [
    "# Salvando os resultados num arquivo CSV\n",
    "\n",
    "Para finalizar o trabalho, salvamos os resultados finais num arquivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "72c1904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_finais_transformadas = previsoes_finais_transformadas.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "88193801",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_finais_transformadas.to_csv(\"predicted.csv\", index = False, header=['rowNumber','predictedValues'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
