{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58cf901c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.utils import class_weight, resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ee2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = pd.read_csv(\"desafio_manutencao_preditiva_treino.csv\")\n",
    "df_teste = pd.read_csv(\"desafio_manutencao_preditiva_teste.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c93f9",
   "metadata": {},
   "source": [
    "# Definição dos tipos de variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9037b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['type', 'air_temperature_k', 'process_temperature_k',\n",
       "       'rotational_speed_rpm', 'torque_nm', 'tool_wear_min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = 'failure_type'\n",
    "features = df_treino.columns.drop([target, 'udi', 'product_id'])\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a964eb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['air_temperature_k', 'process_temperature_k', 'rotational_speed_rpm',\n",
       "       'torque_nm', 'tool_wear_min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical = df_treino[features].select_dtypes('number').columns\n",
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5802b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = 'type'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7530cf",
   "metadata": {},
   "source": [
    "# Separação do dataset em entrada e saída (X e y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ee4c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_treino[features]\n",
    "y = df_treino[target] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d619cb",
   "metadata": {},
   "source": [
    "# Divisão do dataset em treino e validação\n",
    "\n",
    "Como os dados de teste não possuem a variável target, é importante que separemos uma parte do dataset de treino para validar o modelo e garantir que não haja overfitting. Dividiremos então o dataset em treino e validação. Usaremos 25% dos dados para validação.\n",
    "\n",
    "Além disso, é importante dividir os dados antes do pré-processamento para evitar uma falha chamada _data leakage_, que é o compartilhamento de informações entre o conjunto de dados usado no treinamento do modelo e o dataset usado para avaliação do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1f02c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão em Dados de Treino e Validação.\n",
    "X_treino, X_val, y_treino, y_val = train_test_split(X, y, test_size = 0.25, random_state = 101, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fef7cc",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos Dados\n",
    "\n",
    "Muitos algoritmos preditivos esperam receber os dados padronizados e codificados. Portanto, é necessário transformar os dados para adequá-los aos modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93bac177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cópia dos dados = boa prática\n",
    "X_treino_copy = X_treino.copy()\n",
    "y_treino_copy = y_treino.copy()\n",
    "X_val_copy = X_val.copy()\n",
    "y_val_copy = y_val.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7b76e",
   "metadata": {},
   "source": [
    "## Encoding da variável categórica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b763c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número de classes da variável categórica\n",
    "X_treino[categorical].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dfd070",
   "metadata": {},
   "source": [
    "A variável type possui apenas 3 classes, vamos aplicar a técnica de OneHotEncoding para sua codificação;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b5a0cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>302.1</td>\n",
       "      <td>311.2</td>\n",
       "      <td>1598</td>\n",
       "      <td>37.9</td>\n",
       "      <td>148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>301.4</td>\n",
       "      <td>310.6</td>\n",
       "      <td>1630</td>\n",
       "      <td>30.3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>298.0</td>\n",
       "      <td>308.3</td>\n",
       "      <td>1473</td>\n",
       "      <td>42.6</td>\n",
       "      <td>107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>297.3</td>\n",
       "      <td>308.1</td>\n",
       "      <td>1709</td>\n",
       "      <td>30.6</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>298.0</td>\n",
       "      <td>308.7</td>\n",
       "      <td>1436</td>\n",
       "      <td>48.2</td>\n",
       "      <td>201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>298.8</td>\n",
       "      <td>310.0</td>\n",
       "      <td>1295</td>\n",
       "      <td>52.7</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>299.5</td>\n",
       "      <td>309.1</td>\n",
       "      <td>1420</td>\n",
       "      <td>49.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>297.1</td>\n",
       "      <td>308.0</td>\n",
       "      <td>1348</td>\n",
       "      <td>58.0</td>\n",
       "      <td>162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>298.7</td>\n",
       "      <td>309.6</td>\n",
       "      <td>1424</td>\n",
       "      <td>50.4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>297.2</td>\n",
       "      <td>308.6</td>\n",
       "      <td>1636</td>\n",
       "      <td>31.3</td>\n",
       "      <td>192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2685              302.1                  311.2                  1598   \n",
       "2281              301.4                  310.6                  1630   \n",
       "202               298.0                  308.3                  1473   \n",
       "6057              297.3                  308.1                  1709   \n",
       "998               298.0                  308.7                  1436   \n",
       "...                 ...                    ...                   ...   \n",
       "5535              298.8                  310.0                  1295   \n",
       "1450              299.5                  309.1                  1420   \n",
       "769               297.1                  308.0                  1348   \n",
       "6508              298.7                  309.6                  1424   \n",
       "830               297.2                  308.6                  1636   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2685       37.9            148  1.0  0.0  0.0  \n",
       "2281       30.3              2  0.0  0.0  1.0  \n",
       "202        42.6            107  0.0  1.0  0.0  \n",
       "6057       30.6            196  0.0  0.0  1.0  \n",
       "998        48.2            201  0.0  0.0  1.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "5535       52.7             74  0.0  0.0  1.0  \n",
       "1450       49.5              5  0.0  0.0  1.0  \n",
       "769        58.0            162  0.0  1.0  0.0  \n",
       "6508       50.4             32  0.0  0.0  1.0  \n",
       "830        31.3            192  0.0  1.0  0.0  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OneHotEncoding\n",
    "onehot = OneHotEncoder()\n",
    "cat_encoded = onehot.fit_transform(X_treino[[categorical]])\n",
    "X_treino[onehot.categories_[0]] = cat_encoded.toarray()\n",
    "X_treino.drop(columns='type', inplace = True)\n",
    "X_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669d6fc",
   "metadata": {},
   "source": [
    "## Padronização e normalização das variáveis numéricas\n",
    "\n",
    "Os dados podem ser padronizados ou normalizados. A padronização consiste em deixá-los numa distribuição normal com média 0 e desvio padrão 1. Já a normalização trata de deixar os dados numa escala única, geralmente de 0 a 1 ou de -1 a 1. \n",
    "\n",
    "Para os dados cuja distribuição não é gaussiana (ou normal), a normalização geralmente é uma melhor opção.\n",
    "\n",
    "Para os dados nesse problema, será aplicada a padronização para as variáveis numéricas cuja distribuição é normal e a normalização para a variável tool_wear_min que tem uma distribuição uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "065430da",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = [col for col in numerical if col != 'tool_wear_min']\n",
    "scaler = StandardScaler()\n",
    "X_treino[scale] = scaler.fit_transform(X_treino[scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4020852a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>0.338430</td>\n",
       "      <td>-0.217692</td>\n",
       "      <td>148</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0.709081</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>0.518479</td>\n",
       "      <td>-0.978057</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>-0.364888</td>\n",
       "      <td>0.252533</td>\n",
       "      <td>107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>0.962976</td>\n",
       "      <td>-0.948043</td>\n",
       "      <td>196</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>-0.573070</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-1.366413</td>\n",
       "      <td>1.263018</td>\n",
       "      <td>74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.245078</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>-0.663095</td>\n",
       "      <td>0.942865</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>-1.450332</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>-1.068206</td>\n",
       "      <td>1.793273</td>\n",
       "      <td>162</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>-0.646829</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.640589</td>\n",
       "      <td>1.032908</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>0.552238</td>\n",
       "      <td>-0.878009</td>\n",
       "      <td>192</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2685           1.060613               0.813864              0.338430   \n",
       "2281           0.709081               0.410828              0.518479   \n",
       "202           -0.998362              -1.134143             -0.364888   \n",
       "6057          -1.349894              -1.268489              0.962976   \n",
       "998           -0.998362              -0.865453             -0.573070   \n",
       "...                 ...                    ...                   ...   \n",
       "5535          -0.596610               0.007792             -1.366413   \n",
       "1450          -0.245078              -0.596762             -0.663095   \n",
       "769           -1.450332              -1.335661             -1.068206   \n",
       "6508          -0.646829              -0.260899             -0.640589   \n",
       "830           -1.400113              -0.932625              0.552238   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2685  -0.217692            148  1.0  0.0  0.0  \n",
       "2281  -0.978057              2  0.0  0.0  1.0  \n",
       "202    0.252533            107  0.0  1.0  0.0  \n",
       "6057  -0.948043            196  0.0  0.0  1.0  \n",
       "998    0.812802            201  0.0  0.0  1.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "5535   1.263018             74  0.0  0.0  1.0  \n",
       "1450   0.942865              5  0.0  0.0  1.0  \n",
       "769    1.793273            162  0.0  1.0  0.0  \n",
       "6508   1.032908             32  0.0  0.0  1.0  \n",
       "830   -0.878009            192  0.0  1.0  0.0  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5bf8374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>0.338430</td>\n",
       "      <td>-0.217692</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0.709081</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>0.518479</td>\n",
       "      <td>-0.978057</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>-0.364888</td>\n",
       "      <td>0.252533</td>\n",
       "      <td>0.434959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>0.962976</td>\n",
       "      <td>-0.948043</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>-0.573070</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-1.366413</td>\n",
       "      <td>1.263018</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.245078</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>-0.663095</td>\n",
       "      <td>0.942865</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>-1.450332</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>-1.068206</td>\n",
       "      <td>1.793273</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>-0.646829</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.640589</td>\n",
       "      <td>1.032908</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>0.552238</td>\n",
       "      <td>-0.878009</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2685           1.060613               0.813864              0.338430   \n",
       "2281           0.709081               0.410828              0.518479   \n",
       "202           -0.998362              -1.134143             -0.364888   \n",
       "6057          -1.349894              -1.268489              0.962976   \n",
       "998           -0.998362              -0.865453             -0.573070   \n",
       "...                 ...                    ...                   ...   \n",
       "5535          -0.596610               0.007792             -1.366413   \n",
       "1450          -0.245078              -0.596762             -0.663095   \n",
       "769           -1.450332              -1.335661             -1.068206   \n",
       "6508          -0.646829              -0.260899             -0.640589   \n",
       "830           -1.400113              -0.932625              0.552238   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2685  -0.217692       0.601626  1.0  0.0  0.0  \n",
       "2281  -0.978057       0.008130  0.0  0.0  1.0  \n",
       "202    0.252533       0.434959  0.0  1.0  0.0  \n",
       "6057  -0.948043       0.796748  0.0  0.0  1.0  \n",
       "998    0.812802       0.817073  0.0  0.0  1.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "5535   1.263018       0.300813  0.0  0.0  1.0  \n",
       "1450   0.942865       0.020325  0.0  0.0  1.0  \n",
       "769    1.793273       0.658537  0.0  1.0  0.0  \n",
       "6508   1.032908       0.130081  0.0  0.0  1.0  \n",
       "830   -0.878009       0.780488  0.0  1.0  0.0  \n",
       "\n",
       "[5000 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize = ['tool_wear_min']\n",
    "normalizer = MinMaxScaler()\n",
    "X_treino[normalize] = normalizer.fit_transform(X_treino[normalize])\n",
    "X_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd7f6a6",
   "metadata": {},
   "source": [
    "Na análise exploratória, vimos que as variáveis rotational_speed_rpm e torque_nm possuíam valores _outliers_, vamos verificar agora quantos valores desse existem no dataset de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a53bb455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.457986</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>5.948093</td>\n",
       "      <td>-2.638855</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>4.985954</td>\n",
       "      <td>-2.558816</td>\n",
       "      <td>0.048780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.201316</td>\n",
       "      <td>3.106689</td>\n",
       "      <td>-2.108600</td>\n",
       "      <td>0.581301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.558424</td>\n",
       "      <td>0.746691</td>\n",
       "      <td>3.101062</td>\n",
       "      <td>-2.028562</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>3.658090</td>\n",
       "      <td>-2.308696</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.402834</td>\n",
       "      <td>-1.191990</td>\n",
       "      <td>3.534108</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.106454</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-1.591474</td>\n",
       "      <td>3.173936</td>\n",
       "      <td>0.605691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.558424</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.270762</td>\n",
       "      <td>3.354022</td>\n",
       "      <td>0.199187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.257111</td>\n",
       "      <td>-0.126553</td>\n",
       "      <td>-1.203243</td>\n",
       "      <td>3.273984</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.552764</td>\n",
       "      <td>-1.552089</td>\n",
       "      <td>3.043873</td>\n",
       "      <td>0.951220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    air_temperature_k  process_temperature_k  rotational_speed_rpm  torque_nm  \\\n",
       "0            0.457986               0.007792              5.948093  -2.638855   \n",
       "1            0.206892               0.343655              4.985954  -2.558816   \n",
       "2           -0.998362              -1.201316              3.106689  -2.108600   \n",
       "3            0.558424               0.746691              3.101062  -2.028562   \n",
       "4           -1.349894              -1.134143              3.658090  -2.308696   \n",
       "..                ...                    ...                   ...        ...   \n",
       "86          -1.400113              -1.402834             -1.191990   3.534108   \n",
       "87           0.106454               0.679519             -1.591474   3.173936   \n",
       "88           0.558424               0.276483             -1.270762   3.354022   \n",
       "89           0.257111              -0.126553             -1.203243   3.273984   \n",
       "90           1.412145               1.552764             -1.552089   3.043873   \n",
       "\n",
       "    tool_wear_min    H    L    M  \n",
       "0        0.841463  0.0  1.0  0.0  \n",
       "1        0.048780  0.0  1.0  0.0  \n",
       "2        0.581301  0.0  0.0  1.0  \n",
       "3        0.658537  0.0  0.0  1.0  \n",
       "4        0.130081  0.0  1.0  0.0  \n",
       "..            ...  ...  ...  ...  \n",
       "86       0.699187  0.0  1.0  0.0  \n",
       "87       0.605691  0.0  1.0  0.0  \n",
       "88       0.199187  0.0  1.0  0.0  \n",
       "89       0.646341  1.0  0.0  0.0  \n",
       "90       0.951220  0.0  0.0  1.0  \n",
       "\n",
       "[91 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# União de todas as linhas onde uma das duas variáveis possui um outlier\n",
    "outliers = pd.merge(X_treino.loc[abs(X_treino['rotational_speed_rpm']) > 3],\n",
    "                    X_treino.loc[abs(X_treino['torque_nm']) > 3],\n",
    "                    how='outer')\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f52e192f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0182"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proporção de outliers\n",
    "len(outliers)/len(X_treino)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701175e",
   "metadata": {},
   "source": [
    "Como a proporção de outliers é de menos de 2%, não há grande prejuízo em apenas remover essas linhas para que elas não comprometam o treinamento do modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a2a97f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2685</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>0.338430</td>\n",
       "      <td>-0.217692</td>\n",
       "      <td>0.601626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2281</th>\n",
       "      <td>0.709081</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>0.518479</td>\n",
       "      <td>-0.978057</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>-0.364888</td>\n",
       "      <td>0.252533</td>\n",
       "      <td>0.434959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>0.962976</td>\n",
       "      <td>-0.948043</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>-0.573070</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5535</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-1.366413</td>\n",
       "      <td>1.263018</td>\n",
       "      <td>0.300813</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>-0.245078</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>-0.663095</td>\n",
       "      <td>0.942865</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>-1.450332</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>-1.068206</td>\n",
       "      <td>1.793273</td>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6508</th>\n",
       "      <td>-0.646829</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.640589</td>\n",
       "      <td>1.032908</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>0.552238</td>\n",
       "      <td>-0.878009</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4909 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2685           1.060613               0.813864              0.338430   \n",
       "2281           0.709081               0.410828              0.518479   \n",
       "202           -0.998362              -1.134143             -0.364888   \n",
       "6057          -1.349894              -1.268489              0.962976   \n",
       "998           -0.998362              -0.865453             -0.573070   \n",
       "...                 ...                    ...                   ...   \n",
       "5535          -0.596610               0.007792             -1.366413   \n",
       "1450          -0.245078              -0.596762             -0.663095   \n",
       "769           -1.450332              -1.335661             -1.068206   \n",
       "6508          -0.646829              -0.260899             -0.640589   \n",
       "830           -1.400113              -0.932625              0.552238   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2685  -0.217692       0.601626  1.0  0.0  0.0  \n",
       "2281  -0.978057       0.008130  0.0  0.0  1.0  \n",
       "202    0.252533       0.434959  0.0  1.0  0.0  \n",
       "6057  -0.948043       0.796748  0.0  0.0  1.0  \n",
       "998    0.812802       0.817073  0.0  0.0  1.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "5535   1.263018       0.300813  0.0  0.0  1.0  \n",
       "1450   0.942865       0.020325  0.0  0.0  1.0  \n",
       "769    1.793273       0.658537  0.0  1.0  0.0  \n",
       "6508   1.032908       0.130081  0.0  0.0  1.0  \n",
       "830   -0.878009       0.780488  0.0  1.0  0.0  \n",
       "\n",
       "[4909 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_treino = X_treino.loc[abs(X_treino['rotational_speed_rpm']) < 3]\n",
    "X_treino = X_treino.loc[abs(X_treino['torque_nm']) < 3]\n",
    "X_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177add46",
   "metadata": {},
   "source": [
    "Se removemos essas linhas do dataset de entrada X, também as removeremos da saída y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53b1826e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2685    No Failure\n",
       "2281    No Failure\n",
       "202     No Failure\n",
       "6057    No Failure\n",
       "998     No Failure\n",
       "           ...    \n",
       "5535    No Failure\n",
       "1450    No Failure\n",
       "769     No Failure\n",
       "6508    No Failure\n",
       "830     No Failure\n",
       "Name: failure_type, Length: 4909, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removendo as mesmas linhas do dataset y\n",
    "y_treino = y_treino.loc[y_treino_copy.index.isin(X_treino.index)]\n",
    "y_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea2c0a",
   "metadata": {},
   "source": [
    "## Encoding da variável target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7092f413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Número de classes da variável target\n",
    "y_treino.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd9ea6",
   "metadata": {},
   "source": [
    "A variável target failure_type possui 6 classes distintas. Vamos aplicar a técnica de label encoding para sua codificação, a fim de que haja apenas uma coluna para a variável a ser prevista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47512bc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2685    1\n",
       "2281    1\n",
       "202     1\n",
       "6057    1\n",
       "998     1\n",
       "       ..\n",
       "5535    1\n",
       "1450    1\n",
       "769     1\n",
       "6508    1\n",
       "830     1\n",
       "Length: 4909, dtype: int32"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "y_treino = pd.Series(le.fit_transform(y_treino), index = y_treino.index)\n",
    "y_treino"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29995548",
   "metadata": {},
   "source": [
    "## Pré-processamento do dataset de validação\n",
    "\n",
    "Agora, com as transformações treinadas, é preciso transformar todos os dados de validação da mesma forma que os dados de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5735ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variável target\n",
    "y_val = pd.Series(le.transform(y_val), index = y_val.index)\n",
    "\n",
    "# One Hot Encoding\n",
    "cat_encoded_val = onehot.transform(X_val[[categorical]])\n",
    "X_val[onehot.categories_[0]] = cat_encoded_val.toarray()\n",
    "X_val.drop(columns='type', inplace = True)\n",
    "\n",
    "# Scaling\n",
    "X_val[scale] = scaler.transform(X_val[scale])\n",
    "\n",
    "# Normalizing\n",
    "X_val[normalize] = normalizer.transform(X_val[normalize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92beb2a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5089    1\n",
       "2028    1\n",
       "2438    1\n",
       "4076    1\n",
       "2330    4\n",
       "       ..\n",
       "6252    1\n",
       "6532    1\n",
       "6002    1\n",
       "664     1\n",
       "454     1\n",
       "Length: 1667, dtype: int32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42b32358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>-1.078106</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>0.307330</td>\n",
       "      <td>-0.059381</td>\n",
       "      <td>-0.499925</td>\n",
       "      <td>0.232524</td>\n",
       "      <td>0.686992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>-0.691228</td>\n",
       "      <td>0.402605</td>\n",
       "      <td>0.174797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>0.457986</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>0.142480</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>-0.777961</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>2.785976</td>\n",
       "      <td>-2.338711</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532</th>\n",
       "      <td>-0.747267</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>1.030495</td>\n",
       "      <td>-1.068101</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>0.940470</td>\n",
       "      <td>-1.408264</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>-1.902302</td>\n",
       "      <td>-2.141734</td>\n",
       "      <td>0.659143</td>\n",
       "      <td>-0.898019</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>-1.098799</td>\n",
       "      <td>-0.663935</td>\n",
       "      <td>-0.460539</td>\n",
       "      <td>0.112466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "5089           0.206892               0.948209              0.698529   \n",
       "2028           0.307330              -0.059381             -0.499925   \n",
       "2438           1.060613               0.881037             -0.691228   \n",
       "4076           0.457986               0.612346             -0.342382   \n",
       "2330           0.809518               0.343655              0.360936   \n",
       "...                 ...                    ...                   ...   \n",
       "6252          -1.249456              -1.335661              2.785976   \n",
       "6532          -0.747267              -0.529589              1.030495   \n",
       "6002          -1.400113              -1.335661              0.940470   \n",
       "664           -1.902302              -2.141734              0.659143   \n",
       "454           -1.098799              -0.663935             -0.460539   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "5089  -1.078106       0.524390  0.0  1.0  0.0  \n",
       "2028   0.232524       0.686992  0.0  0.0  1.0  \n",
       "2438   0.402605       0.174797  0.0  1.0  0.0  \n",
       "4076   0.142480       0.406504  0.0  1.0  0.0  \n",
       "2330  -0.777961       0.008130  1.0  0.0  0.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "6252  -2.338711       0.520325  0.0  0.0  1.0  \n",
       "6532  -1.068101       0.691057  0.0  0.0  1.0  \n",
       "6002  -1.408264       0.813008  0.0  1.0  0.0  \n",
       "664   -0.898019       0.552846  0.0  1.0  0.0  \n",
       "454    0.112466       0.000000  0.0  1.0  0.0  \n",
       "\n",
       "[1667 rows x 8 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc8a0b5",
   "metadata": {},
   "source": [
    "## Balanceamento das classes da variável target\n",
    "\n",
    "A última etapa do pré-processamento é balancear as instâncias da variável target. Como visto na Análise Exploratória, há muito mais máquinas sem falhas do que com os 5 tipos de falhas apresentados. Se o modelo preditivo for alimentado dessa forma, ele aprenderá muito melhor sobre máquinas sem falhas do que sobre máquinas com falhas, portanto, é preciso aplicar alguma técnica de balanceamento de classes.\n",
    "\n",
    "Pode-se aplicar o _oversampling_, que cria mais registros das classes que minoritárias ou o _undersampling_, que remove registros das classes majoritárias. \n",
    "\n",
    "O dataset possui poucos registros - apenas cerca de 6,5 mil - portanto, se aplicado o _undersampling_, teremos ainda menos dados para o treinamento e perderemos características demais sobre os dados.\n",
    "\n",
    "O _oversampling_ também não funcionaria muito bem para esse modelo, visto que a diferença entre as classes é muito grande e são muitas classes, o que faria com que fossem seriam criados registros artificias demais para resolver a diferença.\n",
    "\n",
    "A abordagem que faremos será a de converter o problema de classificação multiclasse em um problema de classificação binário, respondendo a pergunta: \"Há ou não falha nas máquinas?\". Com essa mudança na pergunta-chave do problema, haverão apenas duas classes a serem balanceadas, e poderemos então aplicar uma técnica de _oversampling_, nesse caso o SMOTE. Após respondido esse primeiro questionamento, abordaremos então, para as máquinas em que o modelo previu falha, qual o tipo de falha presente, se houver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdc8f368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x=y_treino)\n",
    "plt.title('Tipos de falha por máquina');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0bf7126",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkQAAAGzCAYAAADOnwhmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAtj0lEQVR4nO3dfVhUdf7/8deIMqA4o6KCJN6UrUaaN1g6bd5GUuK2fVNbyorUbC2yRTLJa1sst3LTtbTSbLcSt/KbaWkpq8Z6g6VsFkappVffjZLNAEtg0uRGOL8/dpmf46AiAoN9no/rmutyPud9Pud9xjFenTtslmVZAgAAMFgzfzcAAADgbwQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCLgArFt2zbZbDZt27bNMzZ8+HD17t27UbZfVVWl3r1764knnjjndSsqKhQZGaklS5Y0QGeNLz4+Xq1bt9aMGTNUVFSkNm3aqLi42N9t+fj6669ls9mUlpbmGbvrrrsUEhLSaD2MHj1aU6ZMqdO68fHxuuWWW+q5I6BmBCKggaWlpclms9X4evjhh/3dXq397//+r/Ly8nT//fd7jZeVlSklJUUREREKDg7WoEGDlJGR4VXTokULJScn64knnlBpaWljtn1G1SHTZrMpOzvbZ3lN4eHzzz/Xtm3b9Nhjj+ndd99VaGioYmJi1KZNmwbp69RXfHx8vW2noe3YsUPvvfeeUlJSvMafeOIJ3XjjjQoLC5PNZtOjjz5a4/opKSl666239OmnnzZCtzBdc383AJhizpw56t69u9dYYx3dqQ/z589XfHy8nE6n1/hdd92l1atXKykpSZdeeqnS0tI0evRobd26Vddcc42nbuLEiXr44Ye1YsUKTZo0qbHbP6tHH31U69atO2vdxRdfrOzsbF100UVKSkpSfn6+OnXq1CA9PfDAA7ryyiu9xrp169Yg22oI8+fP17XXXqsePXp4jT/yyCMKDw9X//79tWnTptOu379/fw0cOFALFizQ3/72t4ZuF4YjEAGN5IYbbtDAgQP93UadfPLJJ/r000+1YMECr/Fdu3bpjTfe0Pz58zVjxgxJ0p133qnevXtr5syZ2rlzp6e2TZs2GjVqlNLS0ppcIOrXr5/Wr1+v3bt3a8CAAWesDQoK0kUXXSRJatasmSIiIhqsryFDhmjcuHENNn9DKiwsVHp6upYuXeqzLDc3V926ddP333+vDh06nHGeW265RbNnz9aSJUsa9VQfzMMpM8DPvvnmG913333q2bOngoODFRoaqvHjx+vrr7+u9Ryff/65RowYoZYtW+qiiy7SvHnzvJaXl5crNTVV0dHRcjqdatWqlYYMGaKtW7fWav61a9cqMDBQQ4cO9RpfvXq1AgICdM8993jGgoKCNHnyZGVlZSkvL8+r/rrrrtMHH3ygI0eOnHWbhYWFmjx5ssLCwhQUFKS+fftq+fLlXjU1XVcl1XztzJlMmzZNbdu2Pe2pm5OtWbNGo0ePVkREhOx2uy655BL98Y9/VGVlpU/tqlWrFB0dreDgYLVv31633367vv3221r1dCZHjhzRjBkz1KdPH4WEhMjhcOiGG244p1NL3377rW666SaFhISoQ4cOmjFjhs8+/PnPf9bVV1+t0NBQBQcHKzo6WqtXr67V/Onp6Tpx4oRiYmJ8lp3LUa7rrrtOx44d8zkNC9Q3AhHQSEpKSvT99997vSTpo48+0s6dOxUfH69nn31WU6dO1ebNmzV8+HD99NNPZ523qKhI119/vfr27asFCxaoV69eSklJ0YYNGzw1brdbL730koYPH66nnnpKjz76qA4fPqzY2Fjl5OScdRs7d+5U79691aJFC6/xTz75RL/4xS/kcDi8xq+66ipJ8pk7OjpalmV5HTmqyfHjxzV8+HC9+uqrmjBhgubPny+n06m77rpLixYtOmu/58rhcGj69Olat26ddu/efcbaV155Ra1bt1ZycrIWLlyo6Ohopaam+lwPlpaWpltuuUUBAQGaO3eupkyZorffflvXXHNNrS/A/vHHH32+M1VVVfrqq6+0du1ajRkzRk8//bQeeugh7dmzR8OGDdOhQ4fOOm9lZaViY2MVGhqqP//5zxo2bJgWLFigv/zlL151ixYtUv/+/TVnzhw9+eSTat68ucaPH6/09PSzbmPnzp0KDQ1V165da7WvpxMVFaXg4GDt2LHjvOYBzsoC0KCWLVtmSarxZVmW9dNPP/msk5WVZUmy/va3v3nGtm7dakmytm7d6hkbNmyYT11ZWZkVHh5ujR071jN24sQJq6yszGsbRUVFVlhYmDVp0qSz7kPnzp295qt2+eWXWyNHjvQZ37dvnyXJWrp0qdf4oUOHLEnWU089dcbtLVy40JJkvfbaa56x8vJyy+VyWSEhIZbb7bYsq+bPxLIsKzc315JkLVu27IzbqV5/1apVVnFxsdW2bVvrxhtv9CxPSEiwWrVq5bXOsWPHfOb57W9/a7Vs2dIqLS319NqxY0erd+/e1vHjxz1169evtyRZqampteqrpldubq5VWlpqVVZW+uyz3W635syZc8bPISEhwZLkVWdZltW/f38rOjraa+zU72Z5ebnVu3fvGv/OT3XNNdf4zHeqw4cPW5Ks2bNnn7HuF7/4hXXDDTecdZvA+eAIEdBIFi9erIyMDK+XJAUHB3tqKioq9MMPP6hHjx5q06bNWY9WSFJISIhuv/12z/vAwEBdddVV+uqrrzxjAQEBCgwMlPSf2+ePHDmiEydOaODAgbXaxg8//KC2bdv6jB8/flx2u91nPCgoyLP8ZNVzVB8dO52///3vCg8P16233uoZa9GihR544AEdPXpUmZmZZ+35XDmdTiUlJendd9/VJ598ctq6li1bev5cfQRnyJAh+umnn7R//35J0scff6zCwkLdd999ns9CkuLi4tSrV69aHWGRpNTUVJ/vTHh4uOx2u5o1+89/visrK/XDDz8oJCREPXv2rNXfpyRNnTrV6/2QIUO8vjOS93ezqKhIJSUlGjJkyHl9Z+qibdu2Z/3OAOeLi6qBRnLVVVfVeFH18ePHNXfuXC1btkzffvutLMvyLCspKTnrvJ07d5bNZvMaa9u2rT777DOvseXLl2vBggXav3+/KioqPOOn3vl2Oif3VS04OFhlZWU+49W31p/8A/XkOU7t91TffPONLr30Us8P/WqXXXaZZ3lD+N3vfqdnnnlGjz76qN55550aa/bt26dHHnlEW7Zskdvt9lpW/fdV3V/Pnj191u/Vq5c++OCDWvXTp0+fGq/Bqaqq0qJFi7RkyRLl5uZ6XfsTGhp61nmDgoJ8LmZu27atioqKvMbWr1+vxx9/XDk5OV5/z2f7+6tW03emLizLqvU2gbriCBHgZ9OmTdMTTzyhW265RW+++abee+89ZWRkKDQ0VFVVVWddPyAgoMbxk38Yvfbaa7rrrrt0ySWX6OWXX9bGjRuVkZGhkSNH1moboaGhPj8sJalTp0767rvvfMarx069A6t6jvbt2591m7Vxuh+SNV3gXBtnO0pUXFysYcOG6dNPP9WcOXO0bt06ZWRk6KmnnpKkWn2W9eHJJ59UcnKyhg4dqtdee02bNm1SRkaGLr/88vP6zpzs/fff14033qigoCAtWbJEf//735WRkaHbbrutVkHndN+ZuigqKqq37wxwOhwhAvxs9erVSkhI8LqlvbS0tF6ffLx69WpdfPHFevvtt71CxOzZs2u1fq9evZSbm+sz3q9fP23dulVut9vrwuoPP/zQs/xk1XNUH+k5na5du+qzzz5TVVWV11Gi6lNS1RfqVp+SOfWzOp8jSElJSVq4cKEee+wxn4ctbtu2TT/88IPefvttrzvuTv1sqvs7cOCARo4c6bXswIED532h8erVqzVixAi9/PLLXuPFxcX1FhzeeustBQUFadOmTV6nRZctW1ar9Xv16qW33nrrvPs4ceKE8vLydOONN573XMCZcIQI8LOAgACf/+N+7rnn6nyU43TbkLyPGn344YfKysqq1foul0t79+71OT02btw4VVZWet2dVFZWpmXLlmnQoEGKjIz0qs/OzpbNZpPL5Trj9kaPHq38/HytXLnSM3bixAk999xzCgkJ0bBhwyT9J3gEBARo+/btXuufz68IqT5K9M477/jcJVfT51heXu6zvYEDB6pjx45aunSp12e2YcMGffHFF4qLi6tzf9V9nPqdWbVqVb3c0n/yNmw2m9f38Ouvv9batWtrtb7L5VJRUZHPdUnn6vPPP1dpaamuvvrq85oHOBuOEAF+NmbMGL366qtyOp2KiopSVlaW/vGPf9TqWpBz2cbbb7+t//mf/1FcXJxyc3O1dOlSRUVF6ejRo2dd/9e//rX++Mc/KjMzU6NGjfKMDxo0SOPHj9esWbNUWFioHj16aPny5fr66699jl5IUkZGhn75y1+edd/uuecevfjii7rrrruUnZ2tbt26afXq1dqxY4cWLlyo1q1bS/pPeBk/fryee+452Ww2XXLJJVq/fr0KCwvP8RPyVn0t0aeffqpWrVp5xq+++mq1bdtWCQkJeuCBB2Sz2fTqq6/6hJMWLVroqaee0sSJEzVs2DDdeuutKigo0KJFi9StWzdNnz79vPobM2aM5syZo4kTJ+rqq6/Wnj179Prrr+viiy8+r3lPFhcXp6efflrXX3+9brvtNhUWFmrx4sXq0aOHz/Vpp1u/efPm+sc//uH1nCpJevXVV/XNN994Hiuxfft2Pf7445KkO+64w+sIWkZGhlq2bKnrrruu3vYNqJF/bm4DzFF92/1HH31U4/KioiJr4sSJVvv27a2QkBArNjbW2r9/v9W1a1crISHBU3e62+4vv/xynzkTEhKsrl27et5XVVVZTz75pNW1a1fLbrdb/fv3t9avX+9TdyZXXHGFNXnyZJ/x48ePWzNmzLDCw8Mtu91uXXnlldbGjRt96oqLi63AwEDrpZdeqtX2CgoKPJ9LYGCg1adPnxpvoz98+LA1duxYq2XLllbbtm2t3/72t9bevXvP+bb7U82ePduS5HPb/Y4dO6zBgwdbwcHBVkREhDVz5kxr06ZNNd7+v3LlSqt///6W3W632rVrZ02YMMH697//fdZ9P1NflmVZpaWl1oMPPmh16tTJCg4Otn75y19aWVlZ1rBhw6xhw4Z56k532/2p+3Ty/p7s5Zdfti699FLLbrdbvXr1spYtW1Zj3enceOON1rXXXuszXv24iJpep36GgwYNsm6//fZabQ84HzbLqqfbAAD8rL366qtKTEzUwYMH6/SLTBcuXKh58+bpX//6l8/dZ/h5ev/99zV8+HDt379fl1566Tmvn5OTowEDBmj37t0+16MB9Y1ABKBWqqqqdMUVV+jWW2/V73//+3Nat6KiQpdccokefvhh3XfffQ3UIZqiG264QZ07d9Zf//rXc143Pj5eVVVVevPNNxugM8AbgQgAABiPu8wAAIDxCEQAAMB4BCIAAGA8AhEAADAeD2ashaqqKh06dEitW7fmFwwCAHCBsCxLP/74oyIiInx+WfSpCES1cOjQIZ9fQQAAAC4MeXl56ty58xlrCES1UP1rAvLy8rx+gSUAAGi63G63IiMjPT/Hz4RAVAvVp8kcDgeBCACAC0xtLnfhomoAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4zX3dwMAYIKDc/r4uwWgSeqSusffLUjiCBEAAACBCAAAgEAEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGazKB6E9/+pNsNpuSkpI8Y6WlpUpMTFRoaKhCQkI0duxYFRQUeK138OBBxcXFqWXLlurYsaMeeughnThxwqtm27ZtGjBggOx2u3r06KG0tLRG2CMAAHChaBKB6KOPPtKLL76oK664wmt8+vTpWrdunVatWqXMzEwdOnRIN998s2d5ZWWl4uLiVF5erp07d2r58uVKS0tTamqqpyY3N1dxcXEaMWKEcnJylJSUpLvvvlubNm1qtP0DAABNm82yLMufDRw9elQDBgzQkiVL9Pjjj6tfv35auHChSkpK1KFDB61YsULjxo2TJO3fv1+XXXaZsrKyNHjwYG3YsEFjxozRoUOHFBYWJklaunSpUlJSdPjwYQUGBiolJUXp6enau3evZ5vx8fEqLi7Wxo0ba+yprKxMZWVlnvdut1uRkZEqKSmRw+FowE8DwM/VwTl9/N0C0CR1Sd3TYHO73W45nc5a/fz2+xGixMRExcXFKSYmxms8OztbFRUVXuO9evVSly5dlJWVJUnKyspSnz59PGFIkmJjY+V2u7Vv3z5Pzalzx8bGeuaoydy5c+V0Oj2vyMjI895PAADQdPk1EL3xxhvavXu35s6d67MsPz9fgYGBatOmjdd4WFiY8vPzPTUnh6Hq5dXLzlTjdrt1/PjxGvuaNWuWSkpKPK+8vLw67R8AALgwNPfXhvPy8vS73/1OGRkZCgoK8lcbNbLb7bLb7f5uAwAANBK/HSHKzs5WYWGhBgwYoObNm6t58+bKzMzUs88+q+bNmyssLEzl5eUqLi72Wq+goEDh4eGSpPDwcJ+7zqrfn63G4XAoODi4gfYOAABcSPwWiK699lrt2bNHOTk5ntfAgQM1YcIEz59btGihzZs3e9Y5cOCADh48KJfLJUlyuVzas2ePCgsLPTUZGRlyOByKiory1Jw8R3VN9RwAAAB+O2XWunVr9e7d22usVatWCg0N9YxPnjxZycnJateunRwOh6ZNmyaXy6XBgwdLkkaNGqWoqCjdcccdmjdvnvLz8/XII48oMTHRc8pr6tSpev755zVz5kxNmjRJW7Zs0Ztvvqn09PTG3WEAANBk+S0Q1cYzzzyjZs2aaezYsSorK1NsbKyWLFniWR4QEKD169fr3nvvlcvlUqtWrZSQkKA5c+Z4arp376709HRNnz5dixYtUufOnfXSSy8pNjbWH7sEAACaIL8/h+hCcC7PMQCAmvAcIqBmPIcIAACgiSAQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABjPr4HohRde0BVXXCGHwyGHwyGXy6UNGzZ4lpeWlioxMVGhoaEKCQnR2LFjVVBQ4DXHwYMHFRcXp5YtW6pjx4566KGHdOLECa+abdu2acCAAbLb7erRo4fS0tIaY/cAAMAFwq+BqHPnzvrTn/6k7Oxsffzxxxo5cqR+/etfa9++fZKk6dOna926dVq1apUyMzN16NAh3XzzzZ71KysrFRcXp/Lycu3cuVPLly9XWlqaUlNTPTW5ubmKi4vTiBEjlJOTo6SkJN19993atGlTo+8vAABommyWZVn+buJk7dq10/z58zVu3Dh16NBBK1as0Lhx4yRJ+/fv12WXXaasrCwNHjxYGzZs0JgxY3To0CGFhYVJkpYuXaqUlBQdPnxYgYGBSklJUXp6uvbu3evZRnx8vIqLi7Vx48YaeygrK1NZWZnnvdvtVmRkpEpKSuRwOBpw7wH8XB2c08ffLQBNUpfUPQ02t9vtltPprNXP7yZzDVFlZaXeeOMNHTt2TC6XS9nZ2aqoqFBMTIynplevXurSpYuysrIkSVlZWerTp48nDElSbGys3G635yhTVlaW1xzVNdVz1GTu3LlyOp2eV2RkZH3uKgAAaGL8Hoj27NmjkJAQ2e12TZ06VWvWrFFUVJTy8/MVGBioNm3aeNWHhYUpPz9fkpSfn+8VhqqXVy87U43b7dbx48dr7GnWrFkqKSnxvPLy8upjVwEAQBPV3N8N9OzZUzk5OSopKdHq1auVkJCgzMxMv/Zkt9tlt9v92gMAAGg8fg9EgYGB6tGjhyQpOjpaH330kRYtWqTf/OY3Ki8vV3FxsddRooKCAoWHh0uSwsPDtWvXLq/5qu9CO7nm1DvTCgoK5HA4FBwc3FC7BQAALiB+P2V2qqqqKpWVlSk6OlotWrTQ5s2bPcsOHDiggwcPyuVySZJcLpf27NmjwsJCT01GRoYcDoeioqI8NSfPUV1TPQcAAIBfjxDNmjVLN9xwg7p06aIff/xRK1as0LZt27Rp0yY5nU5NnjxZycnJateunRwOh6ZNmyaXy6XBgwdLkkaNGqWoqCjdcccdmjdvnvLz8/XII48oMTHRc8pr6tSpev755zVz5kxNmjRJW7Zs0Ztvvqn09HR/7joAAGhC/BqICgsLdeedd+q7776T0+nUFVdcoU2bNum6666TJD3zzDNq1qyZxo4dq7KyMsXGxmrJkiWe9QMCArR+/Xrde++9crlcatWqlRISEjRnzhxPTffu3ZWenq7p06dr0aJF6ty5s1566SXFxsY2+v4CAICmqck9h6gpOpfnGABATXgOEVAznkMEAADQRBCIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADBenQLRyJEjVVxc7DPudrs1cuTI8+0JAACgUdUpEG3btk3l5eU+46WlpXr//ffPuykAAIDG1Pxcij/77DPPnz///HPl5+d73ldWVmrjxo266KKL6q87AACARnBOgahfv36y2Wyy2Ww1nhoLDg7Wc889V2/NAQAANIZzCkS5ubmyLEsXX3yxdu3apQ4dOniWBQYGqmPHjgoICKj3JgEAABrSOQWirl27SpKqqqoapBkAAAB/OKdAdLIvv/xSW7duVWFhoU9ASk1NPe/GAAAAGkudAtFf//pX3XvvvWrfvr3Cw8Nls9k8y2w2G4EIAABcUOoUiB5//HE98cQTSklJqe9+AAAAGl2dnkNUVFSk8ePH13cvAAAAflGnQDR+/Hi999579d0LAACAX9TplFmPHj30hz/8Qf/85z/Vp08ftWjRwmv5Aw88UC/NAQAANAabZVnWua7UvXv3009os+mrr746r6aaGrfbLafTqZKSEjkcDn+3A+ACdHBOH3+3ADRJXVL3NNjc5/Lzu05HiHJzc+vUGAAAQFNUp2uIAAAAfk7qdIRo0qRJZ1z+yiuv1KkZAAAAf6hTICoqKvJ6X1FRob1796q4uLjGX/oKAADQlNUpEK1Zs8ZnrKqqSvfee68uueSS824KAACgMdXbNUTNmjVTcnKynnnmmfqaEgAAoFHU60XV//rXv3TixIn6nBIAAKDB1emUWXJystd7y7L03XffKT09XQkJCfXSGAAAQGOpUyD65JNPvN43a9ZMHTp00IIFC856BxoAAEBTU6dAtHXr1vruAwAAwG/qFIiqHT58WAcOHJAk9ezZUx06dKiXpgAAABpTnS6qPnbsmCZNmqROnTpp6NChGjp0qCIiIjR58mT99NNP9d0jAABAg6pTIEpOTlZmZqbWrVun4uJiFRcX65133lFmZqYefPDB+u4RAACgQdXplNlbb72l1atXa/jw4Z6x0aNHKzg4WLfccoteeOGF+uoPAACgwdXpCNFPP/2ksLAwn/GOHTtyygwAAFxw6hSIXC6XZs+erdLSUs/Y8ePH9dhjj8nlctVbcwAAAI2hTqfMFi5cqOuvv16dO3dW3759JUmffvqp7Ha73nvvvXptEAAAoKHVKRD16dNHX375pV5//XXt379fknTrrbdqwoQJCg4OrtcGAQAAGlqdAtHcuXMVFhamKVOmeI2/8sorOnz4sFJSUuqlOQAAgMZQp2uIXnzxRfXq1ctn/PLLL9fSpUvPuykAAIDGVKdAlJ+fr06dOvmMd+jQQd999915NwUAANCY6hSIIiMjtWPHDp/xHTt2KCIi4rybAgAAaEx1uoZoypQpSkpKUkVFhUaOHClJ2rx5s2bOnMmTqgEAwAWnToHooYce0g8//KD77rtP5eXlkqSgoCClpKRo1qxZ9dogAABAQ6tTILLZbHrqqaf0hz/8QV988YWCg4N16aWXym6313d/AAAADa5OgahaSEiIrrzyyvrqBQAAwC/qdFE1AADAzwmBCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACM59dANHfuXF155ZVq3bq1OnbsqJtuukkHDhzwqiktLVViYqJCQ0MVEhKisWPHqqCgwKvm4MGDiouLU8uWLdWxY0c99NBDOnHihFfNtm3bNGDAANntdvXo0UNpaWkNvXsAAOAC4ddAlJmZqcTERP3zn/9URkaGKioqNGrUKB07dsxTM336dK1bt06rVq1SZmamDh06pJtvvtmzvLKyUnFxcSovL9fOnTu1fPlypaWlKTU11VOTm5uruLg4jRgxQjk5OUpKStLdd9+tTZs2Ner+AgCApslmWZbl7yaqHT58WB07dlRmZqaGDh2qkpISdejQQStWrNC4ceMkSfv379dll12mrKwsDR48WBs2bNCYMWN06NAhhYWFSZKWLl2qlJQUHT58WIGBgUpJSVF6err27t3r2VZ8fLyKi4u1ceNGnz7KyspUVlbmee92uxUZGamSkhI5HI4G/hQA/BwdnNPH3y0ATVKX1D0NNrfb7ZbT6azVz+8mdQ1RSUmJJKldu3aSpOzsbFVUVCgmJsZT06tXL3Xp0kVZWVmSpKysLPXp08cThiQpNjZWbrdb+/bt89ScPEd1TfUcp5o7d66cTqfnFRkZWX87CQAAmpwmE4iqqqqUlJSkX/7yl+rdu7ckKT8/X4GBgWrTpo1XbVhYmPLz8z01J4eh6uXVy85U43a7dfz4cZ9eZs2apZKSEs8rLy+vXvYRAAA0Tc393UC1xMRE7d27Vx988IG/W5Hdbpfdbvd3GwAAoJE0iSNE999/v9avX6+tW7eqc+fOnvHw8HCVl5eruLjYq76goEDh4eGemlPvOqt+f7Yah8Oh4ODg+t4dAABwgfFrILIsS/fff7/WrFmjLVu2qHv37l7Lo6Oj1aJFC23evNkzduDAAR08eFAul0uS5HK5tGfPHhUWFnpqMjIy5HA4FBUV5ak5eY7qmuo5AACA2fx6yiwxMVErVqzQO++8o9atW3uu+XE6nQoODpbT6dTkyZOVnJysdu3ayeFwaNq0aXK5XBo8eLAkadSoUYqKitIdd9yhefPmKT8/X4888ogSExM9p72mTp2q559/XjNnztSkSZO0ZcsWvfnmm0pPT/fbvgMAgKbDr0eIXnjhBZWUlGj48OHq1KmT57Vy5UpPzTPPPKMxY8Zo7NixGjp0qMLDw/X22297lgcEBGj9+vUKCAiQy+XS7bffrjvvvFNz5szx1HTv3l3p6enKyMhQ3759tWDBAr300kuKjY1t1P0FAABNU5N6DlFTdS7PMQCAmvAcIqBmPIcIAACgiSAQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADCeXwPR9u3b9atf/UoRERGy2Wxau3at13LLspSamqpOnTopODhYMTEx+vLLL71qjhw5ogkTJsjhcKhNmzaaPHmyjh496lXz2WefaciQIQoKClJkZKTmzZvX0LsGAAAuIH4NRMeOHVPfvn21ePHiGpfPmzdPzz77rJYuXaoPP/xQrVq1UmxsrEpLSz01EyZM0L59+5SRkaH169dr+/btuueeezzL3W63Ro0apa5duyo7O1vz58/Xo48+qr/85S8Nvn8AAODCYLMsy/J3E5Jks9m0Zs0a3XTTTZL+c3QoIiJCDz74oGbMmCFJKikpUVhYmNLS0hQfH68vvvhCUVFR+uijjzRw4EBJ0saNGzV69Gj9+9//VkREhF544QX9/ve/V35+vgIDAyVJDz/8sNauXav9+/fXqje32y2n06mSkhI5HI7633kAP3sH5/TxdwtAk9QldU+DzX0uP7+b7DVEubm5ys/PV0xMjGfM6XRq0KBBysrKkiRlZWWpTZs2njAkSTExMWrWrJk+/PBDT83QoUM9YUiSYmNjdeDAARUVFdW47bKyMrndbq8XAAD4+WqygSg/P1+SFBYW5jUeFhbmWZafn6+OHTt6LW/evLnatWvnVVPTHCdv41Rz586V0+n0vCIjI89/hwAAQJPVZAORP82aNUslJSWeV15enr9bAgAADajJBqLw8HBJUkFBgdd4QUGBZ1l4eLgKCwu9lp84cUJHjhzxqqlpjpO3cSq73S6Hw+H1AgAAP19NNhB1795d4eHh2rx5s2fM7Xbrww8/lMvlkiS5XC4VFxcrOzvbU7NlyxZVVVVp0KBBnprt27eroqLCU5ORkaGePXuqbdu2jbQ3AACgKfNrIDp69KhycnKUk5Mj6T8XUufk5OjgwYOy2WxKSkrS448/rnfffVd79uzRnXfeqYiICM+daJdddpmuv/56TZkyRbt27dKOHTt0//33Kz4+XhEREZKk2267TYGBgZo8ebL27dunlStXatGiRUpOTvbTXgMAgKamuT83/vHHH2vEiBGe99UhJSEhQWlpaZo5c6aOHTume+65R8XFxbrmmmu0ceNGBQUFedZ5/fXXdf/99+vaa69Vs2bNNHbsWD377LOe5U6nU++9954SExMVHR2t9u3bKzU11etZRQAAwGxN5jlETRnPIQJwvngOEVAznkMEAADQRBCIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8AhEAADAegQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiAAAgPEIRAAAwHgEIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAYj0AEAACMRyACAADGa+7vBvD/RT/0N3+3ADRJ2fPv9HcLAH7mOEIEAACMRyACAADGIxABAADjEYgAAIDxCEQAAMB4BCIAAGA8owLR4sWL1a1bNwUFBWnQoEHatWuXv1sCAABNgDGBaOXKlUpOTtbs2bO1e/du9e3bV7GxsSosLPR3awAAwM+MCURPP/20pkyZookTJyoqKkpLly5Vy5Yt9corr/i7NQAA4GdGPKm6vLxc2dnZmjVrlmesWbNmiomJUVZWlk99WVmZysrKPO9LSkokSW63u0H7rCw73qDzAxeqhv631xh+LK30dwtAk9SQ/76r57Ys66y1RgSi77//XpWVlQoLC/MaDwsL0/79+33q586dq8cee8xnPDIyssF6BHB6zuem+rsFAA1lrrPBN/Hjjz/K6TzzdowIROdq1qxZSk5O9ryvqqrSkSNHFBoaKpvN5sfO0BjcbrciIyOVl5cnh8Ph73YA1CP+fZvFsiz9+OOPioiIOGutEYGoffv2CggIUEFBgdd4QUGBwsPDfertdrvsdrvXWJs2bRqyRTRBDoeD/2ACP1P8+zbH2Y4MVTPiourAwEBFR0dr8+bNnrGqqipt3rxZLpfLj50BAICmwIgjRJKUnJyshIQEDRw4UFdddZUWLlyoY8eOaeLEif5uDQAA+Jkxgeg3v/mNDh8+rNTUVOXn56tfv37auHGjz4XWgN1u1+zZs31OmwK48PHvG6djs2pzLxoAAMDPmBHXEAEAAJwJgQgAABiPQAQAAIxHIAIAAMYjEAEAAOMRiIBTLF68WN26dVNQUJAGDRqkXbt2+bslAPVg+/bt+tWvfqWIiAjZbDatXbvW3y2hCSEQASdZuXKlkpOTNXv2bO3evVt9+/ZVbGysCgsL/d0agPN07Ngx9e3bV4sXL/Z3K2iCeA4RcJJBgwbpyiuv1PPPPy/pP7/iJTIyUtOmTdPDDz/s5+4A1BebzaY1a9bopptu8ncraCI4QgT8V3l5ubKzsxUTE+MZa9asmWJiYpSVleXHzgAADY1ABPzX999/r8rKSp9f5xIWFqb8/Hw/dQUAaAwEIgAAYDwCEfBf7du3V0BAgAoKCrzGCwoKFB4e7qeuAACNgUAE/FdgYKCio6O1efNmz1hVVZU2b94sl8vlx84AAA2tub8bAJqS5ORkJSQkaODAgbrqqqu0cOFCHTt2TBMnTvR3awDO09GjR/V///d/nve5ubnKyclRu3bt1KVLFz92hqaA2+6BUzz//POaP3++8vPz1a9fPz377LMaNGiQv9sCcJ62bdumESNG+IwnJCQoLS2t8RtCk0IgAgAAxuMaIgAAYDwCEQAAMB6BCAAAGI9ABAAAjEcgAgAAxiMQAQAA4xGIAACA8QhEAADAeAQiAABgPAIRAAAwHoEIAAAY7/8BdMwIunMY5QEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definindo o y_treino binário - falha ou não falha\n",
    "y_treino_bin, y_val_bin = y_treino.where(y_treino == 1, 0), y_val.where(y_val == 1, 0)\n",
    "\n",
    "# Plot\n",
    "sns.countplot(x=y_treino_bin)\n",
    "plt.title('Falha (0) ou Não Falha (1)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "542c8d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o objeto balanceador\n",
    "smote = SMOTE(random_state = 1337)\n",
    "\n",
    "# Aplica o balanceador\n",
    "X_res, y_res = smote.fit_resample(X_treino, y_treino_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e47070f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x=y_res)\n",
    "plt.title('Falha (0) ou Não Falha (1) - depois do oversampling');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a48498",
   "metadata": {},
   "source": [
    "# Criação do Modelo Preditivo - Classificação Binária\n",
    "\n",
    "Após toda a etapa de Análise Exploratória e Pré-Processamento de Dados, podemos finalmente começar a criar e avaliar os modelos preditivos para esse problema. Como o problema é para prever uma classe, um tipo de falha, então é um problema de classificação. Mais especificamente, classificação multiclasse, visto que a variável target possui mais de 2 classes possíveis. No entanto, como mencionado anteriormente, primeiro transformaremos essa classificação multiclasse em classificação binária, e depois faremos a mudança para classificação multiclasse apenas daquelas máquinas em que uma falha for prevista.\n",
    "\n",
    "Entre os algoritmos mais comuns para esse tipo de problema estão:\n",
    "Regressão Logística, KNN, Naive Bayes, Decision Tree, SVM (Support Vector Machines) e Redes Neurais.\n",
    "\n",
    "Criaremos alguns modelos e utilizaremos algumas métricas de avaliação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c1bf1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a função que treina e avalia o modelo\n",
    "def train_and_score_model(algoritmo, X_treino, y_treino, X_teste, y_teste, nome = None, version = '1'):\n",
    "    # Cria o modelo\n",
    "    modelo = algoritmo\n",
    "    \n",
    "    # Treinamento\n",
    "    start = time.time()\n",
    "    modelo.fit(X_treino, y_treino)\n",
    "    end = time.time()\n",
    "    print('Tempo de Treinamento do Modelo:', end - start)\n",
    "    \n",
    "    # Fazendo as previsões para o dataset de teste\n",
    "    previsoes = modelo.predict(X_teste)\n",
    "    \n",
    "    # Avaliação do modelo\n",
    "    # Matriz de confusão\n",
    "    print('\\nMatriz de confusão\\n', confusion_matrix(y_teste, previsoes))\n",
    "    \n",
    "    # Dicionário de métricas e metadados\n",
    "    dict_model =   {'Modelo': nome,\n",
    "                    'Versão': version,\n",
    "                    'Precision':precision_score(y_teste, previsoes),\n",
    "                    'Recall':recall_score(y_teste, previsoes),\n",
    "                    'F1 Score':f1_score(y_teste, previsoes),\n",
    "                    'Acurácia':accuracy_score(y_teste, previsoes)}\n",
    "        \n",
    "    return modelo, dict_model, previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d24a3d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.011992692947387695\n",
      "\n",
      "Matriz de confusão\n",
      " [[   6   52]\n",
      " [   6 1603]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'KNN',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9685800604229607,\n",
       " 'Recall': 0.9962709757613425,\n",
       " 'F1 Score': 0.9822303921568628,\n",
       " 'Acurácia': 0.9652069586082783}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 1 - KNN\n",
    "modelo1, dict1, previsoes1 = train_and_score_model(KNeighborsClassifier(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'KNN', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b85033a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.0030279159545898438\n",
      "\n",
      "Matriz de confusão\n",
      " [[  19   39]\n",
      " [  14 1595]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Naive Bayes',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9761321909424725,\n",
       " 'Recall': 0.9912989434431324,\n",
       " 'F1 Score': 0.9836571076164047,\n",
       " 'Acurácia': 0.9682063587282543}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 2 - Naive Bayes\n",
    "modelo2, dict2, previsoes2 = train_and_score_model(GaussianNB(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'Naive Bayes', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06659399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.013998985290527344\n",
      "\n",
      "Matriz de confusão\n",
      " [[  30   28]\n",
      " [  11 1598]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Decision Tree Classifier',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.982779827798278,\n",
       " 'Recall': 0.9931634555624611,\n",
       " 'F1 Score': 0.9879443585780525,\n",
       " 'Acurácia': 0.9766046790641871}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 3 - Decision Tree Classifier\n",
    "modelo3, dict3, previsoes3 = train_and_score_model(DecisionTreeClassifier(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'Decision Tree Classifier', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "218739c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.4569895267486572\n",
      "\n",
      "Matriz de confusão\n",
      " [[  25   33]\n",
      " [   1 1608]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Random Forest Classifier',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.979890310786106,\n",
       " 'Recall': 0.9993784959602238,\n",
       " 'F1 Score': 0.9895384615384615,\n",
       " 'Acurácia': 0.9796040791841631}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 4 - Decision Tree Classifier\n",
    "modelo4, dict4, previsoes4 = train_and_score_model(RandomForestClassifier(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'Random Forest Classifier', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50ab2b5e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.09900069236755371\n",
      "\n",
      "Matriz de confusão\n",
      " [[   1   57]\n",
      " [   0 1609]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'SVM Classifier',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9657863145258103,\n",
       " 'Recall': 1.0,\n",
       " 'F1 Score': 0.9825954198473282,\n",
       " 'Acurácia': 0.9658068386322736}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 5 - SVM Classifier\n",
    "modelo5, dict5, previsoes5 = train_and_score_model(svm.SVC(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'SVM Classifier', \n",
    "                                      version = 'Binary Classification 1')\n",
    "dict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b980db05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.09499192237854004\n",
      "\n",
      "Matriz de confusão\n",
      " [[   1   57]\n",
      " [   0 1609]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'XGBoost Classifier',\n",
       " 'Versão': 'Binary Classification 1',\n",
       " 'Precision': 0.9657863145258103,\n",
       " 'Recall': 1.0,\n",
       " 'F1 Score': 0.9825954198473282,\n",
       " 'Acurácia': 0.9658068386322736}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 6 - XGBoost Classifier\n",
    "modelo6, dict6, previsoes = train_and_score_model(svm.SVC(), X_treino, y_treino_bin, X_val, y_val_bin, nome = 'XGBoost Classifier', \n",
    "                              version = 'Binary Classification 1')\n",
    "dict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82afacc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict1</th>\n",
       "      <th>dict2</th>\n",
       "      <th>dict3</th>\n",
       "      <th>dict4</th>\n",
       "      <th>dict5</th>\n",
       "      <th>dict6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>XGBoost Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versão</th>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.96858</td>\n",
       "      <td>0.976132</td>\n",
       "      <td>0.98278</td>\n",
       "      <td>0.97989</td>\n",
       "      <td>0.965786</td>\n",
       "      <td>0.965786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.996271</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.993163</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.98223</td>\n",
       "      <td>0.983657</td>\n",
       "      <td>0.987944</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.982595</td>\n",
       "      <td>0.982595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acurácia</th>\n",
       "      <td>0.965207</td>\n",
       "      <td>0.968206</td>\n",
       "      <td>0.976605</td>\n",
       "      <td>0.979604</td>\n",
       "      <td>0.965807</td>\n",
       "      <td>0.965807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dict1                    dict2  \\\n",
       "Modelo                         KNN              Naive Bayes   \n",
       "Versão     Binary Classification 1  Binary Classification 1   \n",
       "Precision                  0.96858                 0.976132   \n",
       "Recall                    0.996271                 0.991299   \n",
       "F1 Score                   0.98223                 0.983657   \n",
       "Acurácia                  0.965207                 0.968206   \n",
       "\n",
       "                              dict3                     dict4  \\\n",
       "Modelo     Decision Tree Classifier  Random Forest Classifier   \n",
       "Versão      Binary Classification 1   Binary Classification 1   \n",
       "Precision                   0.98278                   0.97989   \n",
       "Recall                     0.993163                  0.999378   \n",
       "F1 Score                   0.987944                  0.989538   \n",
       "Acurácia                   0.976605                  0.979604   \n",
       "\n",
       "                             dict5                    dict6  \n",
       "Modelo              SVM Classifier       XGBoost Classifier  \n",
       "Versão     Binary Classification 1  Binary Classification 1  \n",
       "Precision                 0.965786                 0.965786  \n",
       "Recall                         1.0                      1.0  \n",
       "F1 Score                  0.982595                 0.982595  \n",
       "Acurácia                  0.965807                 0.965807  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatena todos os dicionários em um dataframe do Pandas\n",
    "resumo_bin1 = pd.DataFrame({'dict1':pd.Series(dict1),\n",
    "                           'dict2':pd.Series(dict2),\n",
    "                           'dict3':pd.Series(dict3),\n",
    "                           'dict4':pd.Series(dict4),\n",
    "                           'dict5':pd.Series(dict5),\n",
    "                           'dict6':pd.Series(dict6)})\n",
    "\n",
    "resumo_bin1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4058c484",
   "metadata": {},
   "source": [
    "## Repetindo os modelos com o balanceamento de classes (SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5408f73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.021992921829223633\n",
      "\n",
      "Matriz de confusão\n",
      " [[  36   22]\n",
      " [ 107 1502]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'KNN',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9855643044619422,\n",
       " 'Recall': 0.9334990677439403,\n",
       " 'F1 Score': 0.9588254069581871,\n",
       " 'Acurácia': 0.9226154769046191}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 1 - KNN - com SMOTE\n",
    "modelo1, dict1, previsoes1 = train_and_score_model(KNeighborsClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'KNN', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa628f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.003997802734375\n",
      "\n",
      "Matriz de confusão\n",
      " [[  40   18]\n",
      " [ 481 1128]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Naive Bayes',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9842931937172775,\n",
       " 'Recall': 0.7010565568676196,\n",
       " 'F1 Score': 0.8188747731397459,\n",
       " 'Acurácia': 0.7006598680263947}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 2 - Naive Bayes - com SMOTE\n",
    "modelo2, dict2, previsoes2 = train_and_score_model(GaussianNB(), X_res, y_res, X_val, y_val_bin, nome = 'Naive Bayes', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "763bd13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.03700137138366699\n",
      "\n",
      "Matriz de confusão\n",
      " [[  38   20]\n",
      " [  52 1557]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Decision Tree Classifier',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9873176918199112,\n",
       " 'Recall': 0.9676817899316346,\n",
       " 'F1 Score': 0.9774011299435029,\n",
       " 'Acurácia': 0.9568086382723455}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 3 - Decision Tree - com SMOTE\n",
    "modelo3, dict3, previsoes3 = train_and_score_model(DecisionTreeClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'Decision Tree Classifier', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e83ca28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 1.2219958305358887\n",
      "\n",
      "Matriz de confusão\n",
      " [[  36   22]\n",
      " [  39 1570]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Random Forest Classifier',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9861809045226131,\n",
       " 'Recall': 0.9757613424487259,\n",
       " 'F1 Score': 0.9809434551702594,\n",
       " 'Acurácia': 0.9634073185362927}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 4 - Random Forest - com SMOTE\n",
    "modelo4, dict4, previsoes4 = train_and_score_model(RandomForestClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'Random Forest Classifier', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ad66d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 1.2119927406311035\n",
      "\n",
      "Matriz de confusão\n",
      " [[  37   21]\n",
      " [  42 1567]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'SVM Classifier',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9867758186397985,\n",
       " 'Recall': 0.9738968303293971,\n",
       " 'F1 Score': 0.980294025649046,\n",
       " 'Acurácia': 0.9622075584883023}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 5 - SVM Classifier - com SMOTE\n",
    "modelo5, dict5, previsoes5 = train_and_score_model(RandomForestClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'SVM Classifier', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63bec297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.3489987850189209\n",
      "\n",
      "Matriz de confusão\n",
      " [[  36   22]\n",
      " [  30 1579]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'XGBoost Classifier',\n",
       " 'Versão': 'Binary Classification 2',\n",
       " 'Precision': 0.9862585883822611,\n",
       " 'Recall': 0.9813548788067122,\n",
       " 'F1 Score': 0.9838006230529595,\n",
       " 'Acurácia': 0.9688062387522496}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 6 - XGBoost Classifier - com SMOTE\n",
    "modelo6, dict6, previsoes6 = train_and_score_model(XGBClassifier(), X_res, y_res, X_val, y_val_bin, nome = 'XGBoost Classifier', \n",
    "                                          version = 'Binary Classification 2')\n",
    "dict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81e15857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict1</th>\n",
       "      <th>dict2</th>\n",
       "      <th>dict3</th>\n",
       "      <th>dict4</th>\n",
       "      <th>dict5</th>\n",
       "      <th>dict6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>XGBoost Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versão</th>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "      <td>Binary Classification 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.985564</td>\n",
       "      <td>0.984293</td>\n",
       "      <td>0.987318</td>\n",
       "      <td>0.986181</td>\n",
       "      <td>0.986776</td>\n",
       "      <td>0.986259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.933499</td>\n",
       "      <td>0.701057</td>\n",
       "      <td>0.967682</td>\n",
       "      <td>0.975761</td>\n",
       "      <td>0.973897</td>\n",
       "      <td>0.981355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.958825</td>\n",
       "      <td>0.818875</td>\n",
       "      <td>0.977401</td>\n",
       "      <td>0.980943</td>\n",
       "      <td>0.980294</td>\n",
       "      <td>0.983801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acurácia</th>\n",
       "      <td>0.922615</td>\n",
       "      <td>0.70066</td>\n",
       "      <td>0.956809</td>\n",
       "      <td>0.963407</td>\n",
       "      <td>0.962208</td>\n",
       "      <td>0.968806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dict1                    dict2  \\\n",
       "Modelo                         KNN              Naive Bayes   \n",
       "Versão     Binary Classification 2  Binary Classification 2   \n",
       "Precision                 0.985564                 0.984293   \n",
       "Recall                    0.933499                 0.701057   \n",
       "F1 Score                  0.958825                 0.818875   \n",
       "Acurácia                  0.922615                  0.70066   \n",
       "\n",
       "                              dict3                     dict4  \\\n",
       "Modelo     Decision Tree Classifier  Random Forest Classifier   \n",
       "Versão      Binary Classification 2   Binary Classification 2   \n",
       "Precision                  0.987318                  0.986181   \n",
       "Recall                     0.967682                  0.975761   \n",
       "F1 Score                   0.977401                  0.980943   \n",
       "Acurácia                   0.956809                  0.963407   \n",
       "\n",
       "                             dict5                    dict6  \n",
       "Modelo              SVM Classifier       XGBoost Classifier  \n",
       "Versão     Binary Classification 2  Binary Classification 2  \n",
       "Precision                 0.986776                 0.986259  \n",
       "Recall                    0.973897                 0.981355  \n",
       "F1 Score                  0.980294                 0.983801  \n",
       "Acurácia                  0.962208                 0.968806  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatena todos os dicionários em um dataframe do Pandas\n",
    "resumo_bin2 = pd.DataFrame({'dict1':pd.Series(dict1),\n",
    "                           'dict2':pd.Series(dict2),\n",
    "                           'dict3':pd.Series(dict3),\n",
    "                           'dict4':pd.Series(dict4),\n",
    "                           'dict5':pd.Series(dict5),\n",
    "                           'dict6':pd.Series(dict6)})\n",
    "\n",
    "resumo_bin2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1af1eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict1</th>\n",
       "      <th>dict2</th>\n",
       "      <th>dict3</th>\n",
       "      <th>dict4</th>\n",
       "      <th>dict5</th>\n",
       "      <th>dict6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>XGBoost Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versão</th>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "      <td>Binary Classification 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.96858</td>\n",
       "      <td>0.976132</td>\n",
       "      <td>0.98278</td>\n",
       "      <td>0.97989</td>\n",
       "      <td>0.965786</td>\n",
       "      <td>0.965786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.996271</td>\n",
       "      <td>0.991299</td>\n",
       "      <td>0.993163</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.98223</td>\n",
       "      <td>0.983657</td>\n",
       "      <td>0.987944</td>\n",
       "      <td>0.989538</td>\n",
       "      <td>0.982595</td>\n",
       "      <td>0.982595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acurácia</th>\n",
       "      <td>0.965207</td>\n",
       "      <td>0.968206</td>\n",
       "      <td>0.976605</td>\n",
       "      <td>0.979604</td>\n",
       "      <td>0.965807</td>\n",
       "      <td>0.965807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             dict1                    dict2  \\\n",
       "Modelo                         KNN              Naive Bayes   \n",
       "Versão     Binary Classification 1  Binary Classification 1   \n",
       "Precision                  0.96858                 0.976132   \n",
       "Recall                    0.996271                 0.991299   \n",
       "F1 Score                   0.98223                 0.983657   \n",
       "Acurácia                  0.965207                 0.968206   \n",
       "\n",
       "                              dict3                     dict4  \\\n",
       "Modelo     Decision Tree Classifier  Random Forest Classifier   \n",
       "Versão      Binary Classification 1   Binary Classification 1   \n",
       "Precision                   0.98278                   0.97989   \n",
       "Recall                     0.993163                  0.999378   \n",
       "F1 Score                   0.987944                  0.989538   \n",
       "Acurácia                   0.976605                  0.979604   \n",
       "\n",
       "                             dict5                    dict6  \n",
       "Modelo              SVM Classifier       XGBoost Classifier  \n",
       "Versão     Binary Classification 1  Binary Classification 1  \n",
       "Precision                 0.965786                 0.965786  \n",
       "Recall                         1.0                      1.0  \n",
       "F1 Score                  0.982595                 0.982595  \n",
       "Acurácia                  0.965807                 0.965807  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumo_bin1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041ea186",
   "metadata": {},
   "source": [
    "## Avaliando as métricas\n",
    "\n",
    "Para avaliar as métricas é necessário definir: qual a classe positiva e qual a classe negativa do problema?\n",
    "\n",
    "Classe positiva: Máquinas sem falha (1)\n",
    "Classe negativa: Máquinas com falha (0)\n",
    "\n",
    "**Precision**: Essa métrica responde à pergunta: que proporção das previsões positivas são realmente positivas? Para esse problema, significa quantas máquinas que foram previstas como sem falhas realmente não possuem falhas. É necessário otimizar essa métrica se o problema requerer um baixo número de **falsos positivos**. Estes significariam máquinas que têm falhas mas foram previstas como se não tivessem. Como nosso problema é de manutenção preditiva, esse tipo de falha é muito mais grave do que prever falhas nas máquinas que não a possuem, ou falsos negativos.\n",
    "\n",
    "**Recall**: Responde à pergunta: que proporção dos verdadeiros positivos estão corretamente classificados? Para esse problema, significa as máquinas que não possuem falhas e foram classificadas corretamente. É necessário otimizar essa métrica se o problema requerer um baixo número de **falsos negativos**. Estes significariam máquinas que não possuem falhas mas foram previstas como se tivessem. Como nosso problema é de manutenção preditiva, esse tipo de falha é menos grave do que prever falsos positivos.\n",
    "\n",
    "**F1 Score**: É a média harmônica entre precision e recall. Fortemente afetado caso uma das duas métricas seja muito baixa. É necessário otimizar essa métrica quando o problema requerer um bom equilíbrio de falsos positivos e negativos.\n",
    "\n",
    "**Acurácia**: É simplesmente o percentual de acerto do modelo. \n",
    "\n",
    "Considerando nosso problema, Precision será nossa métrica base para a classificação binária. \n",
    "Posteriormente, na avaliação da classificação das falhas, utilizaremos a métrica de ROC AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d7b04",
   "metadata": {},
   "source": [
    "# Criação do Modelo Preditivo - Classificação Multiclasse\n",
    "\n",
    "O melhor modelo encontrado para a classificação binária foi o Decision Tree com aplicação de SMOTE, com Precision de 0.987967. Seguido pelo modelo XGBoost Classifier, também com aplicação de SMOTE e Precision de 0.986259.\n",
    "\n",
    "Agora, criaremos outro modelo usando dados de treinamento apenas nos casos em que há falha, para prever qual o tipo de falha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f3b4213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrando os dados em que há falha - Treino\n",
    "y_falha_treino = y_treino[y_treino != 1]\n",
    "X_falha_treino = X_treino.loc[y_falha_treino.index]\n",
    "\n",
    "# Filtrando os dados em que há falha - Validação\n",
    "y_falha_val = y_val[y_val != 1]\n",
    "X_falha_val = X_val.loc[y_falha_val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4129a74a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>1.864115</td>\n",
       "      <td>1.821454</td>\n",
       "      <td>-1.006314</td>\n",
       "      <td>1.673215</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>-0.646829</td>\n",
       "      <td>-0.126553</td>\n",
       "      <td>-1.034447</td>\n",
       "      <td>1.323047</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3082</th>\n",
       "      <td>1.512583</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>1.613186</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3009</th>\n",
       "      <td>1.311707</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.006314</td>\n",
       "      <td>1.713234</td>\n",
       "      <td>0.272358</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2588</th>\n",
       "      <td>1.211270</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-1.124471</td>\n",
       "      <td>2.753734</td>\n",
       "      <td>0.788618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2371</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.899410</td>\n",
       "      <td>1.303037</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3658</th>\n",
       "      <td>1.361926</td>\n",
       "      <td>1.552764</td>\n",
       "      <td>-1.079459</td>\n",
       "      <td>2.113426</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>-0.194859</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>1.851970</td>\n",
       "      <td>-1.668389</td>\n",
       "      <td>0.914634</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>-0.193726</td>\n",
       "      <td>-0.854397</td>\n",
       "      <td>2.263499</td>\n",
       "      <td>0.577236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3098</th>\n",
       "      <td>1.462364</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-1.169484</td>\n",
       "      <td>1.363066</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "3330           1.864115               1.821454             -1.006314   \n",
       "950           -0.646829              -0.126553             -1.034447   \n",
       "3082           1.512583               0.881037             -0.972555   \n",
       "3009           1.311707               0.276483             -1.006314   \n",
       "2588           1.211270               0.679519             -1.124471   \n",
       "...                 ...                    ...                   ...   \n",
       "2371           1.010394               0.612346             -0.899410   \n",
       "3658           1.361926               1.552764             -1.079459   \n",
       "1448          -0.194859              -0.529589              1.851970   \n",
       "2917           1.010394              -0.193726             -0.854397   \n",
       "3098           1.462364               0.813864             -1.169484   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "3330   1.673215       0.788618  0.0  1.0  0.0  \n",
       "950    1.323047       0.861789  0.0  1.0  0.0  \n",
       "3082   1.613186       0.593496  0.0  1.0  0.0  \n",
       "3009   1.713234       0.272358  0.0  1.0  0.0  \n",
       "2588   2.753734       0.788618  0.0  1.0  0.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "2371   1.303037       0.861789  0.0  1.0  0.0  \n",
       "3658   2.113426       0.691057  0.0  1.0  0.0  \n",
       "1448  -1.668389       0.914634  0.0  0.0  1.0  \n",
       "2917   2.263499       0.577236  0.0  0.0  1.0  \n",
       "3098   1.363066       0.813008  0.0  0.0  1.0  \n",
       "\n",
       "[150 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_falha_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2134f5c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3330    2\n",
       "950     2\n",
       "3082    0\n",
       "3009    0\n",
       "2588    3\n",
       "       ..\n",
       "2371    2\n",
       "3658    4\n",
       "1448    5\n",
       "2917    3\n",
       "3098    0\n",
       "Length: 150, dtype: int32"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_falha_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da4858f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>-0.777961</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3156</th>\n",
       "      <td>1.663240</td>\n",
       "      <td>1.082555</td>\n",
       "      <td>-1.130098</td>\n",
       "      <td>1.673215</td>\n",
       "      <td>0.760163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>0.257111</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>6.161902</td>\n",
       "      <td>-2.788927</td>\n",
       "      <td>0.069106</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4938</th>\n",
       "      <td>0.006016</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>0.867325</td>\n",
       "      <td>-1.108120</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6274</th>\n",
       "      <td>-0.948143</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>-1.034447</td>\n",
       "      <td>1.383076</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.062579</td>\n",
       "      <td>1.503134</td>\n",
       "      <td>0.760163</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2473</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-1.507076</td>\n",
       "      <td>2.523623</td>\n",
       "      <td>0.739837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5720</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>-1.146978</td>\n",
       "      <td>3.193945</td>\n",
       "      <td>0.613821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2982</th>\n",
       "      <td>1.211270</td>\n",
       "      <td>0.209310</td>\n",
       "      <td>-0.899410</td>\n",
       "      <td>1.423095</td>\n",
       "      <td>0.670732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2961</th>\n",
       "      <td>1.311707</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>6.403843</td>\n",
       "      <td>-2.928994</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>-1.801864</td>\n",
       "      <td>-1.738698</td>\n",
       "      <td>7.287211</td>\n",
       "      <td>-3.449244</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>-0.395735</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>-1.085086</td>\n",
       "      <td>2.063402</td>\n",
       "      <td>0.776423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6433</th>\n",
       "      <td>-0.445954</td>\n",
       "      <td>0.007792</td>\n",
       "      <td>-0.961302</td>\n",
       "      <td>1.623191</td>\n",
       "      <td>0.825203</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3188</th>\n",
       "      <td>1.864115</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.158231</td>\n",
       "      <td>1.263018</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>-1.349894</td>\n",
       "      <td>-1.268489</td>\n",
       "      <td>0.434081</td>\n",
       "      <td>-0.467813</td>\n",
       "      <td>0.882114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>-0.345516</td>\n",
       "      <td>-0.731107</td>\n",
       "      <td>-1.574595</td>\n",
       "      <td>2.933820</td>\n",
       "      <td>0.483740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>0.859737</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.747493</td>\n",
       "      <td>0.632716</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3164</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>2.093417</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>-0.395735</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>-1.523956</td>\n",
       "      <td>2.813763</td>\n",
       "      <td>0.308943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>0.960175</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.417052</td>\n",
       "      <td>2.273503</td>\n",
       "      <td>0.089431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2568</th>\n",
       "      <td>1.161051</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.966928</td>\n",
       "      <td>0.832812</td>\n",
       "      <td>0.528455</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>1.361926</td>\n",
       "      <td>1.619936</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>0.622711</td>\n",
       "      <td>1.020325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3048</th>\n",
       "      <td>1.361926</td>\n",
       "      <td>0.545173</td>\n",
       "      <td>-1.478944</td>\n",
       "      <td>1.493129</td>\n",
       "      <td>0.101626</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5338</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-0.781252</td>\n",
       "      <td>1.012898</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2763</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.478001</td>\n",
       "      <td>-0.978181</td>\n",
       "      <td>0.772783</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-1.495823</td>\n",
       "      <td>1.913330</td>\n",
       "      <td>0.804878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>-0.044203</td>\n",
       "      <td>0.142137</td>\n",
       "      <td>-1.405799</td>\n",
       "      <td>2.953830</td>\n",
       "      <td>0.252033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.349533</td>\n",
       "      <td>2.383556</td>\n",
       "      <td>0.239837</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3162</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-1.107592</td>\n",
       "      <td>0.692745</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>-0.445954</td>\n",
       "      <td>-0.798280</td>\n",
       "      <td>-1.175110</td>\n",
       "      <td>1.353062</td>\n",
       "      <td>0.841463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-0.561817</td>\n",
       "      <td>0.662730</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>-0.998362</td>\n",
       "      <td>-1.201316</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>1.283028</td>\n",
       "      <td>0.886179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>-1.299675</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>7.517899</td>\n",
       "      <td>-3.589311</td>\n",
       "      <td>0.479675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-1.017567</td>\n",
       "      <td>1.793273</td>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.074965</td>\n",
       "      <td>-1.191990</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>0.313008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-0.860024</td>\n",
       "      <td>1.633196</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2796</th>\n",
       "      <td>1.110832</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>-1.028820</td>\n",
       "      <td>1.142961</td>\n",
       "      <td>0.512195</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.134143</td>\n",
       "      <td>-0.781252</td>\n",
       "      <td>0.632716</td>\n",
       "      <td>0.536585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-0.905036</td>\n",
       "      <td>0.152485</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>0.257111</td>\n",
       "      <td>-0.059381</td>\n",
       "      <td>-0.792505</td>\n",
       "      <td>0.582692</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.351246</td>\n",
       "      <td>-0.094814</td>\n",
       "      <td>-0.417789</td>\n",
       "      <td>0.873984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2787</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-1.546462</td>\n",
       "      <td>2.053398</td>\n",
       "      <td>0.365854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>-2.153396</td>\n",
       "      <td>-2.544770</td>\n",
       "      <td>4.119467</td>\n",
       "      <td>-2.548811</td>\n",
       "      <td>0.605691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>1.512583</td>\n",
       "      <td>1.485591</td>\n",
       "      <td>-1.580221</td>\n",
       "      <td>3.123912</td>\n",
       "      <td>0.495935</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1534</th>\n",
       "      <td>-0.295297</td>\n",
       "      <td>-0.865453</td>\n",
       "      <td>-0.516805</td>\n",
       "      <td>2.283508</td>\n",
       "      <td>0.544715</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3055</th>\n",
       "      <td>1.512583</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-0.921916</td>\n",
       "      <td>0.812802</td>\n",
       "      <td>0.264228</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.619936</td>\n",
       "      <td>-0.713734</td>\n",
       "      <td>1.373071</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>1.361926</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-0.516805</td>\n",
       "      <td>0.082452</td>\n",
       "      <td>0.857724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4160</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.074965</td>\n",
       "      <td>-0.747493</td>\n",
       "      <td>2.113426</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4113</th>\n",
       "      <td>0.558424</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-1.439558</td>\n",
       "      <td>2.783748</td>\n",
       "      <td>0.040650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>-0.897924</td>\n",
       "      <td>-1.066971</td>\n",
       "      <td>-1.282015</td>\n",
       "      <td>2.093417</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>-1.651207</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-1.574595</td>\n",
       "      <td>2.893801</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3173</th>\n",
       "      <td>1.613021</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>-1.051326</td>\n",
       "      <td>1.373071</td>\n",
       "      <td>0.032520</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>1.010394</td>\n",
       "      <td>0.545173</td>\n",
       "      <td>0.434081</td>\n",
       "      <td>-1.108120</td>\n",
       "      <td>0.955285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2847</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-1.023194</td>\n",
       "      <td>1.543153</td>\n",
       "      <td>0.410569</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>1.864115</td>\n",
       "      <td>1.351246</td>\n",
       "      <td>6.330698</td>\n",
       "      <td>-2.868965</td>\n",
       "      <td>0.288618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>1.311707</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-1.428305</td>\n",
       "      <td>2.793753</td>\n",
       "      <td>0.463415</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4771</th>\n",
       "      <td>0.156673</td>\n",
       "      <td>0.209310</td>\n",
       "      <td>-0.989434</td>\n",
       "      <td>2.023383</td>\n",
       "      <td>0.837398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "2330           0.809518               0.343655              0.360936   \n",
       "3156           1.663240               1.082555             -1.130098   \n",
       "4667           0.257111               0.343655              6.161902   \n",
       "4938           0.006016               0.948209              0.867325   \n",
       "6274          -0.948143              -0.932625             -1.034447   \n",
       "3038           1.261488               0.276483             -1.062579   \n",
       "2473           1.010394               0.813864             -1.507076   \n",
       "5720          -1.249456              -1.268489             -1.146978   \n",
       "2982           1.211270               0.209310             -0.899410   \n",
       "2961           1.311707               0.142137              6.403843   \n",
       "570           -1.801864              -1.738698              7.287211   \n",
       "1726          -0.395735              -0.596762             -1.085086   \n",
       "6433          -0.445954               0.007792             -0.961302   \n",
       "3188           1.864115               1.284073             -1.158231   \n",
       "6005          -1.349894              -1.268489              0.434081   \n",
       "1424          -0.345516              -0.731107             -1.574595   \n",
       "2425           0.859737               0.612346             -0.747493   \n",
       "3164           1.713458               1.216900             -1.304521   \n",
       "1633          -0.395735              -0.932625             -1.523956   \n",
       "2771           0.960175               0.276483             -1.417052   \n",
       "2568           1.161051               0.612346             -0.966928   \n",
       "3611           1.361926               1.619936             -0.342382   \n",
       "3048           1.361926               0.545173             -1.478944   \n",
       "5338           0.357548               1.284073             -0.781252   \n",
       "2763           1.060613               0.478001             -0.978181   \n",
       "5166           0.206892               1.216900             -1.495823   \n",
       "4842          -0.044203               0.142137             -1.405799   \n",
       "3238           1.713458               1.284073             -1.349533   \n",
       "3162           1.713458               1.216900             -1.107592   \n",
       "1665          -0.445954              -0.798280             -1.175110   \n",
       "5238           0.206892               1.284073             -0.561817   \n",
       "1061          -0.998362              -1.201316             -0.972555   \n",
       "320           -1.299675              -0.865453              7.517899   \n",
       "2738           1.010394               0.276483             -1.017567   \n",
       "2901           0.809518               0.074965             -1.191990   \n",
       "730           -1.500550              -1.470007             -0.860024   \n",
       "2796           1.110832               0.410828             -1.028820   \n",
       "817           -1.500550              -1.134143             -0.781252   \n",
       "3235           1.713458               1.284073             -0.905036   \n",
       "4221           0.257111              -0.059381             -0.792505   \n",
       "3226           1.713458               1.351246             -0.094814   \n",
       "2787           1.010394               0.343655             -1.546462   \n",
       "609           -2.153396              -2.544770              4.119467   \n",
       "3749           1.512583               1.485591             -1.580221   \n",
       "1534          -0.295297              -0.865453             -0.516805   \n",
       "3055           1.512583               0.813864             -0.921916   \n",
       "3610           1.412145               1.619936             -0.713734   \n",
       "2994           1.361926               0.343655             -0.516805   \n",
       "4160           0.407767               0.074965             -0.747493   \n",
       "4113           0.558424               0.612346             -1.439558   \n",
       "1053          -0.897924              -1.066971             -1.282015   \n",
       "752           -1.651207              -1.470007             -1.574595   \n",
       "3173           1.613021               0.948209             -1.051326   \n",
       "2702           1.010394               0.545173              0.434081   \n",
       "2847           1.261488               0.612346             -1.023194   \n",
       "3189           1.864115               1.351246              6.330698   \n",
       "2850           1.311707               0.679519             -1.428305   \n",
       "4771           0.156673               0.209310             -0.989434   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "2330  -0.777961       0.008130  1.0  0.0  0.0  \n",
       "3156   1.673215       0.760163  0.0  1.0  0.0  \n",
       "4667  -2.788927       0.069106  0.0  1.0  0.0  \n",
       "4938  -1.108120       0.878049  0.0  0.0  1.0  \n",
       "6274   1.383076       0.841463  0.0  1.0  0.0  \n",
       "3038   1.503134       0.760163  0.0  1.0  0.0  \n",
       "2473   2.523623       0.739837  0.0  1.0  0.0  \n",
       "5720   3.193945       0.613821  0.0  0.0  1.0  \n",
       "2982   1.423095       0.670732  0.0  0.0  1.0  \n",
       "2961  -2.928994       0.341463  0.0  0.0  1.0  \n",
       "570   -3.449244       0.865854  0.0  1.0  0.0  \n",
       "1726   2.063402       0.776423  0.0  1.0  0.0  \n",
       "6433   1.623191       0.825203  0.0  1.0  0.0  \n",
       "3188   1.263018       0.268293  1.0  0.0  0.0  \n",
       "6005  -0.467813       0.882114  0.0  1.0  0.0  \n",
       "1424   2.933820       0.483740  0.0  1.0  0.0  \n",
       "2425   0.632716       0.841463  0.0  1.0  0.0  \n",
       "3164   2.093417       0.873984  0.0  1.0  0.0  \n",
       "1633   2.813763       0.308943  0.0  1.0  0.0  \n",
       "2771   2.273503       0.089431  0.0  1.0  0.0  \n",
       "2568   0.832812       0.528455  1.0  0.0  0.0  \n",
       "3611   0.622711       1.020325  0.0  1.0  0.0  \n",
       "3048   1.493129       0.101626  1.0  0.0  0.0  \n",
       "5338   1.012898       0.902439  0.0  1.0  0.0  \n",
       "2763   0.772783       0.865854  0.0  0.0  1.0  \n",
       "5166   1.913330       0.804878  0.0  1.0  0.0  \n",
       "4842   2.953830       0.252033  0.0  1.0  0.0  \n",
       "3238   2.383556       0.239837  0.0  1.0  0.0  \n",
       "3162   0.692745       0.853659  0.0  0.0  1.0  \n",
       "1665   1.353062       0.841463  0.0  1.0  0.0  \n",
       "5238   0.662730       0.166667  1.0  0.0  0.0  \n",
       "1061   1.283028       0.886179  0.0  1.0  0.0  \n",
       "320   -3.589311       0.479675  0.0  1.0  0.0  \n",
       "2738   1.793273       0.439024  0.0  1.0  0.0  \n",
       "2901   0.812802       0.313008  0.0  1.0  0.0  \n",
       "730    1.633196       0.821138  0.0  1.0  0.0  \n",
       "2796   1.142961       0.512195  0.0  0.0  1.0  \n",
       "817    0.632716       0.536585  0.0  0.0  1.0  \n",
       "3235   0.152485       0.138211  0.0  1.0  0.0  \n",
       "4221   0.582692       0.853659  1.0  0.0  0.0  \n",
       "3226  -0.417789       0.873984  0.0  1.0  0.0  \n",
       "2787   2.053398       0.365854  0.0  1.0  0.0  \n",
       "609   -2.548811       0.605691  0.0  1.0  0.0  \n",
       "3749   3.123912       0.495935  0.0  0.0  1.0  \n",
       "1534   2.283508       0.544715  0.0  1.0  0.0  \n",
       "3055   0.812802       0.264228  0.0  1.0  0.0  \n",
       "3610   1.373071       1.000000  1.0  0.0  0.0  \n",
       "2994   0.082452       0.857724  0.0  1.0  0.0  \n",
       "4160   2.113426       0.768293  0.0  1.0  0.0  \n",
       "4113   2.783748       0.040650  0.0  1.0  0.0  \n",
       "1053   2.093417       0.768293  0.0  1.0  0.0  \n",
       "752    2.893801       0.426829  0.0  0.0  1.0  \n",
       "3173   1.373071       0.032520  0.0  1.0  0.0  \n",
       "2702  -1.108120       0.955285  0.0  1.0  0.0  \n",
       "2847   1.543153       0.410569  1.0  0.0  0.0  \n",
       "3189  -2.868965       0.288618  0.0  0.0  1.0  \n",
       "2850   2.793753       0.463415  0.0  1.0  0.0  \n",
       "4771   2.023383       0.837398  0.0  1.0  0.0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_falha_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c94db8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2330    4\n",
       "3156    0\n",
       "4667    3\n",
       "4938    5\n",
       "6274    2\n",
       "3038    0\n",
       "2473    2\n",
       "5720    3\n",
       "2982    0\n",
       "2961    3\n",
       "570     3\n",
       "1726    2\n",
       "6433    2\n",
       "3188    0\n",
       "6005    5\n",
       "1424    3\n",
       "2425    5\n",
       "3164    0\n",
       "1633    3\n",
       "2771    0\n",
       "2568    0\n",
       "3611    2\n",
       "3048    0\n",
       "5338    2\n",
       "2763    0\n",
       "5166    2\n",
       "4842    3\n",
       "3238    0\n",
       "3162    0\n",
       "1665    2\n",
       "5238    4\n",
       "1061    2\n",
       "320     3\n",
       "2738    0\n",
       "2901    0\n",
       "730     2\n",
       "2796    0\n",
       "817     4\n",
       "3235    0\n",
       "4221    5\n",
       "3226    5\n",
       "2787    0\n",
       "609     3\n",
       "3749    3\n",
       "1534    3\n",
       "3055    0\n",
       "3610    2\n",
       "2994    5\n",
       "4160    3\n",
       "4113    3\n",
       "1053    2\n",
       "752     3\n",
       "3173    0\n",
       "2702    5\n",
       "2847    0\n",
       "3189    3\n",
       "2850    3\n",
       "4771    2\n",
       "dtype: int32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_falha_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f475379",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot das falhas no dataset de treino\n",
    "sns.countplot(x=y_falha_treino)\n",
    "plt.title('Tipo de falha nas máquinas no dataset de treino');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b09764c",
   "metadata": {},
   "source": [
    "Nesse caso já possuímos um dataset com classes melhor balanceadas, não precisamos de _oversampling_. No entanto, alguns algoritmos de Machine Learning, como XGBoost, não aceitam receber os labels das _n_ classes fora da ordem 0 até a classe _n_-1. Então faremos um novo encoding, que será desfeito após a previsão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "775966ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_falha_treino_copia = y_falha_treino.copy()\n",
    "y_falha_val_copia = y_falha_val.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7754daed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novo encoding para ficar no intervalo (0,4)\n",
    "y_falha_treino[y_falha_treino > 0] = y_falha_treino - 1\n",
    "y_falha_val[y_falha_val > 0] = y_falha_val - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dc306b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot das falhas no dataset de treino - Após novo encoding\n",
    "sns.countplot(x=y_falha_treino)\n",
    "plt.title('Tipo de falha nas máquinas no dataset de treino');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10277491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a função que treina e avalia o modelo multiclasse\n",
    "def train_and_score_multiclass_model(algoritmo, X_treino, y_treino, X_teste, y_teste, nome = None, version = '1', auc = False):\n",
    "    # Cria o modelo\n",
    "    modelo = algoritmo\n",
    "    \n",
    "    # Treinamento\n",
    "    start = time.time()\n",
    "    modelo.fit(X_treino, y_treino)\n",
    "    end = time.time()\n",
    "    print('Tempo de Treinamento do Modelo:', end - start)\n",
    "    \n",
    "    # Fazendo as previsões para o dataset de teste\n",
    "    previsoes = modelo.predict(X_teste)\n",
    "    \n",
    "    # Avaliação do modelo\n",
    "    # Matriz de confusão\n",
    "    print('\\nMatriz de confusão\\n', confusion_matrix(y_teste, previsoes))\n",
    "    \n",
    "    # Dicionário de métricas e metadados\n",
    "    dict_model =   {'Modelo': nome,\n",
    "                    'Versão': version,\n",
    "                    'Precision':precision_score(y_teste, previsoes, average = 'macro', zero_division=0),\n",
    "                    'Recall':recall_score(y_teste, previsoes, average = 'macro', zero_division=0),\n",
    "                    'F1 Score':f1_score(y_teste, previsoes, average = 'macro', zero_division=0),\n",
    "                    'Acurácia':accuracy_score(y_teste, previsoes)}\n",
    "    \n",
    "    if auc:\n",
    "        dict_model['ROC AUC'] = roc_auc_score(y_teste, previsoes, multi_class='ovr')\n",
    "        \n",
    "    return modelo, dict_model, previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26efaa0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.0019888877868652344\n",
      "\n",
      "Matriz de confusão\n",
      " [[18  0  1  0  0]\n",
      " [ 2 10  1  0  0]\n",
      " [ 1  2  7  0  6]\n",
      " [ 0  1  0  1  1]\n",
      " [ 3  0  0  1  3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'KNN',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.6194017094017094,\n",
       " 'Recall': 0.5832007904376326,\n",
       " 'F1 Score': 0.5838762496053878,\n",
       " 'Acurácia': 0.6724137931034483}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 1 - KNN - Multiclasse\n",
    "modelo1_multi, dict1, previsoes1 = train_and_score_multiclass_model(KNeighborsClassifier(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'KNN',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bca94286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.003003358840942383\n",
      "\n",
      "Matriz de confusão\n",
      " [[ 2  3  0 12  2]\n",
      " [ 0 11  0  2  0]\n",
      " [ 0  1  3  7  5]\n",
      " [ 0  0  1  2  0]\n",
      " [ 0  2  0  1  4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Naive Bayes',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.5688057040998218,\n",
       " 'Recall': 0.4754024484287642,\n",
       " 'F1 Score': 0.3632804232804233,\n",
       " 'Acurácia': 0.3793103448275862}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 2 - Naive Bayes - Multiclasse\n",
    "modelo2_multi, dict2, previsoes2 = train_and_score_multiclass_model(GaussianNB(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'Naive Bayes',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3e03b252",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.0030040740966796875\n",
      "\n",
      "Matriz de confusão\n",
      " [[15  1  1  1  1]\n",
      " [ 0 11  1  1  0]\n",
      " [ 0  1  9  5  1]\n",
      " [ 0  0  0  3  0]\n",
      " [ 0  0  0  2  5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Decision Tree Classifier',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.7257242757242758,\n",
       " 'Recall': 0.7824826489300174,\n",
       " 'F1 Score': 0.7018918336565395,\n",
       " 'Acurácia': 0.7413793103448276}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 3 - Decision Tree - Multiclasse\n",
    "modelo3_multi, dict3, previsoes3 = train_and_score_multiclass_model(DecisionTreeClassifier(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'Decision Tree Classifier',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d67c9eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.17299342155456543\n",
      "\n",
      "Matriz de confusão\n",
      " [[18  0  1  0  0]\n",
      " [ 0 12  1  0  0]\n",
      " [ 0  1  9  5  1]\n",
      " [ 0  0  1  2  0]\n",
      " [ 0  1  0  1  5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'Random Forest Classifier',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.7380952380952381,\n",
       " 'Recall': 0.7627795450163871,\n",
       " 'F1 Score': 0.7275172275172276,\n",
       " 'Acurácia': 0.7931034482758621}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 4 - Decision Tree - Multiclasse\n",
    "modelo4_multi, dict4, previsoes4 = train_and_score_multiclass_model(RandomForestClassifier(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'Random Forest Classifier',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3c07325f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.0050029754638671875\n",
      "\n",
      "Matriz de confusão\n",
      " [[17  1  1  0  0]\n",
      " [ 0 13  0  0  0]\n",
      " [ 0  1  9  0  6]\n",
      " [ 0  0  0  1  2]\n",
      " [ 2  0  0  0  5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'SVM Classifier',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.8092037786774628,\n",
       " 'Recall': 0.7009711779448622,\n",
       " 'F1 Score': 0.7031231925968768,\n",
       " 'Acurácia': 0.7758620689655172}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 5 - SVM Classifier - Multiclasse\n",
    "modelo5_multi, dict5, previsoes5 = train_and_score_multiclass_model(svm.SVC(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'SVM Classifier',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f76a8c80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tempo de Treinamento do Modelo: 0.09099984169006348\n",
      "\n",
      "Matriz de confusão\n",
      " [[18  1  0  0  0]\n",
      " [ 1 11  0  1  0]\n",
      " [ 1  0  9  5  1]\n",
      " [ 0  0  0  2  1]\n",
      " [ 0  0  0  1  6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'XGBoost Classifier',\n",
       " 'Versão': 'Multiclass Classification 1',\n",
       " 'Precision': 0.7577777777777778,\n",
       " 'Recall': 0.7759663582032003,\n",
       " 'F1 Score': 0.7312820512820513,\n",
       " 'Acurácia': 0.7931034482758621}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 6 - SVM Classifier - Multiclasse\n",
    "modelo6_multi, dict6, previsoes6 = train_and_score_multiclass_model(XGBClassifier(), X_falha_treino, y_falha_treino, \n",
    "                                                                       X_falha_val, y_falha_val,\n",
    "                                                                       nome = 'XGBoost Classifier',\n",
    "                                                                       version = 'Multiclass Classification 1')\n",
    "dict6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eaab7dfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dict1</th>\n",
       "      <th>dict2</th>\n",
       "      <th>dict3</th>\n",
       "      <th>dict4</th>\n",
       "      <th>dict5</th>\n",
       "      <th>dict6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Modelo</th>\n",
       "      <td>KNN</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>SVM Classifier</td>\n",
       "      <td>XGBoost Classifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Versão</th>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "      <td>Multiclass Classification 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Precision</th>\n",
       "      <td>0.619402</td>\n",
       "      <td>0.568806</td>\n",
       "      <td>0.725724</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.809204</td>\n",
       "      <td>0.757778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recall</th>\n",
       "      <td>0.583201</td>\n",
       "      <td>0.475402</td>\n",
       "      <td>0.782483</td>\n",
       "      <td>0.76278</td>\n",
       "      <td>0.700971</td>\n",
       "      <td>0.775966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score</th>\n",
       "      <td>0.583876</td>\n",
       "      <td>0.36328</td>\n",
       "      <td>0.701892</td>\n",
       "      <td>0.727517</td>\n",
       "      <td>0.703123</td>\n",
       "      <td>0.731282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acurácia</th>\n",
       "      <td>0.672414</td>\n",
       "      <td>0.37931</td>\n",
       "      <td>0.741379</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.793103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 dict1                        dict2  \\\n",
       "Modelo                             KNN                  Naive Bayes   \n",
       "Versão     Multiclass Classification 1  Multiclass Classification 1   \n",
       "Precision                     0.619402                     0.568806   \n",
       "Recall                        0.583201                     0.475402   \n",
       "F1 Score                      0.583876                      0.36328   \n",
       "Acurácia                      0.672414                      0.37931   \n",
       "\n",
       "                                 dict3                        dict4  \\\n",
       "Modelo        Decision Tree Classifier     Random Forest Classifier   \n",
       "Versão     Multiclass Classification 1  Multiclass Classification 1   \n",
       "Precision                     0.725724                     0.738095   \n",
       "Recall                        0.782483                      0.76278   \n",
       "F1 Score                      0.701892                     0.727517   \n",
       "Acurácia                      0.741379                     0.793103   \n",
       "\n",
       "                                 dict5                        dict6  \n",
       "Modelo                  SVM Classifier           XGBoost Classifier  \n",
       "Versão     Multiclass Classification 1  Multiclass Classification 1  \n",
       "Precision                     0.809204                     0.757778  \n",
       "Recall                        0.700971                     0.775966  \n",
       "F1 Score                      0.703123                     0.731282  \n",
       "Acurácia                      0.775862                     0.793103  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumo_multi1 = pd.DataFrame({'dict1':pd.Series(dict1),\n",
    "                           'dict2':pd.Series(dict2),\n",
    "                           'dict3':pd.Series(dict3),\n",
    "                           'dict4':pd.Series(dict4),\n",
    "                           'dict5':pd.Series(dict5),\n",
    "                           'dict6':pd.Series(dict6)})\n",
    "\n",
    "resumo_multi1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993f845b",
   "metadata": {},
   "source": [
    "Para o modelo de classificação multiclasse, utilizaremos a métrica F1 Score, que é a média harmônica entre precision e recall para avaliar o modelo. Como agora não temos exatamente uma falha como mais ou menos grave que outra, um bom equilíbrio de falsos positivos e falsos negativos pode ser útil.\n",
    "\n",
    "Dentre os modelos utilizados para a classificação multiclasse, o Random Forest foi o que melhor respondeu. Vamos utilizar esse modelo como base e fazer um GridSearchCV para encontrar os melhores hiperparâmetros para ele e tentar melhorar a resposta final do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b7b8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "?modelo4_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9bb66e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV executou em 980.33 segundos para todas as combinações de candidatos a parâmetros do modelo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.14839993, 0.30980101, 0.45840034, 0.59878712, 0.14742217,\n",
       "        0.28804512, 0.42707171, 0.58675275, 0.14956779, 0.34985271,\n",
       "        0.47846341, 0.63070779, 0.15370054, 0.30017109, 0.43686743,\n",
       "        0.58290906, 0.14741554, 0.29339929, 0.43999958, 0.61764131,\n",
       "        0.20246139, 0.31959972, 0.46041231, 0.59693446, 0.15058608,\n",
       "        0.30004678, 0.44827743, 0.6014883 , 0.14860659, 0.29847598,\n",
       "        0.45386105, 0.5910965 , 0.15725698, 0.30522251, 0.45886798,\n",
       "        0.58508582, 0.15520277, 0.30019784, 0.44939842, 0.60263958,\n",
       "        0.20441699, 0.39515023, 0.4736032 , 0.60084233, 0.14922605,\n",
       "        0.32867012, 0.466645  , 0.64144568, 0.15203967, 0.31458797,\n",
       "        0.46012454, 0.57393064, 0.14418259, 0.28583927, 0.43527298,\n",
       "        0.56894784, 0.1455986 , 0.28580146, 0.42900648, 0.57368455,\n",
       "        0.14583297, 0.2894434 , 0.43226142, 0.58968   , 0.14082122,\n",
       "        0.29748917, 0.49363041, 0.63876162, 0.14778676, 0.30325623,\n",
       "        0.45299792, 0.59859743, 0.15019913, 0.30020714, 0.4494276 ,\n",
       "        0.61615067, 0.15080452, 0.30720205, 0.46724162, 0.57447357,\n",
       "        0.15603876, 0.30040126, 0.44840794, 0.61487641, 0.16441779,\n",
       "        0.31286006, 0.45491824, 0.57439117, 0.14222322, 0.29581027,\n",
       "        0.52524433, 0.64850311, 0.21606236, 0.30402803, 0.44007597,\n",
       "        0.58409567, 0.15819774, 0.33802805, 0.45607014, 0.59764929,\n",
       "        0.15261893, 0.30923033, 0.45910115, 0.62697167, 0.15275006,\n",
       "        0.3058754 , 0.46147294, 0.61632543, 0.15803356, 0.30764575,\n",
       "        0.46843233, 0.61169105, 0.15822401, 0.3977407 , 0.47179093,\n",
       "        0.71726322, 0.15157952, 0.30723701, 0.45766163, 0.6049099 ,\n",
       "        0.18242974, 0.32584958, 0.46426349, 0.62050438, 0.15342155,\n",
       "        0.31203837, 0.46647105, 0.61469908, 0.15676193, 0.3138845 ,\n",
       "        0.46367064, 0.60508518, 0.15722575, 0.30564098, 0.49957285,\n",
       "        0.66884017, 0.16740117, 0.35080576, 0.4939774 , 0.63049564,\n",
       "        0.15481691, 0.3187326 , 0.48502336, 0.68889079, 0.17427716,\n",
       "        0.31006145, 0.47042351, 0.68999462, 0.15540218, 0.31399174,\n",
       "        0.46007934, 0.63372011, 0.19900656, 0.35068254, 0.5660965 ,\n",
       "        0.63228993, 0.15042243, 0.30884337, 0.45926332, 0.61627784,\n",
       "        0.14701743, 0.30187025, 0.44744463, 0.68455811, 0.14962354,\n",
       "        0.31084146, 0.57371054, 0.67035284, 0.17968731, 0.31689286,\n",
       "        0.51460066, 0.58141146, 0.14981756, 0.28303361, 0.42606158,\n",
       "        0.56328826, 0.14162102, 0.28404789, 0.44185491, 0.60470934,\n",
       "        0.16142206, 0.33041959, 0.51033492, 0.65150194, 0.15142803,\n",
       "        0.30787559, 0.50805221, 0.64911489, 0.16002502, 0.37365427,\n",
       "        0.45847397, 0.61789627, 0.15662518, 0.30283499, 0.46102877,\n",
       "        0.60867333, 0.15302491, 0.38874793, 0.50297899, 0.67144527,\n",
       "        0.18782706, 0.35149193, 0.51746478, 0.67691674, 0.15201297,\n",
       "        0.33347588, 0.4866672 , 0.76101174, 0.15300021, 0.31617398,\n",
       "        0.46007214, 0.7081121 , 0.15441813, 0.33243279, 0.62518244,\n",
       "        0.68739991, 0.15079799, 0.32604432, 0.4945353 , 0.59734373,\n",
       "        0.16039982, 0.36540117, 0.54418859, 0.63049636, 0.16000347,\n",
       "        0.344873  , 0.47352099, 0.60092678, 0.14740033, 0.29280128,\n",
       "        0.4425261 , 0.58403611, 0.14694896, 0.29539838, 0.43980012,\n",
       "        0.59557791, 0.16820059, 0.32889018, 0.54453793, 0.72319922,\n",
       "        0.12302599, 0.262322  , 0.42740512, 0.49589963, 0.12294965,\n",
       "        0.22959738, 0.32879791, 0.48633838, 0.11900349, 0.24079714,\n",
       "        0.35419884, 0.43573484, 0.1154007 , 0.21759777, 0.35687428,\n",
       "        0.42801852, 0.11119757, 0.2081984 , 0.33101101, 0.47240434,\n",
       "        0.13180161, 0.25411911, 0.39639821, 0.47573295, 0.12219744,\n",
       "        0.23086605, 0.34020181, 0.46154819, 0.11919851, 0.23719769,\n",
       "        0.345261  , 0.4533978 , 0.11200075, 0.22299914, 0.33859611,\n",
       "        0.45514159, 0.11519971, 0.23199997, 0.34048562, 0.47372413,\n",
       "        0.11919842, 0.24200215, 0.39380016, 0.55870008, 0.12320008,\n",
       "        0.25400143, 0.44731045, 0.47175021, 0.11559987, 0.24620066,\n",
       "        0.34634962, 0.46199903, 0.12119389, 0.23547101, 0.35160012,\n",
       "        0.46035161, 0.117801  , 0.22999725, 0.34588118, 0.46512485,\n",
       "        0.11620154, 0.23060031, 0.36675849, 0.53460245, 0.12380409,\n",
       "        0.24083252, 0.42531824, 0.49894638, 0.11799893, 0.26940036,\n",
       "        0.35874047, 0.47780013, 0.11700101, 0.23559995, 0.37719646,\n",
       "        0.54975057, 0.11859913, 0.27360215, 0.47053113, 0.56423473,\n",
       "        0.13040051, 0.36559978, 0.47179718, 0.47541413, 0.12236924,\n",
       "        0.24236221, 0.41560097, 0.48661251, 0.13374748, 0.25419779,\n",
       "        0.38500023, 0.51871829, 0.12060032, 0.22560968, 0.32672286,\n",
       "        0.43851519, 0.11800084, 0.22499862, 0.33101392, 0.43591943,\n",
       "        0.12500124, 0.24583082, 0.35058708, 0.53900285, 0.14719853,\n",
       "        0.2463181 , 0.39339709, 0.54769273, 0.1275877 , 0.24444442,\n",
       "        0.40216823, 0.55346036, 0.13922644, 0.29539742, 0.3890811 ,\n",
       "        0.54165421, 0.15957017, 0.26206441, 0.41511874, 0.5034585 ,\n",
       "        0.12721553, 0.25444584, 0.3884584 , 0.50468097, 0.13138671,\n",
       "        0.25643425, 0.37685909, 0.50506992, 0.12401915, 0.24650707,\n",
       "        0.41465554, 0.59985976, 0.13502426, 0.25583138, 0.37826443,\n",
       "        0.49288468, 0.12584639, 0.25443902, 0.37817578, 0.50428081,\n",
       "        0.15681515, 0.30043578, 0.42438173, 0.52547507, 0.13102098,\n",
       "        0.31146002, 0.45039964, 0.54991198, 0.13365726, 0.26404076,\n",
       "        0.39958649, 0.59108663, 0.13783035, 0.26043429, 0.4473042 ,\n",
       "        0.6290791 , 0.15707874, 0.25033531, 0.39945269, 0.59622378,\n",
       "        0.12655363, 0.24611607, 0.33668318, 0.4582037 , 0.11381545,\n",
       "        0.2350276 , 0.36202106, 0.47152395, 0.12298579, 0.24245796,\n",
       "        0.36386909, 0.53030958, 0.11860299, 0.22179389, 0.33525295,\n",
       "        0.4474762 , 0.11201906, 0.22564144, 0.36890168, 0.4578692 ,\n",
       "        0.12741117, 0.2969336 , 0.37185068, 0.57346759, 0.15444388,\n",
       "        0.28443923, 0.39254956, 0.53001218, 0.12482157, 0.24541879,\n",
       "        0.36605673, 0.53611588, 0.13419151, 0.25184083, 0.36081271,\n",
       "        0.48447356, 0.1196146 , 0.24263129, 0.3604579 , 0.48203707,\n",
       "        0.129421  , 0.25084043, 0.37523532, 0.49527769, 0.12401752,\n",
       "        0.24224138, 0.36766748, 0.50687828, 0.12401562, 0.25103807,\n",
       "        0.37285395, 0.4873476 , 0.12460032, 0.25236077, 0.36603222,\n",
       "        0.48246102, 0.12502918, 0.24223323, 0.38952966, 0.52441301,\n",
       "        0.15106201, 0.2847815 , 0.37506051, 0.54507117, 0.14331756,\n",
       "        0.24379473, 0.37724123, 0.53725591, 0.1432322 , 0.26884398,\n",
       "        0.42586474, 0.64889688, 0.17179236, 0.26442342, 0.35346084,\n",
       "        0.48066502, 0.13037686, 0.31908107, 0.36081123, 0.49719925]),\n",
       " 'std_fit_time': array([0.01474676, 0.00421517, 0.02711232, 0.03860129, 0.00548922,\n",
       "        0.00920072, 0.00860342, 0.02939244, 0.00761195, 0.05790297,\n",
       "        0.05673737, 0.03765352, 0.01136158, 0.00535774, 0.00703469,\n",
       "        0.00978013, 0.00175067, 0.00574817, 0.00712835, 0.03582869,\n",
       "        0.0238207 , 0.01442782, 0.01266626, 0.00523765, 0.00450774,\n",
       "        0.00236126, 0.00263697, 0.01795803, 0.00163841, 0.00046774,\n",
       "        0.00958447, 0.00597034, 0.01239588, 0.00768779, 0.01176853,\n",
       "        0.00590166, 0.00696627, 0.00897158, 0.01710523, 0.00686894,\n",
       "        0.02810414, 0.04871898, 0.01597912, 0.01428191, 0.00171818,\n",
       "        0.03810639, 0.01950838, 0.03584236, 0.00557684, 0.00826754,\n",
       "        0.03988502, 0.01432179, 0.00290949, 0.00849847, 0.01032989,\n",
       "        0.00511986, 0.00458422, 0.0039711 , 0.00553462, 0.00741147,\n",
       "        0.00731056, 0.00273991, 0.00470146, 0.0119146 , 0.00223168,\n",
       "        0.01265937, 0.02136029, 0.02610019, 0.00544745, 0.00779513,\n",
       "        0.00900441, 0.01061309, 0.00391994, 0.00482628, 0.00502492,\n",
       "        0.03969851, 0.00430288, 0.01432046, 0.01558839, 0.00773253,\n",
       "        0.00688182, 0.00778588, 0.01652638, 0.0301328 , 0.00424336,\n",
       "        0.04063246, 0.02480609, 0.00908181, 0.00354797, 0.02264466,\n",
       "        0.12277855, 0.03946066, 0.03555691, 0.01573682, 0.02010407,\n",
       "        0.01757925, 0.01549051, 0.03622016, 0.00902853, 0.01249525,\n",
       "        0.00273992, 0.00685252, 0.00931437, 0.02557167, 0.0037612 ,\n",
       "        0.00136944, 0.00778248, 0.01109059, 0.00894104, 0.00539266,\n",
       "        0.01376638, 0.00594629, 0.00387223, 0.04360816, 0.03049178,\n",
       "        0.07614505, 0.00553056, 0.01039403, 0.00984314, 0.00740973,\n",
       "        0.02317309, 0.01269036, 0.00710552, 0.00547327, 0.00136169,\n",
       "        0.00389509, 0.00542093, 0.00628337, 0.00417828, 0.0085707 ,\n",
       "        0.00135678, 0.00189537, 0.0076822 , 0.00840465, 0.0507901 ,\n",
       "        0.03942255, 0.00611868, 0.02620753, 0.02491705, 0.03230815,\n",
       "        0.00783131, 0.01330658, 0.02779368, 0.0433491 , 0.00989419,\n",
       "        0.00657224, 0.01143736, 0.0595689 , 0.00300564, 0.00878324,\n",
       "        0.00467138, 0.02909586, 0.02644697, 0.05448934, 0.06425297,\n",
       "        0.06900506, 0.00491303, 0.01274653, 0.01122824, 0.03401737,\n",
       "        0.00731629, 0.0164043 , 0.0147654 , 0.07052202, 0.00388449,\n",
       "        0.02211216, 0.07024276, 0.0911257 , 0.02179294, 0.01318605,\n",
       "        0.05986224, 0.01646097, 0.0076194 , 0.00659766, 0.00683819,\n",
       "        0.01071881, 0.00224684, 0.00540057, 0.02516909, 0.04591623,\n",
       "        0.0096449 , 0.02263811, 0.02398028, 0.07631126, 0.00633523,\n",
       "        0.02009172, 0.01408611, 0.03438517, 0.01143365, 0.06537678,\n",
       "        0.00745688, 0.01930689, 0.00535111, 0.00887479, 0.00802945,\n",
       "        0.01009008, 0.00379438, 0.07504367, 0.0585038 , 0.07747785,\n",
       "        0.02557905, 0.04432726, 0.066242  , 0.04924451, 0.00850541,\n",
       "        0.06099242, 0.02470957, 0.10294109, 0.00414636, 0.0118716 ,\n",
       "        0.01360698, 0.1701853 , 0.00702974, 0.01096017, 0.1020286 ,\n",
       "        0.03070726, 0.00623965, 0.02367025, 0.04250437, 0.03855187,\n",
       "        0.00902424, 0.01717923, 0.08389096, 0.01132487, 0.00672243,\n",
       "        0.02251924, 0.02597201, 0.01408836, 0.00750146, 0.00570609,\n",
       "        0.00654182, 0.00453345, 0.00268775, 0.01036557, 0.00401996,\n",
       "        0.02382388, 0.01154835, 0.0199427 , 0.09786608, 0.07380484,\n",
       "        0.01000263, 0.02350415, 0.04861998, 0.01888186, 0.01766863,\n",
       "        0.00985283, 0.00685083, 0.03132935, 0.0152351 , 0.02941172,\n",
       "        0.0287169 , 0.0083745 , 0.00504416, 0.00431939, 0.02073178,\n",
       "        0.02168861, 0.00337186, 0.00462368, 0.01148503, 0.02875045,\n",
       "        0.01051418, 0.03170342, 0.05041004, 0.01826242, 0.00949695,\n",
       "        0.0058848 , 0.0049154 , 0.01892226, 0.00854196, 0.00815584,\n",
       "        0.0114966 , 0.00820509, 0.00297003, 0.00334798, 0.00488349,\n",
       "        0.00552611, 0.00172545, 0.00663567, 0.00658504, 0.03860444,\n",
       "        0.00376408, 0.0247796 , 0.0534117 , 0.04434953, 0.00466309,\n",
       "        0.01658231, 0.03610138, 0.01868473, 0.00338491, 0.01044054,\n",
       "        0.00783517, 0.01276096, 0.00466664, 0.00445769, 0.007365  ,\n",
       "        0.00925785, 0.00343079, 0.00178905, 0.01745336, 0.01076871,\n",
       "        0.00421136, 0.00445465, 0.01874688, 0.05589863, 0.00511751,\n",
       "        0.01020636, 0.04371407, 0.03066701, 0.00551464, 0.03097426,\n",
       "        0.01504766, 0.0117881 , 0.00303427, 0.00608832, 0.04937621,\n",
       "        0.06594294, 0.0051226 , 0.00997425, 0.10090964, 0.11221941,\n",
       "        0.00889052, 0.0383006 , 0.076405  , 0.04373439, 0.00957485,\n",
       "        0.02416088, 0.03781182, 0.03043313, 0.01349493, 0.03603964,\n",
       "        0.04128609, 0.0822674 , 0.01984926, 0.00840353, 0.00313101,\n",
       "        0.00818097, 0.01269776, 0.00807344, 0.00167836, 0.0064855 ,\n",
       "        0.0211025 , 0.02417388, 0.00489171, 0.04137685, 0.0279392 ,\n",
       "        0.00446111, 0.03955005, 0.02302688, 0.00587178, 0.01209954,\n",
       "        0.01494918, 0.03694817, 0.00568739, 0.04095854, 0.00968432,\n",
       "        0.01597598, 0.0236598 , 0.02479626, 0.02703966, 0.04939841,\n",
       "        0.00292444, 0.01047144, 0.02866961, 0.00356365, 0.00416495,\n",
       "        0.00366627, 0.01017121, 0.01616225, 0.00089853, 0.00332525,\n",
       "        0.02737063, 0.05859345, 0.01431866, 0.01468453, 0.00826878,\n",
       "        0.00412098, 0.00145344, 0.00811902, 0.00825448, 0.022725  ,\n",
       "        0.03122703, 0.02854793, 0.04508352, 0.01568091, 0.00979341,\n",
       "        0.03883659, 0.0587447 , 0.02939244, 0.00469573, 0.01601635,\n",
       "        0.01400778, 0.08267508, 0.01802954, 0.01944598, 0.07333005,\n",
       "        0.03512947, 0.01660299, 0.00769486, 0.0163095 , 0.08199559,\n",
       "        0.00858354, 0.01881594, 0.00258212, 0.00882112, 0.00396852,\n",
       "        0.01104253, 0.01505543, 0.02287963, 0.00972504, 0.01482439,\n",
       "        0.02678046, 0.08958224, 0.00418476, 0.00776724, 0.01506052,\n",
       "        0.00516063, 0.00141355, 0.00422985, 0.02326292, 0.00719189,\n",
       "        0.00940332, 0.06372894, 0.00510418, 0.05252175, 0.00584359,\n",
       "        0.03534412, 0.03642379, 0.03527445, 0.00270603, 0.00700935,\n",
       "        0.00664421, 0.08275969, 0.01371215, 0.01229093, 0.00164052,\n",
       "        0.00926942, 0.00185592, 0.0051588 , 0.00585999, 0.0052094 ,\n",
       "        0.00885649, 0.01424591, 0.00581759, 0.00742921, 0.00295863,\n",
       "        0.00494311, 0.00664567, 0.0314654 , 0.0035643 , 0.00993859,\n",
       "        0.01375097, 0.00537799, 0.00294098, 0.00953764, 0.00198819,\n",
       "        0.00458113, 0.00410741, 0.00214024, 0.02728048, 0.02722295,\n",
       "        0.01064773, 0.03589133, 0.01401758, 0.04183749, 0.01781658,\n",
       "        0.00965881, 0.03727146, 0.02170509, 0.00936257, 0.01170181,\n",
       "        0.0307546 , 0.11891818, 0.01803293, 0.02650792, 0.00376908,\n",
       "        0.0090235 , 0.00760566, 0.03770564, 0.0054773 , 0.01086792]),\n",
       " 'mean_score_time': array([0.01119871, 0.01940055, 0.02839975, 0.03699994, 0.01120052,\n",
       "        0.02020264, 0.0284019 , 0.0380024 , 0.01099968, 0.02160754,\n",
       "        0.02920485, 0.03740296, 0.01100202, 0.01940269, 0.02760663,\n",
       "        0.0379849 , 0.01160078, 0.02040281, 0.02840009, 0.03800111,\n",
       "        0.01580276, 0.02219806, 0.02942829, 0.03720179, 0.01139731,\n",
       "        0.02039723, 0.02860031, 0.0378026 , 0.01122985, 0.01999474,\n",
       "        0.02779984, 0.03860836, 0.01160097, 0.02000246, 0.02920556,\n",
       "        0.03720202, 0.01119847, 0.01980138, 0.02899985, 0.03781157,\n",
       "        0.01321621, 0.02301369, 0.02780294, 0.03701625, 0.01100698,\n",
       "        0.02100348, 0.02940068, 0.03999743, 0.0112133 , 0.02099876,\n",
       "        0.0292027 , 0.03720965, 0.01159945, 0.02000322, 0.0288033 ,\n",
       "        0.03800464, 0.01059942, 0.02000437, 0.02840652, 0.04980793,\n",
       "        0.01140018, 0.0199914 , 0.02940598, 0.03881044, 0.01080098,\n",
       "        0.02060075, 0.03079991, 0.03919735, 0.01100159, 0.01939793,\n",
       "        0.0281981 , 0.03620553, 0.01140127, 0.02020278, 0.02800102,\n",
       "        0.03660755, 0.01099973, 0.01980581, 0.0294024 , 0.03832269,\n",
       "        0.01099968, 0.01959867, 0.02820001, 0.03604536, 0.01160169,\n",
       "        0.01980066, 0.02899995, 0.03660622, 0.01060028, 0.02040453,\n",
       "        0.02967563, 0.0395997 , 0.01599865, 0.01979847, 0.02799854,\n",
       "        0.03700008, 0.01640015, 0.0212544 , 0.0282063 , 0.03844686,\n",
       "        0.01120634, 0.01962028, 0.02840538, 0.03719902, 0.01117368,\n",
       "        0.0192039 , 0.03020082, 0.03737922, 0.01119795, 0.02000661,\n",
       "        0.02900643, 0.03664083, 0.01160331, 0.01999898, 0.02880673,\n",
       "        0.04260674, 0.01099997, 0.02040014, 0.02903137, 0.03698573,\n",
       "        0.01160183, 0.02140255, 0.02820754, 0.03760147, 0.01140547,\n",
       "        0.0202157 , 0.02840319, 0.03700056, 0.01140304, 0.02019353,\n",
       "        0.02920599, 0.03700662, 0.0118    , 0.0192018 , 0.02899828,\n",
       "        0.03818297, 0.0115984 , 0.02160039, 0.03059616, 0.03740125,\n",
       "        0.01199851, 0.01980057, 0.02920098, 0.04139037, 0.01179857,\n",
       "        0.01939759, 0.02799931, 0.04342122, 0.0113996 , 0.02120218,\n",
       "        0.02820678, 0.03801413, 0.01320548, 0.02240396, 0.0312067 ,\n",
       "        0.03680086, 0.01199942, 0.02000437, 0.02940226, 0.03582716,\n",
       "        0.01180458, 0.02020259, 0.03000908, 0.04117422, 0.01140356,\n",
       "        0.01959968, 0.03580165, 0.03900332, 0.01219897, 0.0213974 ,\n",
       "        0.03859897, 0.03720293, 0.01120253, 0.01940236, 0.02820373,\n",
       "        0.03680892, 0.01100116, 0.0202044 , 0.02921495, 0.03679404,\n",
       "        0.01140871, 0.02019815, 0.04320054, 0.03781028, 0.01059928,\n",
       "        0.02020268, 0.02860823, 0.03860183, 0.0110013 , 0.02160139,\n",
       "        0.02839766, 0.03680668, 0.01079988, 0.02004905, 0.0287991 ,\n",
       "        0.03780465, 0.01139879, 0.02240191, 0.03460183, 0.04025602,\n",
       "        0.01179671, 0.02080274, 0.04059806, 0.0384027 , 0.01179223,\n",
       "        0.02039766, 0.0296041 , 0.04360042, 0.0111999 , 0.01999073,\n",
       "        0.02840347, 0.03940864, 0.01120486, 0.01979971, 0.03539877,\n",
       "        0.03880343, 0.01040015, 0.02140245, 0.02999864, 0.03739986,\n",
       "        0.01140089, 0.02759929, 0.030198  , 0.03759775, 0.01199861,\n",
       "        0.02260032, 0.02919827, 0.03680229, 0.01059904, 0.0199996 ,\n",
       "        0.02839909, 0.03720188, 0.01300564, 0.02000151, 0.02879944,\n",
       "        0.03940282, 0.0146029 , 0.02060137, 0.03540149, 0.04659843,\n",
       "        0.01119785, 0.02260237, 0.03279524, 0.03838944, 0.01080022,\n",
       "        0.01920204, 0.02880049, 0.0374289 , 0.01159787, 0.02020059,\n",
       "        0.02919998, 0.03740249, 0.01099892, 0.0196002 , 0.02840123,\n",
       "        0.03700299, 0.01060061, 0.02000079, 0.02791405, 0.03799467,\n",
       "        0.02000103, 0.02059932, 0.03540373, 0.03840084, 0.01120081,\n",
       "        0.01959963, 0.02840214, 0.03699989, 0.01120172, 0.02020044,\n",
       "        0.03159938, 0.03700604, 0.01039901, 0.01959777, 0.02880044,\n",
       "        0.03580284, 0.01119852, 0.02020135, 0.02899833, 0.03639903,\n",
       "        0.01100078, 0.02039833, 0.04971943, 0.041401  , 0.01220102,\n",
       "        0.02060146, 0.03180327, 0.03800373, 0.01159997, 0.02019901,\n",
       "        0.02820039, 0.03780217, 0.01060419, 0.02060194, 0.02960248,\n",
       "        0.03820028, 0.0119988 , 0.02020044, 0.02899942, 0.03719888,\n",
       "        0.01080084, 0.0202002 , 0.02960348, 0.03919616, 0.01160102,\n",
       "        0.01979876, 0.03500228, 0.04420176, 0.0105998 , 0.01980047,\n",
       "        0.02820096, 0.03919988, 0.0107995 , 0.01954489, 0.03039966,\n",
       "        0.04040356, 0.01140008, 0.02200036, 0.03439865, 0.04200025,\n",
       "        0.01160107, 0.03099675, 0.03480272, 0.038202  , 0.01100416,\n",
       "        0.02020211, 0.02920227, 0.03820348, 0.01160197, 0.02040248,\n",
       "        0.03220367, 0.03959799, 0.01280031, 0.01920004, 0.02859941,\n",
       "        0.03639855, 0.01100588, 0.02000012, 0.02920375, 0.0382009 ,\n",
       "        0.01179929, 0.0197978 , 0.02760115, 0.04319878, 0.01140099,\n",
       "        0.01959877, 0.03260159, 0.04434643, 0.01159821, 0.0200007 ,\n",
       "        0.03140249, 0.03899865, 0.01239996, 0.02080021, 0.03143559,\n",
       "        0.03980689, 0.0119987 , 0.02062726, 0.04220185, 0.03820415,\n",
       "        0.01160483, 0.02099967, 0.02900314, 0.03740854, 0.01100063,\n",
       "        0.02020178, 0.02860513, 0.03760386, 0.01120138, 0.02000179,\n",
       "        0.0301847 , 0.03941164, 0.01139956, 0.0196054 , 0.02980371,\n",
       "        0.03800063, 0.01119823, 0.02038431, 0.0298461 , 0.0388    ,\n",
       "        0.01400146, 0.02368345, 0.02840257, 0.03841238, 0.01100082,\n",
       "        0.02279968, 0.03140082, 0.04200034, 0.01139917, 0.02080045,\n",
       "        0.0298429 , 0.04299045, 0.01340127, 0.02020512, 0.03762064,\n",
       "        0.04740052, 0.01380038, 0.02023606, 0.02879858, 0.04380317,\n",
       "        0.01119981, 0.0191999 , 0.02839818, 0.03620338, 0.01040626,\n",
       "        0.01960344, 0.02920465, 0.0369812 , 0.01099973, 0.02000508,\n",
       "        0.02799883, 0.04140072, 0.01080079, 0.01957641, 0.02960501,\n",
       "        0.03579969, 0.01160049, 0.02019315, 0.02916479, 0.03840189,\n",
       "        0.01180086, 0.02191358, 0.02980576, 0.04738412, 0.01199503,\n",
       "        0.02439151, 0.0287919 , 0.03780541, 0.01120057, 0.02001543,\n",
       "        0.02840428, 0.03920746, 0.01159892, 0.019803  , 0.02860117,\n",
       "        0.03780766, 0.0118073 , 0.01960378, 0.02839909, 0.03701429,\n",
       "        0.01100187, 0.0206037 , 0.03000474, 0.03878007, 0.01080241,\n",
       "        0.01960034, 0.02779584, 0.03780398, 0.01100917, 0.02100697,\n",
       "        0.02940431, 0.03819909, 0.0106009 , 0.0194016 , 0.02782822,\n",
       "        0.03781672, 0.01119819, 0.01940012, 0.02999716, 0.03860803,\n",
       "        0.01340222, 0.02080321, 0.02880487, 0.03939905, 0.01360087,\n",
       "        0.02040091, 0.03160453, 0.03783374, 0.01239991, 0.0195981 ,\n",
       "        0.03460107, 0.04419556, 0.01244154, 0.02020631, 0.02919879,\n",
       "        0.03800211, 0.01500249, 0.02179751, 0.0278245 , 0.03799949]),\n",
       " 'std_score_time': array([4.03103773e-04, 4.87107616e-04, 1.20189909e-03, 1.41556777e-03,\n",
       "        9.79705842e-04, 9.73656489e-04, 1.85143134e-03, 1.89758193e-03,\n",
       "        8.96550107e-04, 2.85918074e-03, 3.97547664e-04, 2.15787494e-03,\n",
       "        6.32719237e-04, 7.99804502e-04, 7.98991743e-04, 1.65488158e-03,\n",
       "        1.19890829e-03, 1.01706974e-03, 1.35643959e-03, 2.52819001e-03,\n",
       "        3.86628494e-03, 1.60168573e-03, 1.87665718e-03, 1.16485623e-03,\n",
       "        1.02144763e-03, 4.90532154e-04, 1.20174696e-03, 1.59621780e-03,\n",
       "        7.41005720e-04, 1.25396503e-03, 7.50116291e-04, 1.36018481e-03,\n",
       "        8.05146298e-04, 1.55454672e-03, 1.17044151e-03, 9.85536788e-04,\n",
       "        7.49746766e-04, 1.16757235e-03, 6.32637298e-04, 2.70315740e-03,\n",
       "        3.93729798e-04, 2.17229417e-03, 7.51364235e-04, 1.54366280e-03,\n",
       "        1.52174569e-05, 2.28432602e-03, 1.35822167e-03, 3.74190888e-03,\n",
       "        4.01199348e-04, 1.41532628e-03, 1.72629706e-03, 1.59719662e-03,\n",
       "        1.20354673e-03, 1.78691584e-03, 1.47169804e-03, 2.53385135e-03,\n",
       "        4.89630326e-04, 1.26587620e-03, 1.20412792e-03, 2.76245945e-02,\n",
       "        1.20109087e-03, 1.66598699e-03, 1.01548333e-03, 1.16565719e-03,\n",
       "        3.98007712e-04, 1.35777721e-03, 1.93905480e-03, 2.04135053e-03,\n",
       "        4.09636146e-06, 8.02545785e-04, 3.98127692e-04, 9.78820524e-04,\n",
       "        8.00027596e-04, 1.16497260e-03, 9.00768027e-04, 7.98568279e-04,\n",
       "        6.28195005e-04, 1.17533546e-03, 2.05988126e-03, 1.73659227e-03,\n",
       "        6.30607199e-04, 1.20082031e-03, 3.97531552e-04, 1.83136824e-03,\n",
       "        1.01586352e-03, 1.16458879e-03, 8.91596114e-04, 2.33704784e-03,\n",
       "        7.99871150e-04, 7.98703783e-04, 1.87987553e-03, 5.81697238e-03,\n",
       "        8.04988507e-03, 9.80291189e-04, 1.54929704e-03, 2.28177561e-03,\n",
       "        1.08572482e-02, 3.88954129e-03, 1.17159595e-03, 1.84726590e-03,\n",
       "        1.17493064e-03, 8.38899405e-04, 1.02266313e-03, 1.47101241e-03,\n",
       "        4.15070502e-04, 3.98287181e-04, 2.22804623e-03, 8.21569687e-04,\n",
       "        9.81103775e-04, 8.83505582e-04, 2.61958356e-03, 1.42360069e-03,\n",
       "        4.83329760e-04, 1.09672231e-03, 1.16796573e-03, 6.27696940e-03,\n",
       "        4.51965165e-06, 1.19913640e-03, 1.41651687e-03, 6.69367837e-04,\n",
       "        4.90686464e-04, 1.85201591e-03, 9.81370410e-04, 1.50010872e-03,\n",
       "        8.01139401e-04, 9.71434941e-04, 1.34942389e-03, 1.26655596e-03,\n",
       "        1.02584940e-03, 1.14952194e-03, 1.47024117e-03, 1.79667316e-03,\n",
       "        1.16968871e-03, 9.77469297e-04, 1.78545101e-03, 2.10281723e-03,\n",
       "        4.91392072e-04, 2.80006001e-03, 2.41784422e-03, 1.35611542e-03,\n",
       "        2.52947039e-03, 9.77993576e-04, 1.47180496e-03, 4.22910051e-03,\n",
       "        1.16916248e-03, 4.91951202e-04, 8.95377533e-04, 1.13198154e-02,\n",
       "        4.92092542e-04, 2.13495845e-03, 3.97321179e-04, 2.60019082e-03,\n",
       "        2.03551600e-03, 3.55064554e-03, 2.99731621e-03, 1.46935734e-03,\n",
       "        1.26640482e-03, 6.43477277e-04, 2.33276935e-03, 4.15461925e-04,\n",
       "        1.17155064e-03, 1.93721723e-03, 8.95854648e-04, 5.44950629e-03,\n",
       "        4.89503827e-04, 2.24417753e-03, 9.94805312e-03, 2.45046120e-03,\n",
       "        9.79370433e-04, 3.37857734e-03, 1.50287534e-02, 1.60205097e-03,\n",
       "        7.53135881e-04, 4.86242884e-04, 7.49734508e-04, 9.84760266e-04,\n",
       "        8.92554632e-04, 2.04280617e-03, 1.82804075e-03, 9.77906319e-04,\n",
       "        1.18937367e-03, 7.50936101e-04, 2.79013727e-02, 1.31888382e-03,\n",
       "        4.89221181e-04, 1.60220111e-03, 1.19530933e-03, 1.02107778e-03,\n",
       "        6.38388982e-04, 2.05747413e-03, 1.95717240e-03, 9.83326144e-04,\n",
       "        4.00185851e-04, 1.30477479e-03, 9.82163077e-04, 9.74658690e-04,\n",
       "        1.01698692e-03, 2.86781475e-03, 1.32435508e-02, 4.75739975e-03,\n",
       "        1.71878450e-03, 1.46993931e-03, 1.23700505e-02, 7.99945385e-04,\n",
       "        9.72697610e-04, 1.01862079e-03, 1.35076037e-03, 7.33503504e-03,\n",
       "        9.79582816e-04, 8.83306226e-04, 1.35109476e-03, 4.36032755e-03,\n",
       "        7.46108057e-04, 1.83341364e-03, 7.00445216e-03, 4.62070821e-03,\n",
       "        7.99965949e-04, 1.02082918e-03, 6.35205598e-04, 1.20036632e-03,\n",
       "        1.02013280e-03, 7.91518701e-03, 2.70987471e-03, 8.07572974e-04,\n",
       "        1.26519865e-03, 3.38225112e-03, 1.32761144e-03, 1.16585510e-03,\n",
       "        4.89317704e-04, 6.32409775e-04, 1.49657054e-03, 1.32856364e-03,\n",
       "        5.52070975e-03, 1.41465216e-03, 1.60061146e-03, 3.44224263e-03,\n",
       "        5.74699906e-03, 1.02034005e-03, 1.38334035e-02, 1.67376623e-02,\n",
       "        4.02439132e-04, 2.05714466e-03, 4.26077940e-03, 2.22891315e-03,\n",
       "        7.49952856e-04, 9.80102367e-04, 1.60000254e-03, 5.29790083e-04,\n",
       "        1.01863012e-03, 1.16580024e-03, 1.71790731e-03, 1.02111805e-03,\n",
       "        6.32869127e-04, 1.85542747e-03, 8.03194021e-04, 1.89356050e-03,\n",
       "        4.89921543e-04, 2.09948952e-03, 1.29236171e-03, 6.32404971e-04,\n",
       "        1.46185498e-02, 1.20086651e-03, 5.15976728e-03, 2.15992487e-03,\n",
       "        4.00481208e-04, 1.20046467e-03, 1.19919181e-03, 1.41822725e-03,\n",
       "        7.46587558e-04, 1.59917332e-03, 6.37969531e-03, 1.40795313e-03,\n",
       "        4.88907898e-04, 8.00973524e-04, 1.16412893e-03, 7.50320155e-04,\n",
       "        7.49672941e-04, 1.16431748e-03, 1.41293460e-03, 1.35745939e-03,\n",
       "        6.30907073e-04, 1.49797693e-03, 3.96582630e-02, 5.46599927e-03,\n",
       "        7.48916324e-04, 1.02047193e-03, 2.03774002e-03, 1.99857104e-03,\n",
       "        8.00887341e-04, 7.47449803e-04, 1.16303555e-03, 1.94126283e-03,\n",
       "        4.91873359e-04, 1.35660985e-03, 1.35882231e-03, 1.72003550e-03,\n",
       "        8.93678295e-04, 1.16594265e-03, 1.09576088e-03, 1.16752182e-03,\n",
       "        7.44193063e-04, 1.47065896e-03, 1.20303432e-03, 2.03991482e-03,\n",
       "        4.86036034e-04, 1.16528039e-03, 9.18521816e-03, 1.14788875e-02,\n",
       "        8.02614075e-04, 9.81197304e-04, 1.46914774e-03, 2.31511647e-03,\n",
       "        3.98580779e-04, 1.23157491e-03, 2.41664089e-03, 3.92556990e-03,\n",
       "        4.91178442e-04, 2.75726883e-03, 4.22372323e-03, 6.44675917e-03,\n",
       "        1.01908441e-03, 6.03230696e-03, 5.34713512e-03, 4.11784350e-03,\n",
       "        1.09193957e-03, 9.81483021e-04, 1.60065205e-03, 2.92779974e-03,\n",
       "        4.89632137e-04, 1.19848585e-03, 5.19368737e-03, 4.63253713e-03,\n",
       "        3.25003198e-03, 7.48610730e-04, 1.01975177e-03, 1.74201668e-03,\n",
       "        8.93950047e-04, 1.78912561e-03, 2.22958940e-03, 1.72033204e-03,\n",
       "        7.49272624e-04, 7.49228403e-04, 7.99252898e-04, 8.68084622e-03,\n",
       "        7.99342573e-04, 1.20351500e-03, 3.93022305e-03, 5.61015445e-03,\n",
       "        1.01908519e-03, 1.55102217e-03, 2.41347559e-03, 2.27506599e-03,\n",
       "        1.49737709e-03, 1.83269565e-03, 3.95764306e-03, 4.29633598e-03,\n",
       "        1.26482191e-03, 1.00445527e-03, 9.14903546e-03, 9.83355392e-04,\n",
       "        1.73889703e-03, 1.09332410e-03, 6.32274564e-04, 1.19581848e-03,\n",
       "        4.18151117e-06, 7.48368000e-04, 1.35237844e-03, 1.36029323e-03,\n",
       "        1.16601743e-03, 1.26572827e-03, 9.22893867e-04, 1.48275502e-03,\n",
       "        8.00049775e-04, 4.93306765e-04, 1.71798211e-03, 1.09558460e-03,\n",
       "        7.46238809e-04, 5.04677406e-04, 7.64886407e-04, 2.13842065e-03,\n",
       "        4.14796516e-03, 3.64821196e-03, 1.20012972e-03, 2.25368601e-03,\n",
       "        1.09649848e-03, 3.65448713e-03, 2.41785505e-03, 5.09837923e-03,\n",
       "        4.90644063e-04, 1.59974232e-03, 9.50014524e-04, 6.88188012e-03,\n",
       "        3.38680248e-03, 1.16895199e-03, 9.77700619e-03, 7.39157046e-03,\n",
       "        2.78614003e-03, 1.60354426e-03, 7.43936908e-04, 1.31484883e-02,\n",
       "        9.80330818e-04, 9.81025136e-04, 1.74409012e-03, 7.47601243e-04,\n",
       "        4.86045497e-04, 1.03005972e-03, 7.50420232e-04, 1.24407805e-03,\n",
       "        6.33474251e-04, 1.55122790e-03, 6.32579219e-04, 2.57890622e-03,\n",
       "        4.00524917e-04, 7.54808936e-04, 1.01985544e-03, 9.76705673e-04,\n",
       "        1.02315502e-03, 9.74863179e-04, 1.79519320e-03, 3.25928139e-03,\n",
       "        1.46936809e-03, 4.30800983e-03, 1.84033779e-03, 1.18733812e-02,\n",
       "        1.26237138e-03, 6.31172635e-03, 9.74528142e-04, 1.16865357e-03,\n",
       "        1.46758338e-03, 1.25350984e-03, 8.02995012e-04, 2.62955852e-03,\n",
       "        1.35901259e-03, 9.82917464e-04, 1.35873309e-03, 1.33342989e-03,\n",
       "        7.43614616e-04, 7.96955211e-04, 1.49551892e-03, 1.27205712e-03,\n",
       "        4.71176885e-06, 1.01714424e-03, 1.89875504e-03, 1.13885487e-03,\n",
       "        7.47485762e-04, 1.20023361e-03, 9.80138905e-04, 2.13287500e-03,\n",
       "        6.21591382e-04, 2.29668183e-03, 2.93402617e-03, 2.22735581e-03,\n",
       "        4.89181372e-04, 1.01753244e-03, 1.58648579e-03, 2.12460937e-03,\n",
       "        3.98210779e-04, 8.03240805e-04, 8.93568565e-04, 1.01935320e-03,\n",
       "        2.50051812e-03, 1.32759378e-03, 1.32621319e-03, 1.19801194e-03,\n",
       "        1.95968574e-03, 2.33286777e-03, 7.70933931e-03, 1.65335636e-03,\n",
       "        1.35588308e-03, 4.85222370e-04, 9.35315962e-03, 8.08564974e-03,\n",
       "        1.33987035e-03, 1.59571205e-03, 1.16791581e-03, 6.31976980e-04,\n",
       "        7.50802025e-03, 2.13703778e-03, 1.15110908e-03, 1.26625474e-03]),\n",
       " 'param_bootstrap': masked_array(data=[True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,\n",
       "                    5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4,\n",
       "                    4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2,\n",
       "                    2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1,\n",
       "                    2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1,\n",
       "                    1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5,\n",
       "                    1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,\n",
       "                    5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4,\n",
       "                    4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2,\n",
       "                    2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1,\n",
       "                    2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1,\n",
       "                    1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5,\n",
       "                    1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5,\n",
       "                    5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4,\n",
       "                    4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 1, 1, 1, 1, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400, 100, 200, 300, 400,\n",
       "                    100, 200, 300, 400, 100, 200, 300, 400, 100, 200, 300,\n",
       "                    400, 100, 200, 300, 400, 100, 200, 300, 400, 100, 200,\n",
       "                    300, 400, 100, 200, 300, 400, 100, 200, 300, 400, 100,\n",
       "                    200, 300, 400, 100, 200, 300, 400],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 4,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 100},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 300},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 400}],\n",
       " 'split0_test_score': array([0.65863636, 0.62167558, 0.65863636, 0.65863636, 0.80724556,\n",
       "        0.65863636, 0.65863636, 0.65863636, 0.83077498, 0.65863636,\n",
       "        0.65863636, 0.63652174, 0.62596737, 0.65863636, 0.65863636,\n",
       "        0.65863636, 0.65863636, 0.62909091, 0.63652174, 0.65863636,\n",
       "        0.62909091, 0.6529972 , 0.70181818, 0.83077498, 0.83077498,\n",
       "        0.7397619 , 0.7397619 , 0.67966387, 0.67966387, 0.70181818,\n",
       "        0.67966387, 0.7997619 , 0.62521645, 0.65188312, 0.77077498,\n",
       "        0.83077498, 0.67966387, 0.65188312, 0.83077498, 0.83077498,\n",
       "        0.73934641, 0.6529972 , 0.73934641, 0.61378788, 0.67966387,\n",
       "        0.78601307, 0.70833333, 0.7397619 , 0.77809524, 0.69142857,\n",
       "        0.7397619 , 0.7397619 , 0.79292929, 0.7997619 , 0.65188312,\n",
       "        0.67515152, 0.72833333, 0.65188312, 0.67515152, 0.7997619 ,\n",
       "        0.76253968, 0.70833333, 0.6647619 , 0.77077498, 0.73142857,\n",
       "        0.73934641, 0.77077498, 0.73934641, 0.76253968, 0.73142857,\n",
       "        0.76253968, 0.7997619 , 0.6529972 , 0.79292929, 0.67966387,\n",
       "        0.67515152, 0.76181818, 0.7397619 , 0.76181818, 0.76181818,\n",
       "        0.67681818, 0.70603175, 0.65863636, 0.65613445, 0.65391304,\n",
       "        0.67777778, 0.65863636, 0.65613445, 0.82888889, 0.67777778,\n",
       "        0.65863636, 0.67777778, 0.62222473, 0.65863636, 0.65613445,\n",
       "        0.67777778, 0.70408213, 0.70603175, 0.65613445, 0.68430184,\n",
       "        0.67966387, 0.6529972 , 0.77077498, 0.82888889, 0.83077498,\n",
       "        0.6529972 , 0.83077498, 0.65188312, 0.82888889, 0.82888889,\n",
       "        0.83077498, 0.82888889, 0.82888889, 0.80974747, 0.82888889,\n",
       "        0.82888889, 0.73803922, 0.7397619 , 0.82888889, 0.65111111,\n",
       "        0.75312693, 0.78601307, 0.77077498, 0.78601307, 0.75312693,\n",
       "        0.83077498, 0.80920635, 0.79292929, 0.78601307, 0.7397619 ,\n",
       "        0.77077498, 0.83077498, 0.64426407, 0.6529972 , 0.83077498,\n",
       "        0.76888889, 0.7397619 , 0.67777778, 0.65111111, 0.7997619 ,\n",
       "        0.83169082, 0.7397619 , 0.77077498, 0.61378788, 0.7997619 ,\n",
       "        0.66619883, 0.76253968, 0.79292929, 0.66354978, 0.77077498,\n",
       "        0.77077498, 0.82888889, 0.67515152, 0.6529972 , 0.67966387,\n",
       "        0.67777778, 0.6529972 , 0.67515152, 0.77077498, 0.6529972 ,\n",
       "        0.70603175, 0.67966387, 0.65613445, 0.68430184, 0.65613445,\n",
       "        0.63652174, 0.65863636, 0.67777778, 0.82888889, 0.68430184,\n",
       "        0.67777778, 0.67777778, 0.67966387, 0.63652174, 0.67777778,\n",
       "        0.65863636, 0.67777778, 0.72908213, 0.65613445, 0.82888889,\n",
       "        0.77077498, 0.73730994, 0.67966387, 0.65188312, 0.69871148,\n",
       "        0.82888889, 0.82888889, 0.83077498, 0.82888889, 0.77077498,\n",
       "        0.76888889, 0.82888889, 0.7898226 , 0.85292929, 0.83077498,\n",
       "        0.77077498, 0.69871148, 0.67777778, 0.83077498, 0.83077498,\n",
       "        0.77077498, 0.73934641, 0.85292929, 0.83077498, 0.64156863,\n",
       "        0.77077498, 0.77077498, 0.73934641, 0.76515873, 0.83077498,\n",
       "        0.77077498, 0.77077498, 0.62521645, 0.77077498, 0.67966387,\n",
       "        0.76888889, 0.62521645, 0.82888889, 0.82888889, 0.83077498,\n",
       "        0.77077498, 0.77077498, 0.83077498, 0.73934641, 0.83077498,\n",
       "        0.67966387, 0.64156863, 0.78601307, 0.79292929, 0.7997619 ,\n",
       "        0.83077498, 0.7997619 , 0.67515152, 0.67966387, 0.6529972 ,\n",
       "        0.77077498, 0.67515152, 0.67393939, 0.77077498, 0.67777778,\n",
       "        0.6810101 , 0.65613445, 0.65863636, 0.80974747, 0.80974747,\n",
       "        0.80724556, 0.67777778, 0.65863636, 0.66186869, 0.82888889,\n",
       "        0.67777778, 0.65863636, 0.83077498, 0.65863636, 0.68289619,\n",
       "        0.80724556, 0.70603175, 0.80974747, 0.65613445, 0.65863636,\n",
       "        0.78601307, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.77077498, 0.73934641, 0.83077498,\n",
       "        0.73934641, 0.73934641, 0.74974747, 0.83077498, 0.77077498,\n",
       "        0.77077498, 0.77077498, 0.77077498, 0.77077498, 0.77077498,\n",
       "        0.73934641, 0.70833333, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.71849206, 0.73934641, 0.73934641, 0.755     , 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.70833333, 0.73934641, 0.73934641,\n",
       "        0.78601307, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.70833333, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.70603175, 0.67777778, 0.65613445, 0.70603175, 0.70783125,\n",
       "        0.65613445, 0.68430184, 0.67777778, 0.68430184, 0.70603175,\n",
       "        0.70603175, 0.67777778, 0.67777778, 0.67777778, 0.70603175,\n",
       "        0.67777778, 0.70603175, 0.70603175, 0.67777778, 0.67777778,\n",
       "        0.82888889, 0.77077498, 0.73730994, 0.77077498, 0.78397661,\n",
       "        0.75934641, 0.77077498, 0.73730994, 0.78601307, 0.73934641,\n",
       "        0.77077498, 0.76888889, 0.73934641, 0.77077498, 0.76888889,\n",
       "        0.76888889, 0.83077498, 0.77077498, 0.82888889, 0.82888889,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.76253968, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.78601307, 0.76253968, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.76515873,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.78601307, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78397661, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.70603175, 0.67777778, 0.68430184, 0.67777778, 0.70603175,\n",
       "        0.65613445, 0.67777778, 0.67777778, 0.65613445, 0.67777778,\n",
       "        0.70603175, 0.67777778, 0.67777778, 0.70603175, 0.70603175,\n",
       "        0.67777778, 0.67777778, 0.70603175, 0.70408213, 0.67777778,\n",
       "        0.82888889, 0.77077498, 0.80920635, 0.76888889, 0.78397661,\n",
       "        0.82888889, 0.83077498, 0.83077498, 0.77077498, 0.73730994,\n",
       "        0.73934641, 0.77077498, 0.76888889, 0.83077498, 0.77077498,\n",
       "        0.83077498, 0.83077498, 0.83077498, 0.77077498, 0.77077498,\n",
       "        0.71849206, 0.73934641, 0.73934641, 0.73934641, 0.66823529,\n",
       "        0.73934641, 0.73730994, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73730994, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73730994, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.74141414, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.76253968, 0.73934641,\n",
       "        0.73934641, 0.83077498, 0.73934641, 0.73730994, 0.73934641]),\n",
       " 'split1_test_score': array([0.646     , 0.646     , 0.646     , 0.62208791, 0.646     ,\n",
       "        0.646     , 0.66833333, 0.646     , 0.65377778, 0.66833333,\n",
       "        0.66833333, 0.61431169, 0.60502646, 0.646     , 0.646     ,\n",
       "        0.646     , 0.62208791, 0.62208791, 0.646     , 0.646     ,\n",
       "        0.66833333, 0.66797101, 0.63878788, 0.66384615, 0.64285714,\n",
       "        0.66384615, 0.63878788, 0.63878788, 0.70601023, 0.63800764,\n",
       "        0.69166667, 0.66797101, 0.61574359, 0.63878788, 0.69812253,\n",
       "        0.66797101, 0.72777778, 0.66175889, 0.66797101, 0.63878788,\n",
       "        0.61899767, 0.66797101, 0.69166667, 0.66384615, 0.64285714,\n",
       "        0.66384615, 0.66797101, 0.66797101, 0.66797101, 0.66797101,\n",
       "        0.64285714, 0.66797101, 0.66797101, 0.66797101, 0.63878788,\n",
       "        0.69166667, 0.67515152, 0.84642857, 0.69166667, 0.64285714,\n",
       "        0.66797101, 0.66797101, 0.64285714, 0.66797101, 0.64285714,\n",
       "        0.61025641, 0.69166667, 0.66797101, 0.64285714, 0.66797101,\n",
       "        0.84642857, 0.64285714, 0.66175889, 0.69812253, 0.66797101,\n",
       "        0.66797101, 0.63800764, 0.69166667, 0.66384615, 0.63800764,\n",
       "        0.82309524, 0.66833333, 0.646     , 0.66833333, 0.64197861,\n",
       "        0.62508951, 0.61431169, 0.66833333, 0.68953964, 0.62208791,\n",
       "        0.66833333, 0.66833333, 0.62208791, 0.63878788, 0.646     ,\n",
       "        0.66833333, 0.63878788, 0.66833333, 0.646     , 0.646     ,\n",
       "        0.68953964, 0.66175889, 0.66175889, 0.63463768, 0.69812253,\n",
       "        0.69812253, 0.69166667, 0.63463768, 0.8165208 , 0.82309524,\n",
       "        0.68953964, 0.60952381, 0.72005348, 0.63463768, 0.72005348,\n",
       "        0.68953964, 0.87481538, 0.66175889, 0.66833333, 0.69166667,\n",
       "        0.66833333, 0.84642857, 0.69812253, 0.63800764, 0.63463768,\n",
       "        0.63800764, 0.84642857, 0.63800764, 0.66797101, 0.64130435,\n",
       "        0.84642857, 0.66797101, 0.64333333, 0.66797101, 0.66797101,\n",
       "        0.60952381, 0.66833333, 0.66175889, 0.69166667, 0.58241026,\n",
       "        0.85128597, 0.8165208 , 0.61025641, 0.63800764, 0.72005348,\n",
       "        0.66175889, 0.8165208 , 0.81860806, 0.63463768, 0.66384615,\n",
       "        0.66384615, 0.84430155, 0.63463768, 0.69166667, 0.66833333,\n",
       "        0.66175889, 0.66384615, 0.63463768, 0.69812253, 0.69166667,\n",
       "        0.66833333, 0.66175889, 0.62208791, 0.646     , 0.62208791,\n",
       "        0.646     , 0.646     , 0.59603989, 0.64861893, 0.66833333,\n",
       "        0.65377778, 0.66833333, 0.67581699, 0.646     , 0.66833333,\n",
       "        0.62208791, 0.61933333, 0.62208791, 0.646     , 0.66833333,\n",
       "        0.64197861, 0.60952381, 0.79354978, 0.68953964, 0.66797101,\n",
       "        0.66833333, 0.68953964, 0.66175889, 0.66384615, 0.60952381,\n",
       "        0.68953964, 0.63463768, 0.64333333, 0.63878788, 0.68953964,\n",
       "        0.68953964, 0.60952381, 0.66797101, 0.69166667, 0.66175889,\n",
       "        0.66797101, 0.63878788, 0.66833333, 0.66384615, 0.69812253,\n",
       "        0.69812253, 0.63800764, 0.79276955, 0.8165208 , 0.66797101,\n",
       "        0.69166667, 0.66797101, 0.69166667, 0.69652406, 0.66175889,\n",
       "        0.72005348, 0.69812253, 0.81860806, 0.69166667, 0.63800764,\n",
       "        0.63878788, 0.66797101, 0.66175889, 0.64285714, 0.58574026,\n",
       "        0.63800764, 0.66797101, 0.66797101, 0.66384615, 0.61025641,\n",
       "        0.69812253, 0.66175889, 0.69652406, 0.68953964, 0.66175889,\n",
       "        0.82273292, 0.82661064, 0.82273292, 0.66797101, 0.60952381,\n",
       "        0.59545455, 0.61431169, 0.61431169, 0.63878788, 0.56878788,\n",
       "        0.59545455, 0.63878788, 0.63878788, 0.56428571, 0.61431169,\n",
       "        0.61431169, 0.59545455, 0.61933333, 0.63878788, 0.64333333,\n",
       "        0.63878788, 0.61933333, 0.59545455, 0.61933333, 0.59545455,\n",
       "        0.66384615, 0.63463768, 0.63463768, 0.66175889, 0.67184874,\n",
       "        0.78666667, 0.63463768, 0.63463768, 0.84642857, 0.69166667,\n",
       "        0.63463768, 0.66175889, 0.8165208 , 0.63463768, 0.63571429,\n",
       "        0.63463768, 0.66384615, 0.57948718, 0.78939959, 0.81860806,\n",
       "        0.64130435, 0.81860806, 0.81860806, 0.81860806, 0.84015152,\n",
       "        0.81860806, 0.87225673, 0.66384615, 0.89809524, 0.81860806,\n",
       "        0.81860806, 0.89809524, 0.84642857, 0.66384615, 0.66666667,\n",
       "        0.84642857, 0.66384615, 0.81860806, 0.89809524, 0.81860806,\n",
       "        0.84642857, 0.81860806, 0.81860806, 0.66384615, 0.63800764,\n",
       "        0.66384615, 0.66384615, 0.66384615, 0.69166667, 0.66384615,\n",
       "        0.64432234, 0.81860806, 0.79276955, 0.66384615, 0.81860806,\n",
       "        0.81860806, 0.87225673, 0.66384615, 0.81860806, 0.66384615,\n",
       "        0.82742857, 0.646     , 0.8007619 , 0.646     , 0.82309524,\n",
       "        0.77684982, 0.8007619 , 0.646     , 0.646     , 0.82309524,\n",
       "        0.646     , 0.66833333, 0.66833333, 0.646     , 0.8007619 ,\n",
       "        0.646     , 0.646     , 0.77684982, 0.8007619 , 0.8007619 ,\n",
       "        0.82309524, 0.85288443, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.8007619 , 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.63878788, 0.8007619 , 0.82309524,\n",
       "        0.82309524, 0.8007619 , 0.66833333, 0.82309524, 0.82309524,\n",
       "        0.79354978, 0.82309524, 0.82309524, 0.82309524, 0.79354978,\n",
       "        0.85288443, 0.85288443, 0.82309524, 0.79354978, 0.66833333,\n",
       "        0.82309524, 0.85288443, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.85288443, 0.85288443, 0.85288443, 0.82309524,\n",
       "        0.9045511 , 0.82273292, 0.82309524, 0.85288443, 0.82309524,\n",
       "        0.85288443, 0.82273292, 0.85288443, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.85288443, 0.85288443, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.85288443, 0.82309524,\n",
       "        0.64861893, 0.646     , 0.646     , 0.646     , 0.80338083,\n",
       "        0.646     , 0.646     , 0.646     , 0.62208791, 0.646     ,\n",
       "        0.646     , 0.646     , 0.646     , 0.646     , 0.82309524,\n",
       "        0.646     , 0.66833333, 0.646     , 0.8007619 , 0.646     ,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.84430155, 0.8007619 ,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.66833333, 0.82309524, 0.85288443, 0.82309524, 0.8007619 ,\n",
       "        0.82309524, 0.82309524, 0.8007619 , 0.82309524, 0.82309524,\n",
       "        0.85288443, 0.82309524, 0.85288443, 0.85288443, 0.86077213,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.82273292, 0.85288443,\n",
       "        0.82309524, 0.82309524, 0.82991342, 0.85288443, 0.82309524,\n",
       "        0.82309524, 0.79761905, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.82309524, 0.85288443, 0.87481538,\n",
       "        0.85288443, 0.82273292, 0.85288443, 0.85288443, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.82309524, 0.82273292, 0.85288443]),\n",
       " 'split2_test_score': array([0.59823529, 0.59823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.59823529, 0.58823529, 0.58823529, 0.628815  , 0.59823529,\n",
       "        0.628815  , 0.59823529, 0.58823529, 0.58823529, 0.59823529,\n",
       "        0.59823529, 0.628815  , 0.58823529, 0.58823529, 0.59823529,\n",
       "        0.628815  , 0.66338681, 0.66338681, 0.66338681, 0.628815  ,\n",
       "        0.66338681, 0.628815  , 0.66338681, 0.66338681, 0.66338681,\n",
       "        0.63845174, 0.66338681, 0.66338681, 0.66338681, 0.693863  ,\n",
       "        0.66338681, 0.63845174, 0.628815  , 0.66338681, 0.66338681,\n",
       "        0.63672014, 0.63672014, 0.63672014, 0.63672014, 0.63672014,\n",
       "        0.63672014, 0.75338681, 0.63672014, 0.81823529, 0.67204482,\n",
       "        0.66338681, 0.66338681, 0.66338681, 0.63672014, 0.81338681,\n",
       "        0.63845174, 0.63845174, 0.843863  , 0.66338681, 0.66338681,\n",
       "        0.6129972 , 0.63672014, 0.63672014, 0.75338681, 0.72966387,\n",
       "        0.63672014, 0.66338681, 0.75338681, 0.6615873 , 0.66338681,\n",
       "        0.66338681, 0.66338681, 0.693863  , 0.6129972 , 0.78845174,\n",
       "        0.81338681, 0.693863  , 0.693863  , 0.63845174, 0.66338681,\n",
       "        0.59823529, 0.63823529, 0.628815  , 0.59823529, 0.67214834,\n",
       "        0.59823529, 0.59823529, 0.59823529, 0.59823529, 0.63823529,\n",
       "        0.63823529, 0.59823529, 0.63823529, 0.59823529, 0.59823529,\n",
       "        0.59823529, 0.63823529, 0.59823529, 0.63823529, 0.59823529,\n",
       "        0.66338681, 0.66338681, 0.81338681, 0.75338681, 0.628815  ,\n",
       "        0.78845174, 0.81338681, 0.81338681, 0.75338681, 0.81338681,\n",
       "        0.843863  , 0.75338681, 0.81338681, 0.628815  , 0.66338681,\n",
       "        0.81338681, 0.81338681, 0.67214834, 0.81338681, 0.67214834,\n",
       "        0.81338681, 0.81338681, 0.67204482, 0.72966387, 0.72966387,\n",
       "        0.843863  , 0.78871148, 0.78871148, 0.64548167, 0.67214834,\n",
       "        0.75338681, 0.81338681, 0.81338681, 0.66823529, 0.81338681,\n",
       "        0.81338681, 0.70187166, 0.81338681, 0.843863  , 0.81338681,\n",
       "        0.78871148, 0.78871148, 0.75338681, 0.75338681, 0.81338681,\n",
       "        0.75338681, 0.75338681, 0.74204482, 0.81823529, 0.67214834,\n",
       "        0.64548167, 0.72966387, 0.61475703, 0.843863  , 0.81338681,\n",
       "        0.843863  , 0.843863  , 0.81338681, 0.66338681, 0.66338681,\n",
       "        0.59823529, 0.59823529, 0.59823529, 0.63823529, 0.59823529,\n",
       "        0.58823529, 0.63823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.59823529, 0.59823529, 0.63823529, 0.59823529, 0.59823529,\n",
       "        0.59823529, 0.58823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.81338681, 0.81338681, 0.67214834, 0.67214834, 0.75338681,\n",
       "        0.75338681, 0.66338681, 0.78871148, 0.66338681, 0.81338681,\n",
       "        0.81338681, 0.81338681, 0.63823529, 0.66338681, 0.67214834,\n",
       "        0.66338681, 0.67214834, 0.66338681, 0.693863  , 0.66338681,\n",
       "        0.68005348, 0.78871148, 0.64548167, 0.78871148, 0.75338681,\n",
       "        0.72966387, 0.843863  , 0.693863  , 0.64702317, 0.843863  ,\n",
       "        0.75338681, 0.843863  , 0.78871148, 0.81338681, 0.67214834,\n",
       "        0.63672014, 0.843863  , 0.81338681, 0.81338681, 0.75338681,\n",
       "        0.78871148, 0.64548167, 0.78871148, 0.63672014, 0.75338681,\n",
       "        0.843863  , 0.78871148, 0.63672014, 0.70181818, 0.843863  ,\n",
       "        0.75338681, 0.75338681, 0.70187166, 0.63672014, 0.81338681,\n",
       "        0.81338681, 0.67204482, 0.66338681, 0.81338681, 0.81338681,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.59823529, 0.63823529,\n",
       "        0.63823529, 0.67214834, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.81823529, 0.843863  , 0.81823529, 0.70187166, 0.76368984,\n",
       "        0.67214834, 0.843863  , 0.843863  , 0.778815  , 0.843863  ,\n",
       "        0.70187166, 0.81823529, 0.81338681, 0.81338681, 0.6473262 ,\n",
       "        0.843863  , 0.76368984, 0.67214834, 0.70187166, 0.843863  ,\n",
       "        0.85777778, 0.76368984, 0.76368984, 0.76368984, 0.843863  ,\n",
       "        0.76368984, 0.76368984, 0.76368984, 0.76368984, 0.81823529,\n",
       "        0.72966387, 0.81823529, 0.843863  , 0.76368984, 0.81823529,\n",
       "        0.72966387, 0.76368984, 0.62187166, 0.81823529, 0.76368984,\n",
       "        0.62187166, 0.68005348, 0.72966387, 0.76368984, 0.78871148,\n",
       "        0.76368984, 0.76368984, 0.76368984, 0.78871148, 0.81823529,\n",
       "        0.81823529, 0.76368984, 0.81823529, 0.70187166, 0.81823529,\n",
       "        0.81823529, 0.81823529, 0.75338681, 0.81823529, 0.76368984,\n",
       "        0.67777778, 0.67777778, 0.67777778, 0.67777778, 0.71169082,\n",
       "        0.71169082, 0.63823529, 0.63823529, 0.70181818, 0.67777778,\n",
       "        0.67214834, 0.67214834, 0.67214834, 0.67777778, 0.67777778,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.67777778, 0.63823529,\n",
       "        0.88340548, 0.73229437, 0.88340548, 0.88340548, 0.85777778,\n",
       "        0.73229437, 0.88340548, 0.82825397, 0.81338681, 0.843863  ,\n",
       "        0.88340548, 0.88340548, 0.73229437, 0.88340548, 0.73229437,\n",
       "        0.843863  , 0.66338681, 0.88340548, 0.74141414, 0.88340548,\n",
       "        0.78871148, 0.76920635, 0.82825397, 0.88340548, 0.81823529,\n",
       "        0.80323232, 0.78871148, 0.80323232, 0.76920635, 0.843863  ,\n",
       "        0.88340548, 0.82825397, 0.68545455, 0.80323232, 0.74141414,\n",
       "        0.82825397, 0.66141414, 0.82825397, 0.70666667, 0.88340548,\n",
       "        0.80323232, 0.78871148, 0.7815873 , 0.82825397, 0.80323232,\n",
       "        0.78871148, 0.71047619, 0.71047619, 0.70666667, 0.85777778,\n",
       "        0.88340548, 0.843863  , 0.73229437, 0.82825397, 0.80323232,\n",
       "        0.80323232, 0.70666667, 0.82825397, 0.76920635, 0.843863  ,\n",
       "        0.67214834, 0.71169082, 0.67214834, 0.67777778, 0.67214834,\n",
       "        0.67777778, 0.67214834, 0.67777778, 0.67777778, 0.63823529,\n",
       "        0.67777778, 0.67777778, 0.67777778, 0.67777778, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.67777778, 0.67777778, 0.67777778,\n",
       "        0.82825397, 0.85777778, 0.82825397, 0.88340548, 0.73229437,\n",
       "        0.843863  , 0.88340548, 0.88340548, 0.82825397, 0.85292929,\n",
       "        0.73229437, 0.88340548, 0.80323232, 0.81338681, 0.70187166,\n",
       "        0.88340548, 0.74141414, 0.74141414, 0.74141414, 0.74141414,\n",
       "        0.71047619, 0.78871148, 0.82825397, 0.82825397, 0.80323232,\n",
       "        0.71047619, 0.80323232, 0.69466089, 0.82825397, 0.85777778,\n",
       "        0.88340548, 0.82825397, 0.88340548, 0.79292929, 0.843863  ,\n",
       "        0.82825397, 0.843863  , 0.88340548, 0.85777778, 0.82825397,\n",
       "        0.80323232, 0.82825397, 0.82825397, 0.82825397, 0.76368984,\n",
       "        0.82825397, 0.78871148, 0.82825397, 0.85777778, 0.66823529,\n",
       "        0.82825397, 0.85777778, 0.78871148, 0.88340548, 0.82825397,\n",
       "        0.88340548, 0.82825397, 0.82825397, 0.85777778, 0.88340548]),\n",
       " 'split3_test_score': array([0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.62494279,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.62494279,\n",
       "        0.62494279, 0.60545455, 0.60545455, 0.60545455, 0.62494279,\n",
       "        0.60545455, 0.60545455, 0.62494279, 0.60545455, 0.60545455,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.69984962,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.6738756 , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.6738756 , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.61341615,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.61212121, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.61341615, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.68      , 0.60545455, 0.64571429, 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.66539075, 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.60545455, 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.64571429,\n",
       "        0.68      , 0.64571429, 0.64571429, 0.68      , 0.68      ,\n",
       "        0.61212121, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.61341615, 0.61341615, 0.61341615, 0.60545455,\n",
       "        0.60545455, 0.62494279, 0.60545455, 0.60545455, 0.61341615,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.64571429,\n",
       "        0.68      , 0.60545455, 0.64571429, 0.68      , 0.64571429,\n",
       "        0.68      , 0.64571429, 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.60545455, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.61341615, 0.60545455, 0.60545455, 0.60545455, 0.61341615,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.6738756 , 0.64080201, 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.68      , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.64080201,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.64080201, 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.64080201, 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.60545455, 0.61212121, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.61341615, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.68      , 0.64571429, 0.64571429, 0.64571429, 0.60545455,\n",
       "        0.64571429, 0.68      , 0.68      , 0.64571429, 0.64571429,\n",
       "        0.64571429, 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.64571429, 0.68      ,\n",
       "        0.6738756 , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.64571429, 0.6738756 ,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.64571429, 0.68      ,\n",
       "        0.64080201, 0.64080201, 0.64080201, 0.68      , 0.64571429,\n",
       "        0.6738756 , 0.64080201, 0.6738756 , 0.68      , 0.68      ,\n",
       "        0.68      , 0.6738756 , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.61341615, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.61341615,\n",
       "        0.60545455, 0.60545455, 0.61341615, 0.60545455, 0.60545455,\n",
       "        0.64571429, 0.64571429, 0.64571429, 0.64571429, 0.68      ,\n",
       "        0.64571429, 0.60545455, 0.68      , 0.64571429, 0.64571429,\n",
       "        0.68      , 0.68      , 0.64571429, 0.64571429, 0.68      ,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.64571429, 0.64571429,\n",
       "        0.6738756 , 0.64571429, 0.68      , 0.64571429, 0.64080201,\n",
       "        0.6738756 , 0.68      , 0.64571429, 0.6738756 , 0.68      ,\n",
       "        0.64571429, 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.6738756 , 0.64080201, 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.64571429, 0.6738756 ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.68      ]),\n",
       " 'split4_test_score': array([0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61206349,\n",
       "        0.66555556, 0.61984127, 0.61984127, 0.61984127, 0.61206349,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61206349, 0.61206349, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.61206349, 0.66555556, 0.66555556,\n",
       "        0.61206349, 0.66555556, 0.65206349, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.61206349, 0.66555556,\n",
       "        0.61239316, 0.66555556, 0.66555556, 0.66555556, 0.6557265 ,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.61239316,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.6557265 , 0.6557265 ,\n",
       "        0.61239316, 0.60461538, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61239316, 0.66555556, 0.61206349, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.61239316, 0.66555556, 0.61239316, 0.61984127,\n",
       "        0.6557265 , 0.66555556, 0.66555556, 0.61239316, 0.66555556,\n",
       "        0.66555556, 0.61206349, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.64700855, 0.61984127, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.60247073, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.61239316, 0.61984127, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.6557265 , 0.61206349, 0.61984127,\n",
       "        0.61239316, 0.61984127, 0.6557265 , 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.61239316, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.6557265 , 0.60461538, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.60247073, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.64700855, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61206349, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61206349, 0.61984127, 0.6557265 , 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.61206349,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61239316, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61239316, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.60461538, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.61239316, 0.66555556, 0.66555556, 0.61206349,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.65461538, 0.66239316, 0.66239316, 0.65461538, 0.66239316,\n",
       "        0.70017094, 0.61206349, 0.66239316, 0.70017094, 0.65461538,\n",
       "        0.65461538, 0.70017094, 0.65461538, 0.70017094, 0.65461538,\n",
       "        0.65461538, 0.67619048, 0.70017094, 0.61984127, 0.65461538,\n",
       "        0.61206349, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.61206349,\n",
       "        0.66555556, 0.66555556, 0.64531025, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.6557265 , 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.6557265 ,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.69920635, 0.66555556,\n",
       "        0.66555556, 0.6557265 , 0.66555556, 0.66555556, 0.61206349,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61206349, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.65311111, 0.6557265 , 0.66555556, 0.66555556,\n",
       "        0.61206349, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.70017094,\n",
       "        0.66239316, 0.61984127, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.66239316, 0.61984127, 0.70017094,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.70017094, 0.61984127, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.61206349, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.60461538, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.70017094, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.61206349, 0.66555556, 0.70017094, 0.70017094, 0.66239316,\n",
       "        0.66555556, 0.70017094, 0.61984127, 0.70017094, 0.70017094,\n",
       "        0.70017094, 0.61984127, 0.62349206, 0.66555556, 0.66555556,\n",
       "        0.70017094, 0.61984127, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61239316, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66239316, 0.66555556, 0.66555556, 0.66555556]),\n",
       " 'mean_test_score': array([0.62563349, 0.61824134, 0.62563349, 0.62085108, 0.65769743,\n",
       "        0.63477635, 0.62810016, 0.62363349, 0.66773271, 0.63244225,\n",
       "        0.64011375, 0.61487291, 0.60890499, 0.62363349, 0.62953114,\n",
       "        0.62563349, 0.62696702, 0.61683964, 0.61921057, 0.62563349,\n",
       "        0.64366055, 0.6552837 , 0.66990969, 0.69156984, 0.68045768,\n",
       "        0.68251008, 0.66144121, 0.65478041, 0.67892329, 0.66975364,\n",
       "        0.66036915, 0.68847791, 0.64728207, 0.65077981, 0.70166321,\n",
       "        0.69239481, 0.66228979, 0.65760251, 0.69083926, 0.69570104,\n",
       "        0.65749148, 0.65379164, 0.68265775, 0.65198195, 0.66296345,\n",
       "        0.68642698, 0.68590649, 0.67800172, 0.70597142, 0.66476751,\n",
       "        0.66916943, 0.68333506, 0.69396853, 0.68803591, 0.68795686,\n",
       "        0.65953262, 0.6653104 , 0.72840319, 0.67515211, 0.69031228,\n",
       "        0.66595533, 0.67171601, 0.64728054, 0.69839481, 0.68075817,\n",
       "        0.6663757 , 0.68364432, 0.70002708, 0.67187546, 0.67252553,\n",
       "        0.72161631, 0.69031228, 0.67083493, 0.67928844, 0.69632844,\n",
       "        0.70041298, 0.67715046, 0.68502657, 0.68193433, 0.67261078,\n",
       "        0.67012236, 0.64757924, 0.64089229, 0.62959978, 0.64025948,\n",
       "        0.62527968, 0.61929583, 0.62959978, 0.67753479, 0.63267936,\n",
       "        0.63943349, 0.63392844, 0.62156875, 0.62419107, 0.62513311,\n",
       "        0.63392844, 0.63939844, 0.64872209, 0.64227597, 0.63076659,\n",
       "        0.6649967 , 0.64068774, 0.7114381 , 0.71249379, 0.69379647,\n",
       "        0.69702541, 0.72713394, 0.68909263, 0.74594856, 0.7621853 ,\n",
       "        0.73280378, 0.70061387, 0.73243409, 0.6596992 , 0.71157695,\n",
       "        0.72633132, 0.74521654, 0.66784494, 0.72209006, 0.66295348,\n",
       "        0.70693767, 0.74913394, 0.68815672, 0.69070517, 0.67659681,\n",
       "        0.72478309, 0.75798039, 0.71107498, 0.67830585, 0.67061117,\n",
       "        0.7325967 , 0.72239481, 0.68734214, 0.65780896, 0.72239481,\n",
       "        0.69832816, 0.69110449, 0.69055295, 0.70643927, 0.69908005,\n",
       "        0.76344877, 0.72896709, 0.68685189, 0.67014758, 0.73575155,\n",
       "        0.67623716, 0.72496809, 0.73068469, 0.69239566, 0.68132215,\n",
       "        0.67598881, 0.74053911, 0.65205455, 0.68777131, 0.68538791,\n",
       "        0.69664819, 0.68525238, 0.67774631, 0.68642512, 0.66157839,\n",
       "        0.63743846, 0.63299077, 0.6203507 , 0.63876659, 0.6203507 ,\n",
       "        0.61921057, 0.63363349, 0.62861261, 0.66564124, 0.63523326,\n",
       "        0.63101733, 0.63552076, 0.64539471, 0.62124733, 0.63392844,\n",
       "        0.62085108, 0.63516895, 0.63494023, 0.62513311, 0.66574299,\n",
       "        0.69833919, 0.68515522, 0.69818351, 0.67182533, 0.68626783,\n",
       "        0.71009006, 0.69861704, 0.72536018, 0.70033548, 0.70784823,\n",
       "        0.72347418, 0.72449379, 0.68338936, 0.69327476, 0.6916037 ,\n",
       "        0.6938514 , 0.65027875, 0.65493823, 0.71237204, 0.6934381 ,\n",
       "        0.68217259, 0.68648026, 0.70049416, 0.70977763, 0.6877267 ,\n",
       "        0.69968053, 0.71964024, 0.70516404, 0.71485165, 0.7269345 ,\n",
       "        0.70313394, 0.72563291, 0.67423003, 0.7146158 , 0.66268247,\n",
       "        0.68510076, 0.69340865, 0.75065538, 0.72675673, 0.70440214,\n",
       "        0.69276598, 0.66691147, 0.72536018, 0.67289585, 0.70309152,\n",
       "        0.69227516, 0.67812886, 0.68039481, 0.68592075, 0.70918896,\n",
       "        0.72556797, 0.70294978, 0.68382056, 0.66115298, 0.68559683,\n",
       "        0.75049005, 0.70387251, 0.70112294, 0.71953767, 0.68924879,\n",
       "        0.63654629, 0.63530583, 0.63580621, 0.66936812, 0.65851599,\n",
       "        0.66931218, 0.6344638 , 0.64070145, 0.63400304, 0.66830116,\n",
       "        0.63807894, 0.63959034, 0.66968271, 0.640257  , 0.64490695,\n",
       "        0.66886773, 0.65583169, 0.66981256, 0.62779978, 0.63047923,\n",
       "        0.70517446, 0.71268053, 0.70069784, 0.6897065 , 0.70408811,\n",
       "        0.70188625, 0.70582339, 0.71896624, 0.73288625, 0.72481648,\n",
       "        0.68428226, 0.71297923, 0.73413592, 0.724871  , 0.6798742 ,\n",
       "        0.7121091 , 0.70680749, 0.67359321, 0.72152036, 0.75576032,\n",
       "        0.71557194, 0.71939776, 0.73221509, 0.73221509, 0.7505926 ,\n",
       "        0.72804422, 0.74294483, 0.70126271, 0.75124325, 0.74312418,\n",
       "        0.7254099 , 0.7602465 , 0.74467097, 0.70799287, 0.7127359 ,\n",
       "        0.730974  , 0.6992969 , 0.69764884, 0.75902162, 0.72151668,\n",
       "        0.71874889, 0.71548782, 0.7254099 , 0.70126271, 0.69448462,\n",
       "        0.70126271, 0.70126271, 0.70126271, 0.69901381, 0.7121718 ,\n",
       "        0.70826704, 0.73221509, 0.72725807, 0.68889907, 0.75245752,\n",
       "        0.73650946, 0.75136503, 0.69723629, 0.74312418, 0.70126271,\n",
       "        0.68575123, 0.65584646, 0.67199399, 0.66016392, 0.70272548,\n",
       "        0.68313704, 0.67886183, 0.64660463, 0.66062602, 0.70250605,\n",
       "        0.65840556, 0.64871105, 0.64871105, 0.65451313, 0.69111631,\n",
       "        0.63746178, 0.65384775, 0.67779291, 0.67632266, 0.68448009,\n",
       "        0.77618903, 0.73344472, 0.7510161 , 0.74856625, 0.74717195,\n",
       "        0.73212425, 0.75542339, 0.74684294, 0.74228633, 0.7435149 ,\n",
       "        0.74856625, 0.76418903, 0.68205399, 0.75324244, 0.72482395,\n",
       "        0.74942339, 0.71895299, 0.71605831, 0.74093362, 0.77618903,\n",
       "        0.72306491, 0.73544071, 0.73125023, 0.75828054, 0.74867074,\n",
       "        0.73906089, 0.74308109, 0.73310305, 0.72267448, 0.70905192,\n",
       "        0.75142339, 0.74406522, 0.71583565, 0.73310305, 0.72988227,\n",
       "        0.75658357, 0.71762162, 0.75320807, 0.72203347, 0.75828054,\n",
       "        0.75069748, 0.73142967, 0.72093444, 0.75320807, 0.74055123,\n",
       "        0.7440747 , 0.71578262, 0.72842764, 0.72312325, 0.753155  ,\n",
       "        0.75828054, 0.74914716, 0.73401615, 0.73720807, 0.7422459 ,\n",
       "        0.7422459 , 0.73878189, 0.73810738, 0.74139855, 0.74122918,\n",
       "        0.64886341, 0.66129574, 0.66161513, 0.66143621, 0.68988172,\n",
       "        0.65018447, 0.66031032, 0.64537027, 0.65391745, 0.65352771,\n",
       "        0.667087  , 0.64537027, 0.64610043, 0.66016392, 0.6892668 ,\n",
       "        0.65352771, 0.64192844, 0.66175625, 0.69072638, 0.65451313,\n",
       "        0.75830159, 0.74344071, 0.75436508, 0.76157315, 0.73251769,\n",
       "        0.76142339, 0.76165716, 0.77656625, 0.73753595, 0.74492086,\n",
       "        0.69710593, 0.76456625, 0.7472551 , 0.74656252, 0.72379282,\n",
       "        0.76970911, 0.74816798, 0.73684417, 0.72931084, 0.72931084,\n",
       "        0.71362429, 0.73248459, 0.75320807, 0.74635093, 0.72771946,\n",
       "        0.72842764, 0.74779645, 0.71963232, 0.74595289, 0.75911283,\n",
       "        0.75142339, 0.74725023, 0.75237974, 0.73700028, 0.75037204,\n",
       "        0.74725023, 0.73801237, 0.74913768, 0.753155  , 0.74725023,\n",
       "        0.74861729, 0.75198319, 0.73026778, 0.75198319, 0.74345656,\n",
       "        0.75198319, 0.73804439, 0.75198319, 0.75225569, 0.72335495,\n",
       "        0.75320807, 0.76844617, 0.72929958, 0.76887703, 0.74725023,\n",
       "        0.74823838, 0.77086131, 0.74725023, 0.75267524, 0.76423838]),\n",
       " 'std_test_score': array([0.02321803, 0.0164234 , 0.02321803, 0.02087489, 0.07641013,\n",
       "        0.02770432, 0.03072917, 0.02578119, 0.08302321, 0.0268922 ,\n",
       "        0.01953574, 0.01310868, 0.01315228, 0.02578119, 0.0210367 ,\n",
       "        0.02321803, 0.01757117, 0.01463228, 0.02081559, 0.02321803,\n",
       "        0.02592428, 0.02328406, 0.02073663, 0.0724152 , 0.07791099,\n",
       "        0.02927651, 0.04424197, 0.02611429, 0.0152019 , 0.02097184,\n",
       "        0.03016952, 0.05619321, 0.02380863, 0.02056683, 0.03639471,\n",
       "        0.07211755, 0.03804054, 0.0169918 , 0.07373155, 0.06882377,\n",
       "        0.04723122, 0.0118047 , 0.03377744, 0.0236716 , 0.02360018,\n",
       "        0.05172035, 0.04422157, 0.03399413, 0.07784392, 0.02738297,\n",
       "        0.04064392, 0.02879309, 0.04981317, 0.05766642, 0.06411503,\n",
       "        0.02953639, 0.04168883, 0.0972058 , 0.010264  , 0.05599184,\n",
       "        0.05489772, 0.02317818, 0.02344137, 0.0560315 , 0.0449604 ,\n",
       "        0.0436803 , 0.05129617, 0.03819175, 0.0505412 , 0.03579197,\n",
       "        0.0730984 , 0.05599184, 0.01444106, 0.06654169, 0.04643736,\n",
       "        0.05671938, 0.05142864, 0.03845132, 0.04212209, 0.04915209,\n",
       "        0.08164491, 0.03599617, 0.02168369, 0.02780766, 0.0216506 ,\n",
       "        0.02796461, 0.02101401, 0.02780766, 0.08328224, 0.02483112,\n",
       "        0.02161112, 0.03283191, 0.01040313, 0.02210532, 0.02251778,\n",
       "        0.03283191, 0.03528297, 0.04092276, 0.02059131, 0.03135738,\n",
       "        0.0276127 , 0.02360812, 0.06753852, 0.07005808, 0.07226875,\n",
       "        0.04812209, 0.08145216, 0.06393059, 0.07052299, 0.07331127,\n",
       "        0.08871647, 0.07972442, 0.07929266, 0.07566708, 0.06208269,\n",
       "        0.08116288, 0.09106779, 0.04005428, 0.08349522, 0.02529222,\n",
       "        0.06821522, 0.08538248, 0.04882258, 0.06085178, 0.05405268,\n",
       "        0.09241651, 0.07213165, 0.06647631, 0.05861058, 0.04076471,\n",
       "        0.08007847, 0.08402655, 0.06439443, 0.02082843, 0.08402655,\n",
       "        0.08074607, 0.02748839, 0.06510294, 0.07005031, 0.09322826,\n",
       "        0.07688606, 0.0716349 , 0.0661426 , 0.04744748, 0.06067405,\n",
       "        0.04349049, 0.0711286 , 0.07292347, 0.06461846, 0.04934171,\n",
       "        0.05144158, 0.08593434, 0.02458598, 0.08279218, 0.06719777,\n",
       "        0.07670987, 0.08061988, 0.07020324, 0.04951198, 0.024753  ,\n",
       "        0.04256789, 0.03207828, 0.01997369, 0.02681016, 0.01997369,\n",
       "        0.02081559, 0.01888666, 0.03550227, 0.08420538, 0.03462911,\n",
       "        0.03019139, 0.0315816 , 0.02766498, 0.01744263, 0.03283191,\n",
       "        0.02087489, 0.03254832, 0.04790097, 0.02251778, 0.08488304,\n",
       "        0.07821856, 0.07836462, 0.04798074, 0.0127885 , 0.03759831,\n",
       "        0.07318695, 0.06661244, 0.07043427, 0.06456908, 0.07393247,\n",
       "        0.05752026, 0.08040875, 0.05531897, 0.08047612, 0.07349435,\n",
       "        0.03963946, 0.03668123, 0.02039488, 0.06005045, 0.06902618,\n",
       "        0.05094421, 0.06576205, 0.07709905, 0.08387285, 0.03768595,\n",
       "        0.05043308, 0.07642832, 0.05811037, 0.06498132, 0.09309136,\n",
       "        0.054231  , 0.07075692, 0.06258379, 0.07054729, 0.0224236 ,\n",
       "        0.05453008, 0.08111826, 0.08807032, 0.08097428, 0.078133  ,\n",
       "        0.07174462, 0.05581776, 0.07043427, 0.03669068, 0.0831218 ,\n",
       "        0.07934763, 0.05998839, 0.05411097, 0.06177442, 0.09634683,\n",
       "        0.06045425, 0.06487972, 0.01349504, 0.02759452, 0.06680775,\n",
       "        0.06598368, 0.06154776, 0.06109528, 0.06094151, 0.06714881,\n",
       "        0.03009954, 0.02239638, 0.0228789 , 0.07199077, 0.08170977,\n",
       "        0.07807228, 0.02550425, 0.02022019, 0.04657922, 0.08215137,\n",
       "        0.02637692, 0.03780957, 0.08225205, 0.03716631, 0.02506358,\n",
       "        0.07101539, 0.03760517, 0.07895771, 0.01758455, 0.02564565,\n",
       "        0.08152402, 0.07389097, 0.0691822 , 0.02853301, 0.03974893,\n",
       "        0.05282146, 0.07807783, 0.07715662, 0.07825492, 0.09536098,\n",
       "        0.03514273, 0.05955539, 0.07618409, 0.08090053, 0.04791129,\n",
       "        0.08170447, 0.05000205, 0.06067616, 0.04954932, 0.07185247,\n",
       "        0.07815833, 0.0648123 , 0.05718785, 0.05718785, 0.07966394,\n",
       "        0.05727606, 0.07473223, 0.04188691, 0.08374088, 0.06658232,\n",
       "        0.05502099, 0.08741586, 0.09035094, 0.0381453 , 0.05949239,\n",
       "        0.06471688, 0.04370765, 0.06645693, 0.08856703, 0.07179538,\n",
       "        0.08373161, 0.05776403, 0.05502099, 0.04188691, 0.05966514,\n",
       "        0.04188691, 0.04188691, 0.04188691, 0.05038765, 0.05993824,\n",
       "        0.06349002, 0.05718785, 0.07593973, 0.02867576, 0.06863768,\n",
       "        0.07432592, 0.08353362, 0.04076721, 0.06658232, 0.04188691,\n",
       "        0.08052923, 0.02475917, 0.0693016 , 0.0335765 , 0.0713282 ,\n",
       "        0.05774118, 0.06649124, 0.02487828, 0.03328498, 0.07012862,\n",
       "        0.03295672, 0.02994597, 0.02994597, 0.02714274, 0.06389653,\n",
       "        0.0246326 , 0.03098481, 0.05940454, 0.06885027, 0.06662762,\n",
       "        0.08713774, 0.07478808, 0.09088695, 0.1013687 , 0.09601837,\n",
       "        0.05916839, 0.09514806, 0.06871487, 0.07155483, 0.08007842,\n",
       "        0.1013687 , 0.08309145, 0.04806894, 0.08804454, 0.07026425,\n",
       "        0.08064078, 0.082006  , 0.09903521, 0.07646629, 0.08713774,\n",
       "        0.0672568 , 0.05795912, 0.08676305, 0.08359601, 0.06304222,\n",
       "        0.08339707, 0.07747006, 0.07576787, 0.05770948, 0.07738953,\n",
       "        0.09082916, 0.08773548, 0.07874851, 0.07576787, 0.05574474,\n",
       "        0.07011089, 0.07918899, 0.07589703, 0.07303728, 0.08359601,\n",
       "        0.09579755, 0.06961744, 0.07886144, 0.07589703, 0.07206436,\n",
       "        0.07063643, 0.06350847, 0.06761626, 0.07313977, 0.07622802,\n",
       "        0.08359601, 0.07374121, 0.06597824, 0.09361734, 0.06327769,\n",
       "        0.06327769, 0.05500692, 0.08088785, 0.06742185, 0.08452613,\n",
       "        0.03754738, 0.03516153, 0.03319647, 0.03288372, 0.06533133,\n",
       "        0.0247036 , 0.03239772, 0.02948181, 0.03276306, 0.03275732,\n",
       "        0.03733297, 0.02948181, 0.02887956, 0.0335765 , 0.07363542,\n",
       "        0.03275732, 0.02761963, 0.03103697, 0.06381021, 0.02714274,\n",
       "        0.08408549, 0.09486016, 0.08109636, 0.09421601, 0.05393956,\n",
       "        0.08686834, 0.10677953, 0.08736884, 0.08824765, 0.08243273,\n",
       "        0.0320574 , 0.08311621, 0.07967983, 0.09342711, 0.05276645,\n",
       "        0.09563332, 0.0692175 , 0.07254803, 0.06590053, 0.06590053,\n",
       "        0.07906288, 0.06845059, 0.07589703, 0.08338128, 0.08759289,\n",
       "        0.06761626, 0.07153532, 0.07369738, 0.06981569, 0.08237767,\n",
       "        0.09082916, 0.06866317, 0.09200247, 0.08189989, 0.07252151,\n",
       "        0.06866317, 0.07553374, 0.09496422, 0.07622802, 0.06866317,\n",
       "        0.07143376, 0.07710838, 0.08774827, 0.07710838, 0.07561944,\n",
       "        0.07710838, 0.0618276 , 0.07710838, 0.0897727 , 0.06738148,\n",
       "        0.07589703, 0.08225297, 0.08703051, 0.08807795, 0.06866317,\n",
       "        0.10611034, 0.08201521, 0.06866317, 0.07623942, 0.0888965 ]),\n",
       " 'rank_test_score': array([458, 477, 458, 470, 382, 438, 455, 466, 351, 447, 423, 479, 480,\n",
       "        466, 453, 458, 457, 478, 475, 458, 415, 387, 341, 260, 310, 304,\n",
       "        368, 389, 314, 343, 373, 275, 407, 399, 218, 257, 364, 383, 263,\n",
       "        247, 384, 394, 303, 398, 361, 284, 288, 318, 205, 360, 347, 301,\n",
       "        249, 277, 278, 378, 358, 135, 327, 267, 355, 336, 408, 237, 309,\n",
       "        354, 299, 231, 334, 332, 165, 267, 337, 313, 246, 229, 322, 295,\n",
       "        307, 331, 340, 406, 418, 451, 421, 462, 474, 451, 321, 446, 425,\n",
       "        441, 468, 465, 463, 441, 426, 403, 416, 449, 359, 420, 193, 188,\n",
       "        251, 244, 139, 273,  75,  12, 114, 227, 118, 377, 192, 142,  76,\n",
       "        350, 163, 362, 202,  56, 276, 265, 323, 153,  23, 194, 316, 338,\n",
       "        115, 161, 281, 381, 161, 239, 262, 266, 204, 234,  11, 132, 282,\n",
       "        339, 106, 325, 149, 126, 256, 308, 326,  95, 397, 279, 291, 245,\n",
       "        292, 320, 285, 367, 430, 445, 472, 427, 472, 475, 444, 454, 357,\n",
       "        435, 448, 433, 411, 469, 441, 470, 436, 437, 463, 356, 238, 293,\n",
       "        240, 335, 286, 195, 236, 147, 230, 201, 156, 154, 300, 254, 259,\n",
       "        250, 400, 388, 189, 252, 305, 283, 228, 196, 280, 232, 169, 208,\n",
       "        182, 140, 212, 143, 328, 183, 363, 294, 253,  49, 141, 209, 255,\n",
       "        353, 147, 330, 213, 258, 317, 311, 287, 197, 144, 214, 298, 371,\n",
       "        290,  51, 211, 225, 171, 272, 431, 434, 432, 345, 379, 346, 439,\n",
       "        419, 440, 349, 428, 424, 344, 422, 414, 348, 386, 342, 456, 450,\n",
       "        207, 187, 226, 270, 210, 217, 206, 173, 113, 152, 297, 185, 108,\n",
       "        150, 312, 191, 203, 329, 166,  25, 180, 172, 119, 119,  50, 136,\n",
       "         87, 219,  46,  84, 145,  16,  78, 200, 186, 125, 233, 241,  18,\n",
       "        167, 175, 181, 145, 219, 248, 219, 219, 219, 235, 190, 199, 119,\n",
       "        138, 274,  36, 105,  45, 242,  84, 219, 289, 385, 333, 375, 215,\n",
       "        302, 315, 409, 372, 216, 380, 404, 404, 390, 261, 429, 393, 319,\n",
       "        324, 296,   2, 110,  47,  59,  70, 122,  26,  71,  88,  81,  59,\n",
       "         10, 306,  28, 151,  53, 174, 177,  93,   2, 159, 107, 124,  20,\n",
       "         57,  96,  86, 111, 160, 198,  43,  80, 178, 111, 128,  24, 176,\n",
       "         29, 164,  20,  48, 123, 168,  29,  94,  79, 179, 133, 158,  33,\n",
       "         20,  54, 109, 102,  89,  89,  97,  98,  91,  92, 402, 370, 366,\n",
       "        369, 269, 401, 374, 412, 392, 395, 352, 412, 410, 375, 271, 395,\n",
       "        417, 365, 264, 390,  19,  83,  27,  14, 116,  15,  13,   1, 101,\n",
       "         77, 243,   8,  64,  72, 155,   5,  62, 104, 129, 129, 184, 117,\n",
       "         29,  73, 137, 133,  63, 170,  74,  17,  43,  65,  37, 103,  52,\n",
       "         65, 100,  55,  33,  65,  58,  39, 127,  39,  82,  39,  99,  39,\n",
       "         38, 157,  29,   7, 131,   6,  65,  61,   4,  65,  35,   9]),\n",
       " 'split0_train_score': array([0.81596079, 0.80900667, 0.81011968, 0.81935476, 0.80666166,\n",
       "        0.81173957, 0.81830839, 0.80778208, 0.81011968, 0.81011968,\n",
       "        0.81830839, 0.81173957, 0.78653745, 0.80916489, 0.81011968,\n",
       "        0.81935476, 0.82792098, 0.81087219, 0.81087219, 0.81087219,\n",
       "        0.98241758, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99076479, 0.95308363, 0.98241758,\n",
       "        0.96231884, 0.93188084, 0.96231884, 0.98241758, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.99287476, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.83130119, 0.83899986, 0.85397142, 0.83899986, 0.81428635,\n",
       "        0.84761258, 0.80829765, 0.79646179, 0.80318287, 0.80495436,\n",
       "        0.84573529, 0.80318287, 0.85273088, 0.79938354, 0.84056998,\n",
       "        0.81173957, 0.78370193, 0.79427258, 0.84880238, 0.83201735,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 1.        , 0.98241758,\n",
       "        0.98241758, 1.        , 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.79646179, 0.80829765, 0.80829765, 0.83899986, 0.85397142,\n",
       "        0.81011968, 0.81685786, 0.85273088, 0.85080998, 0.79427258,\n",
       "        0.80318287, 0.83899986, 0.81685786, 0.81685786, 0.79427258,\n",
       "        0.80318287, 0.79810903, 0.80318287, 0.85397142, 0.84761258,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 0.98241758,\n",
       "        0.98241758, 0.97698887, 0.98241758, 0.97411477, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99287476, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.79982793, 0.81173957, 0.80360498, 0.85367307, 0.81428635,\n",
       "        0.85367307, 0.84855478, 0.84042019, 0.81674705, 0.80666166,\n",
       "        0.81173957, 0.85367307, 0.80498462, 0.81011968, 0.81685786,\n",
       "        0.81173957, 0.80062846, 0.80574675, 0.81173957, 0.81685786,\n",
       "        1.        , 0.99169719, 1.        , 0.99169719, 0.99169719,\n",
       "        1.        , 0.99169719, 1.        , 0.99169719, 1.        ,\n",
       "        0.99169719, 0.99169719, 0.97411477, 0.97411477, 0.99169719,\n",
       "        1.        , 1.        , 0.97411477, 0.97411477, 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.80528983, 0.80457273, 0.8121179 , 0.8206746 , 0.85414966,\n",
       "        0.8206746 , 0.8489433 , 0.84211523, 0.81279708, 0.81550929,\n",
       "        0.80439817, 0.80695665, 0.80528983, 0.80528983, 0.79307077,\n",
       "        0.81550929, 0.79810903, 0.78806286, 0.80439817, 0.84625587,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.80666166, 0.80528983, 0.8206746 , 0.85654762, 0.78785184,\n",
       "        0.8489433 , 0.8575    , 0.79674717, 0.8206746 , 0.8206746 ,\n",
       "        0.81550929, 0.81550929, 0.81210602, 0.81668638, 0.80528983,\n",
       "        0.8206746 , 0.79674717, 0.8206746 , 0.80689342, 0.8206746 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split1_train_score': array([0.79063237, 0.78024276, 0.79546485, 0.81097506, 0.80244827,\n",
       "        0.80244827, 0.79063237, 0.84866858, 0.8362863 , 0.78659197,\n",
       "        0.79287187, 0.79546485, 0.84949245, 0.79710659, 0.81097506,\n",
       "        0.80244827, 0.80244827, 0.80239708, 0.81325908, 0.79546485,\n",
       "        0.99267399, 1.        , 0.98241758, 0.98241758, 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 0.98241758, 0.96231884,\n",
       "        1.        , 1.        , 0.95693637, 0.95693637, 0.96231884,\n",
       "        0.96231884, 0.97698887, 0.97698887, 0.96231884, 0.94974913,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 1.        , 0.98241758,\n",
       "        0.83715025, 0.83888634, 0.80239708, 0.80239708, 0.79309237,\n",
       "        0.81208956, 0.81325908, 0.81208956, 0.80510131, 0.80171152,\n",
       "        0.83888634, 0.80398298, 0.82993804, 0.80510131, 0.80918294,\n",
       "        0.83888634, 0.83310985, 0.79287187, 0.80354869, 0.78868592,\n",
       "        0.99267399, 1.        , 0.98241758, 1.        , 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 0.98241758, 0.97698887,\n",
       "        0.96231884, 0.95693637, 0.95693637, 0.97698887, 0.96231884,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.80239708, 0.83888634, 0.84746432, 0.79443933, 0.78818477,\n",
       "        0.79130464, 0.8192178 , 0.84327042, 0.79546485, 0.83715025,\n",
       "        0.83358634, 0.83888634, 0.80398298, 0.84003795, 0.80239708,\n",
       "        0.80354869, 0.79574496, 0.79954649, 0.80239708, 0.80354869,\n",
       "        0.98241758, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 0.98241758, 1.        ,\n",
       "        0.98241758, 1.        , 0.98241758, 0.96231884, 0.98241758,\n",
       "        1.        , 0.97149711, 0.98241758, 0.95508685, 0.97698887,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.9789011 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 0.98241758, 1.        ,\n",
       "        0.83195411, 0.87683085, 0.8683089 , 0.87004499, 0.86311276,\n",
       "        0.87004499, 0.83888634, 0.8683089 , 0.87274922, 0.87973747,\n",
       "        0.87973747, 0.8683089 , 0.85781688, 0.86163811, 0.87004499,\n",
       "        0.83195411, 0.87973747, 0.8683089 , 0.87004499, 0.87973747,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 0.98241758, 1.        ,\n",
       "        0.98241758, 0.98241758, 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.86525855, 0.83358634, 0.83358634, 0.83888634, 0.83888634,\n",
       "        0.82820818, 0.82820818, 0.83358634, 0.83603575, 0.83888634,\n",
       "        0.83888634, 0.83358634, 0.83888634, 0.82820818, 0.83888634,\n",
       "        0.83888634, 0.83358634, 0.82820818, 0.83888634, 0.82820818,\n",
       "        1.        , 0.99267399, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.99267399, 0.99267399,\n",
       "        1.        , 1.        , 0.99267399, 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.78552904, 0.82820818, 0.83888634, 0.83888634, 0.83888634,\n",
       "        0.83358634, 0.83358634, 0.82820818, 0.82820818, 0.83888634,\n",
       "        0.83358634, 0.82820818, 0.83888634, 0.82820818, 0.83358634,\n",
       "        0.82820818, 0.83358634, 0.83358634, 0.83888634, 0.83358634,\n",
       "        0.99267399, 1.        , 1.        , 1.        , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 0.99267399, 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.99267399, 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split2_train_score': array([0.82484127, 0.86071429, 0.82331296, 0.86071429, 0.85892857,\n",
       "        0.85540828, 0.85540828, 0.86071429, 0.79270382, 0.87117889,\n",
       "        0.82671856, 0.82061151, 0.84746432, 0.84458165, 0.82484127,\n",
       "        0.82671856, 0.82142018, 0.85003292, 0.82484127, 0.85540828,\n",
       "        0.96763746, 1.        , 0.98241758, 0.97513935, 0.99267399,\n",
       "        1.        , 0.97513935, 0.98241758, 0.98241758, 0.97513935,\n",
       "        0.97513935, 0.97513935, 0.97513935, 0.95508685, 0.96231884,\n",
       "        0.98241758, 0.98241758, 0.95508685, 0.97513935, 0.95508685,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 1.        , 0.98241758,\n",
       "        0.82998498, 0.8890949 , 0.85446429, 0.85980335, 0.80574675,\n",
       "        0.86391292, 0.84580861, 0.85314126, 0.84679205, 0.85476651,\n",
       "        0.90214279, 0.85980335, 0.86472728, 0.88048934, 0.85540828,\n",
       "        0.85648453, 0.82671856, 0.90214279, 0.84416426, 0.85476651,\n",
       "        1.        , 0.98241758, 1.        , 1.        , 0.97513935,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98241758,\n",
       "        0.98241758, 1.        , 0.98241758, 0.98241758, 0.98241758,\n",
       "        0.98241758, 0.94233535, 0.98241758, 0.98241758, 0.97513935,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 1.        , 0.98241758, 0.98241758,\n",
       "        0.82094169, 0.85205861, 0.86071429, 0.85792607, 0.88383262,\n",
       "        0.84679205, 0.85980335, 0.86071429, 0.84679205, 0.85205861,\n",
       "        0.88048934, 0.88383262, 0.85446429, 0.86071429, 0.85559283,\n",
       "        0.85446429, 0.84416426, 0.87944614, 0.89028423, 0.85540828,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99267399,\n",
       "        0.98241758, 0.98241758, 0.98241758, 0.98241758, 0.98241758,\n",
       "        0.98241758, 0.98241758, 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.97698887, 0.98241758, 0.98241758, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98241758, 0.98241758, 1.        , 1.        ,\n",
       "        0.8041549 , 0.84145494, 0.85205861, 0.84145494, 0.86987088,\n",
       "        0.84679205, 0.80747645, 0.84145494, 0.84505596, 0.80747645,\n",
       "        0.84679205, 0.85032892, 0.80747645, 0.80563795, 0.87521639,\n",
       "        0.80563795, 0.80563795, 0.80747645, 0.83971045, 0.84145494,\n",
       "        0.99267399, 0.99267399, 0.99267399, 0.99267399, 0.99169719,\n",
       "        0.99267399, 0.99267399, 0.99267399, 0.96656664, 0.99267399,\n",
       "        0.99267399, 0.99267399, 0.99267399, 0.99267399, 0.99267399,\n",
       "        0.99267399, 0.97513935, 0.97513935, 0.97513935, 0.97513935,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.87533309, 0.83180627, 0.84607294, 0.80496947, 0.87432468,\n",
       "        0.87432468, 0.84607294, 0.87432468, 0.83500333, 0.86419578,\n",
       "        0.87432468, 0.8354349 , 0.8633135 , 0.830012  , 0.84078647,\n",
       "        0.86366722, 0.86366722, 0.86444614, 0.86366722, 0.86366722,\n",
       "        1.        , 1.        , 0.99267399, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99267399,\n",
       "        0.99267399, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 0.98241758, 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.87432468, 0.86924305, 0.90125109, 0.87432468, 0.86419578,\n",
       "        0.83610735, 0.8633135 , 0.8354349 , 0.84145494, 0.830012  ,\n",
       "        0.87432468, 0.86876477, 0.87432468, 0.84607294, 0.83350679,\n",
       "        0.86366722, 0.83860434, 0.87432468, 0.86366722, 0.87944614,\n",
       "        1.        , 0.99267399, 1.        , 1.        , 1.        ,\n",
       "        0.99267399, 1.        , 1.        , 1.        , 0.99267399,\n",
       "        1.        , 1.        , 0.99267399, 0.99267399, 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split3_train_score': array([0.87803197, 0.87803197, 0.84802551, 0.84802551, 0.83875434,\n",
       "        0.8428501 , 0.87803197, 0.84802551, 0.84802551, 0.84802551,\n",
       "        0.83975335, 0.84802551, 0.84802551, 0.84802551, 0.84802551,\n",
       "        0.84802551, 0.84802551, 0.84802551, 0.84802551, 0.84802551,\n",
       "        0.99169719, 1.        , 0.99169719, 0.99169719, 1.        ,\n",
       "        1.        , 1.        , 0.99169719, 0.98241758, 0.99169719,\n",
       "        0.99169719, 0.99169719, 0.99169719, 0.97411477, 0.98241758,\n",
       "        0.97411477, 0.94333333, 0.96699634, 0.97411477, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.8428501 , 0.87803197, 0.8428501 , 0.85703344, 0.87150741,\n",
       "        0.83975335, 0.87285657, 0.85481948, 0.87285657, 0.84802551,\n",
       "        0.88703991, 0.8428501 , 0.84816797, 0.87285657, 0.84802551,\n",
       "        0.87285657, 0.87285657, 0.87285657, 0.85703344, 0.87803197,\n",
       "        0.99169719, 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        0.99169719, 0.99169719, 1.        , 1.        , 0.99169719,\n",
       "        1.        , 0.99169719, 0.97411477, 1.        , 0.99169719,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.99169719, 0.96699634,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.85703344, 0.84802551, 0.88886852, 0.84802551, 0.88886852,\n",
       "        0.84802551, 0.85176646, 0.85481948, 0.86916025, 0.85703344,\n",
       "        0.88703991, 0.88177293, 0.83117586, 0.88709663, 0.84969281,\n",
       "        0.8428501 , 0.84802551, 0.85176646, 0.85703344, 0.87803197,\n",
       "        0.99169719, 1.        , 1.        , 1.        , 0.97411477,\n",
       "        0.99169719, 1.        , 0.99169719, 1.        , 0.99169719,\n",
       "        0.99169719, 0.99169719, 0.97411477, 0.99169719, 0.97411477,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.97411477, 0.97411477,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.99169719, 1.        ,\n",
       "        0.87803197, 0.88052551, 0.84802551, 0.84969281, 0.84969281,\n",
       "        0.85481948, 0.84969281, 0.8788582 , 0.84969281, 0.84969281,\n",
       "        0.85481948, 0.88565217, 0.84802551, 0.84802551, 0.84802551,\n",
       "        0.84802551, 0.83975335, 0.84802551, 0.84969281, 0.8788582 ,\n",
       "        0.99169719, 0.99169719, 0.99169719, 0.99169719, 0.99169719,\n",
       "        1.        , 0.99169719, 1.        , 0.99169719, 0.99169719,\n",
       "        0.99169719, 0.97411477, 0.99169719, 0.99169719, 1.        ,\n",
       "        0.99169719, 0.97411477, 0.99169719, 0.99169719, 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.89408676, 0.8658498 , 0.85860917, 0.85860917, 0.84991573,\n",
       "        0.86393544, 0.85860917, 0.86393544, 0.88886852, 0.86393544,\n",
       "        0.86393544, 0.85860917, 0.88886852, 0.85343377, 0.85860917,\n",
       "        0.85860917, 0.89419479, 0.86393544, 0.85860917, 0.86393544,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99169719,\n",
       "        1.        , 1.        , 0.99169719, 1.        , 1.        ,\n",
       "        1.        , 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        0.99169719, 1.        , 0.99169719, 0.99169719, 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.85860917, 0.86382741, 0.85875164, 0.85860917, 0.84969281,\n",
       "        0.86393544, 0.85860917, 0.88901099, 0.85343377, 0.86382741,\n",
       "        0.86393544, 0.86393544, 0.86565603, 0.85860917, 0.87995216,\n",
       "        0.85860917, 0.8658498 , 0.85343377, 0.85860917, 0.85860917,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.99169719, 0.99169719, 0.99169719, 0.99169719, 1.        ,\n",
       "        0.99169719, 0.99169719, 1.        , 1.        , 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split4_train_score': array([0.88969284, 0.89271759, 0.82969974, 0.82338276, 0.89216994,\n",
       "        0.83356677, 0.86212735, 0.83605887, 0.91871657, 0.87731847,\n",
       "        0.83722837, 0.8495927 , 0.81706307, 0.83449387, 0.82338276,\n",
       "        0.83625796, 0.86200865, 0.83067314, 0.82765581, 0.88111776,\n",
       "        0.98441948, 0.98441948, 0.98441948, 1.        , 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 1.        ,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98441948,\n",
       "        1.        , 0.98441948, 0.98441948, 0.98441948, 1.        ,\n",
       "        0.84684067, 0.83297305, 0.89496096, 0.87181492, 0.89394816,\n",
       "        0.87181492, 0.82672472, 0.86003589, 0.84154021, 0.8538809 ,\n",
       "        0.89216994, 0.86070381, 0.84797334, 0.89496096, 0.89394816,\n",
       "        0.86003589, 0.84562792, 0.876116  , 0.86658276, 0.88525459,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98441948,\n",
       "        1.        , 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.86912896, 0.8659753 , 0.89800956, 0.88969284, 0.86171998,\n",
       "        0.89496096, 0.88969284, 0.83722837, 0.85027326, 0.86366342,\n",
       "        0.89813709, 0.89216994, 0.88659709, 0.8801546 , 0.88969284,\n",
       "        0.85547165, 0.83098459, 0.85918994, 0.8628973 , 0.86876632,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 1.        , 0.98441948,\n",
       "        0.98441948, 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.98441948,\n",
       "        1.        , 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.84211619, 0.83916215, 0.84436054, 0.81227165, 0.88322692,\n",
       "        0.85820959, 0.80704347, 0.8495927 , 0.8421959 , 0.87219014,\n",
       "        0.81227165, 0.8018497 , 0.83697315, 0.85820959, 0.8495927 ,\n",
       "        0.83021911, 0.8020202 , 0.82114607, 0.8018497 , 0.82114607,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.98441948, 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.85466886, 0.84797334, 0.82653523, 0.85796445, 0.85441931,\n",
       "        0.8552221 , 0.8552221 , 0.8552221 , 0.83603645, 0.85114253,\n",
       "        0.82653523, 0.85796445, 0.86045426, 0.8552221 , 0.84873785,\n",
       "        0.8552221 , 0.8538809 , 0.85796445, 0.82653523, 0.8552221 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.89005811, 0.86758175, 0.8571185 , 0.85796445, 0.89005811,\n",
       "        0.87546934, 0.85441931, 0.8552221 , 0.8571185 , 0.86758175,\n",
       "        0.87188283, 0.8552221 , 0.85648175, 0.86665067, 0.82653523,\n",
       "        0.8552221 , 0.83703888, 0.85441931, 0.86478338, 0.85796445,\n",
       "        0.98441948, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'mean_train_score': array([0.83983185, 0.84414266, 0.82132455, 0.83249047, 0.83979256,\n",
       "        0.8292026 , 0.84090167, 0.84024986, 0.84117037, 0.8386469 ,\n",
       "        0.82297611, 0.82508683, 0.82971656, 0.8266745 , 0.82346886,\n",
       "        0.82656101, 0.83236472, 0.82840017, 0.82493077, 0.83817772,\n",
       "        0.98376914, 0.9968839 , 0.98819037, 0.98985083, 0.99541869,\n",
       "        0.9968839 , 0.98839528, 0.99170685, 0.98633444, 0.98271497,\n",
       "        0.9902512 , 0.9902512 , 0.97979144, 0.96472822, 0.97477846,\n",
       "        0.9731179 , 0.96380802, 0.96916207, 0.97568201, 0.967497  ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.9968839 , 1.        ,\n",
       "        0.9968839 , 0.99379829, 0.99336741, 0.9968839 , 0.99336741,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.9968839 ,\n",
       "        1.        , 0.98985093, 0.98985093, 0.9968839 , 0.99296703,\n",
       "        0.83762544, 0.85559723, 0.84972877, 0.84600973, 0.83571621,\n",
       "        0.84703667, 0.83338933, 0.83530959, 0.8338946 , 0.83266776,\n",
       "        0.87319486, 0.83410462, 0.8487075 , 0.85055834, 0.84942697,\n",
       "        0.84800058, 0.83240296, 0.84765196, 0.84402631, 0.84775127,\n",
       "        0.99375813, 0.99170685, 0.99336741, 0.9968839 , 0.98839528,\n",
       "        0.99522333, 0.99522333, 0.9968839 , 0.9968839 , 0.99170685,\n",
       "        0.99336741, 0.98819037, 0.9811574 , 0.98985093, 0.98358814,\n",
       "        0.97713765, 0.97156119, 0.97606116, 0.98358814, 0.97425832,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99296703, 0.9968839 , 0.9968839 ,\n",
       "        0.9968839 , 0.98985093, 0.9968839 , 0.99336741, 0.99336741,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99648352, 1.        , 0.9968839 ,\n",
       "        1.        , 0.99336741, 0.9968839 , 0.99336741, 0.99336741,\n",
       "        0.82919259, 0.84264868, 0.86067087, 0.84581672, 0.85531546,\n",
       "        0.83824057, 0.84746766, 0.84975269, 0.84250008, 0.84083566,\n",
       "        0.86048711, 0.86713234, 0.83861562, 0.85697226, 0.83832963,\n",
       "        0.83190352, 0.82340567, 0.83862638, 0.85331669, 0.85067357,\n",
       "        0.99170685, 0.9968839 , 0.9968839 , 0.9968839 , 0.99024165,\n",
       "        0.99522333, 0.99336741, 0.99522333, 0.98985093, 0.99375813,\n",
       "        0.98819037, 0.99170685, 0.9811574 , 0.98417062, 0.9811574 ,\n",
       "        0.98467388, 0.97788756, 0.9811574 , 0.97403069, 0.9784111 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99545885, 1.        , 0.9968839 ,\n",
       "        0.9968839 , 0.99117799, 0.99336741, 0.99336741, 0.99336741,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.9968839 ,\n",
       "        1.        , 1.        , 0.9968839 , 0.9968839 , 0.9968839 ,\n",
       "        1.        , 0.98985093, 0.99336741, 0.99170685, 0.9968839 ,\n",
       "        0.83121702, 0.8499426 , 0.84327171, 0.84542749, 0.85603794,\n",
       "        0.85670784, 0.83033077, 0.85572699, 0.84528819, 0.84315171,\n",
       "        0.84107204, 0.85196255, 0.83105532, 0.83672617, 0.85194749,\n",
       "        0.82551525, 0.82555548, 0.83014073, 0.8346075 , 0.84761091,\n",
       "        0.99687424, 0.99521368, 0.99687424, 0.99521368, 0.99501832,\n",
       "        0.9985348 , 0.99521368, 0.9985348 , 0.9868761 , 0.99687424,\n",
       "        0.99521368, 0.99169719, 0.9850646 , 0.9850646 , 0.99375813,\n",
       "        0.99024165, 0.98321824, 0.98155768, 0.98155768, 0.98507416,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.85892742, 0.8367577 , 0.83538432, 0.83622081, 0.85433914,\n",
       "        0.848473  , 0.84741114, 0.85383676, 0.84174823, 0.84673388,\n",
       "        0.84161597, 0.8385103 , 0.85136249, 0.83443318, 0.83601812,\n",
       "        0.84637882, 0.84868766, 0.84052341, 0.83841923, 0.85145776,\n",
       "        1.        , 0.9985348 , 0.9985348 , 1.        , 0.99833944,\n",
       "        1.        , 1.        , 0.99833944, 0.9985348 , 0.9970696 ,\n",
       "        0.9985348 , 0.99833944, 0.99541869, 0.9968839 , 0.9968839 ,\n",
       "        0.99522333, 0.98985093, 0.98819037, 0.98819037, 0.99024165,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.84303654, 0.84683005, 0.85533644, 0.85726645, 0.84613698,\n",
       "        0.85160835, 0.85348566, 0.84092467, 0.840178  , 0.84419642,\n",
       "        0.85184772, 0.84632796, 0.84949097, 0.84324547, 0.83577407,\n",
       "        0.84527625, 0.83436531, 0.84728774, 0.84656791, 0.85005614,\n",
       "        0.99541869, 0.9985348 , 1.        , 1.        , 0.99687424,\n",
       "        0.9985348 , 1.        , 1.        , 0.9985348 , 0.9970696 ,\n",
       "        0.99833944, 0.99833944, 0.99375813, 0.99375813, 0.99541869,\n",
       "        0.99522333, 0.99375813, 0.98985093, 0.99336741, 0.99170685,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       " 'std_train_score': array([0.0378431 , 0.04266231, 0.01778377, 0.0187421 , 0.03347509,\n",
       "        0.01955909, 0.03186562, 0.01801027, 0.043353  , 0.03511494,\n",
       "        0.01689178, 0.02098743, 0.0247622 , 0.0200924 , 0.01370559,\n",
       "        0.01541515, 0.02077762, 0.01919212, 0.01322628, 0.03100871,\n",
       "        0.0089959 , 0.00623221, 0.00682107, 0.00980919, 0.0061884 ,\n",
       "        0.00623221, 0.00996593, 0.00744243, 0.00687663, 0.01308243,\n",
       "        0.00953463, 0.00953463, 0.01286527, 0.01237027, 0.01019947,\n",
       "        0.0094704 , 0.02183319, 0.01042765, 0.00778507, 0.01298429,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00623221, 0.        ,\n",
       "        0.00623221, 0.00583356, 0.00814786, 0.00623221, 0.00814786,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00623221,\n",
       "        0.        , 0.00831886, 0.00831886, 0.00623221, 0.00861359,\n",
       "        0.00649332, 0.02320341, 0.02957695, 0.02420288, 0.03961356,\n",
       "        0.02084921, 0.02362684, 0.02591646, 0.02651452, 0.02408541,\n",
       "        0.02577139, 0.02572117, 0.01118878, 0.04012448, 0.02726868,\n",
       "        0.02113091, 0.02903789, 0.04530991, 0.02162983, 0.03495398,\n",
       "        0.00584047, 0.00744243, 0.00814786, 0.00623221, 0.00996593,\n",
       "        0.0062866 , 0.0062866 , 0.00623221, 0.00623221, 0.00744243,\n",
       "        0.00814786, 0.00682107, 0.00360566, 0.00831886, 0.00474844,\n",
       "        0.00821602, 0.02024183, 0.01020016, 0.00474844, 0.00855437,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00861359, 0.00623221, 0.00623221,\n",
       "        0.00623221, 0.00831886, 0.00623221, 0.00814786, 0.00814786,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00703297, 0.        , 0.00623221,\n",
       "        0.        , 0.00814786, 0.00623221, 0.00814786, 0.00814786,\n",
       "        0.02907717, 0.01926518, 0.03195827, 0.03086336, 0.0360233 ,\n",
       "        0.0357207 , 0.02716111, 0.00840924, 0.02478296, 0.02486416,\n",
       "        0.03613887, 0.02327832, 0.02926285, 0.02590304, 0.03548941,\n",
       "        0.0237198 , 0.02235858, 0.03176556, 0.02851469, 0.02580066,\n",
       "        0.00744243, 0.00623221, 0.00623221, 0.00623221, 0.00990294,\n",
       "        0.0062866 , 0.00814786, 0.0062866 , 0.00831886, 0.00584047,\n",
       "        0.00682107, 0.00744243, 0.00360566, 0.01255272, 0.00360566,\n",
       "        0.00844549, 0.00487989, 0.00360566, 0.01036485, 0.0042683 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00617108, 0.        , 0.00623221,\n",
       "        0.00623221, 0.01082162, 0.00814786, 0.00814786, 0.00814786,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.00623221,\n",
       "        0.        , 0.        , 0.00623221, 0.00623221, 0.00623221,\n",
       "        0.        , 0.00831886, 0.00814786, 0.00744243, 0.00623221,\n",
       "        0.02838524, 0.0257139 , 0.02145369, 0.01901207, 0.02351213,\n",
       "        0.00763195, 0.01920843, 0.01529037, 0.0178758 , 0.03107662,\n",
       "        0.02610204, 0.02799674, 0.02132995, 0.02401749, 0.02059694,\n",
       "        0.01519983, 0.03068955, 0.02435699, 0.02492163, 0.02717551,\n",
       "        0.0038407 , 0.00392426, 0.0038407 , 0.00392426, 0.00406753,\n",
       "        0.0029304 , 0.00392426, 0.0029304 , 0.01128921, 0.0038407 ,\n",
       "        0.00392426, 0.0094654 , 0.00677076, 0.00677076, 0.00584047,\n",
       "        0.00629879, 0.00929358, 0.00645493, 0.00645493, 0.00622761,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02980112, 0.02021182, 0.01597815, 0.0209736 , 0.01147013,\n",
       "        0.02067056, 0.01057638, 0.01463647, 0.02517634, 0.01820333,\n",
       "        0.02525099, 0.01903557, 0.02796617, 0.01844361, 0.02257249,\n",
       "        0.01752563, 0.03196405, 0.02942239, 0.02165167, 0.01331568,\n",
       "        0.        , 0.0029304 , 0.0029304 , 0.        , 0.00332112,\n",
       "        0.        , 0.        , 0.00332112, 0.0029304 , 0.003589  ,\n",
       "        0.0029304 , 0.00332112, 0.0061884 , 0.00623221, 0.00623221,\n",
       "        0.0062866 , 0.00831886, 0.00682107, 0.00682107, 0.00629879,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.04015651, 0.02566807, 0.02681434, 0.01123938, 0.03381585,\n",
       "        0.01608403, 0.01035187, 0.03052936, 0.01405692, 0.01851983,\n",
       "        0.02326668, 0.02083582, 0.02207707, 0.01856527, 0.02439785,\n",
       "        0.01738663, 0.02205503, 0.01852359, 0.02191918, 0.02065912,\n",
       "        0.0061884 , 0.0029304 , 0.        , 0.        , 0.0038407 ,\n",
       "        0.0029304 , 0.        , 0.        , 0.0029304 , 0.003589  ,\n",
       "        0.00332112, 0.00332112, 0.00584047, 0.00584047, 0.0061884 ,\n",
       "        0.0062866 , 0.00584047, 0.00831886, 0.00814786, 0.00744243,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search\n",
    "\n",
    "# Usando um grid completo de todos os parâmetros\n",
    "param_grid = {\"max_depth\": range(3,10,2),\n",
    "              \"min_samples_split\": range(1,6),\n",
    "              \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"n_estimators\": range(100,500,100),\n",
    "              \"bootstrap\": [True, False]}\n",
    "\n",
    "# Executando o Grid Search\n",
    "grid_search = GridSearchCV(modelo4_multi, param_grid = param_grid, return_train_score = True, scoring='f1_macro')\n",
    "start = time.time()\n",
    "grid_search.fit(X_falha_treino, y_falha_treino)\n",
    "\n",
    "print(\"GridSearchCV executou em %.2f segundos para todas as combinações de candidatos a parâmetros do modelo.\"\n",
    "      % (time.time() - start))\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd6baf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'log_loss',\n",
       " 'max_depth': 5,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 400}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d84f432",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7765662507427213"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da00bfc7",
   "metadata": {},
   "source": [
    "A primeira tentativa de otimização dos hiperparâmetros, chegamos a 77,66% de F1 Score. Vamos fazer uma segunda otimização de hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9bf3a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridSearchCV executou em 2125.31 segundos para todas as combinações de candidatos a parâmetros do modelo.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.6382925 , 0.86337924, 0.57826548, 0.70983901, 0.571874  ,\n",
       "        0.71210365, 0.58989124, 0.73408246, 0.59889526, 0.7369154 ,\n",
       "        0.57488723, 0.723105  , 0.58667922, 0.72871199, 0.58188357,\n",
       "        0.7250988 , 0.58628983, 0.76479602, 0.6044445 , 0.76331239,\n",
       "        0.7064805 , 0.7845706 , 0.58929691, 0.73530822, 0.59529271,\n",
       "        0.73371525, 0.59650488, 0.72667532, 0.58269153, 0.7389071 ,\n",
       "        0.5940865 , 0.76409888, 0.75295868, 0.85536556, 0.70210891,\n",
       "        0.81454892, 0.65831013, 0.90626774, 0.66989775, 0.90212126,\n",
       "        0.6170023 , 0.84601102, 0.63369417, 1.06143317, 0.68740082,\n",
       "        0.83952179, 0.62766051, 0.77974248, 0.66195388, 0.76015978,\n",
       "        0.74020085, 0.79356089, 0.59739933, 0.811936  , 0.62449784,\n",
       "        0.77814097, 0.62173967, 0.76629853, 0.6168716 , 0.75740499,\n",
       "        0.61559772, 0.75930037, 0.59636464, 0.77853909, 0.61580095,\n",
       "        0.80820456, 0.70891356, 0.80743051, 0.63451147, 0.76935587,\n",
       "        0.61636415, 0.75853753, 0.59547415, 0.74555736, 0.60840077,\n",
       "        0.75773873, 0.70430059, 0.89869523, 0.69633241, 0.89400234,\n",
       "        0.63519864, 0.8099649 , 0.63693299, 0.76875567, 0.60426855,\n",
       "        0.78294106, 0.61862354, 0.80313983, 0.62375269, 0.88151059,\n",
       "        0.64834814, 0.80895591, 0.64375582, 0.91715188, 0.75078387,\n",
       "        0.81634293, 0.7375185 , 0.84674711, 0.63888679, 0.80865784,\n",
       "        0.62679577, 0.78777037, 0.65789757, 0.79388676, 0.63648791,\n",
       "        0.79274745, 0.63160777, 0.78872266, 0.63713636, 0.77631931,\n",
       "        0.63532624, 0.80447392, 0.62174444, 0.77352591, 0.65447226,\n",
       "        0.79906139, 0.64260001, 0.78595095, 0.6389298 , 0.80355048,\n",
       "        0.61532474, 0.80514565, 0.64212108, 0.77812519, 0.63340068,\n",
       "        0.80594206, 0.64073153, 0.8170361 , 0.66991277, 0.83314314,\n",
       "        0.64172997, 0.80147424, 0.6460145 , 0.78514767, 0.63539848,\n",
       "        0.77285066, 0.77277617, 0.78148661, 0.62194285, 0.77241836,\n",
       "        0.62020025, 0.79079785, 0.60439467, 0.76299882, 0.61819925,\n",
       "        0.7444005 , 0.60039878, 0.75100074, 0.60820007, 0.7495985 ,\n",
       "        0.63059936, 0.76599903, 0.60679731, 0.75500073, 0.60759792,\n",
       "        0.74439926, 0.61739807, 0.743398  , 0.59540186, 0.76559629,\n",
       "        0.58760076, 0.74540038, 0.60039787, 0.75859942, 0.60780125,\n",
       "        0.75379953, 0.60659952, 0.75720081, 0.5725986 , 0.71999984,\n",
       "        0.56280107, 0.70600057, 0.566998  , 0.70540133, 0.58020096,\n",
       "        0.72259994, 0.58520422, 0.728197  , 0.59360013, 0.72260156,\n",
       "        0.58720074, 0.72700067, 0.59539819, 0.72700019, 0.57279797,\n",
       "        0.72799888, 0.5912015 , 0.74260011, 0.5888    , 0.72640262,\n",
       "        0.59459929, 0.74180021, 0.59220147, 0.73860297, 0.58859873,\n",
       "        0.73599958, 0.57619953, 0.72360077, 0.60000134, 0.74199834,\n",
       "        0.67040195, 0.90039864, 0.69760938, 0.86419568, 0.70260196,\n",
       "        0.78679914, 0.59420185, 0.73599935, 0.59099951, 0.80379906,\n",
       "        0.59999881, 0.76639962, 0.60220046, 0.77320085, 0.61440201,\n",
       "        0.75139804, 0.59939938, 0.7411994 , 0.57920008, 0.74719944,\n",
       "        0.58339834, 0.73879886, 0.60480056, 0.82760053, 0.63899937,\n",
       "        0.75200133, 0.60480056, 0.76780086, 0.5972013 , 0.75319996,\n",
       "        0.59080262, 0.75419912, 0.60099931, 0.76379972, 0.60980062,\n",
       "        0.75200076, 0.60580182, 0.75719628, 0.60600119, 0.74999905,\n",
       "        0.59319959, 0.73719983, 0.63360047, 0.73039856, 0.59440069,\n",
       "        0.75299797, 0.6081943 , 0.75860038, 0.62459464, 0.75580111,\n",
       "        0.59420137, 0.81599884, 0.56539841, 0.53100119, 0.4205966 ,\n",
       "        0.53459897, 0.41799893, 0.53661489, 0.43830352, 0.54539866,\n",
       "        0.42360258, 0.53740029, 0.44000087, 0.58339891, 0.43260236,\n",
       "        0.53379908, 0.43280058, 0.53139873, 0.66160164, 0.61400018,\n",
       "        0.47959957, 0.60859981, 0.47440062, 0.65580034, 0.47720103,\n",
       "        0.59519873, 0.50120111, 0.74619608, 0.54139867, 0.58119955,\n",
       "        0.53139739, 0.6323998 , 0.58479795, 0.75280433, 0.58879972,\n",
       "        0.72219954, 0.55539737, 0.60259905, 0.49180298, 0.66240072,\n",
       "        0.58619833, 0.87559466, 0.65860152, 0.8295989 , 0.66359816,\n",
       "        0.8455976 , 0.67219849, 0.83620081, 0.88440118, 0.90279818,\n",
       "        0.73580074, 0.94459791, 0.76159873, 0.95000014, 0.69619675,\n",
       "        0.92879806, 0.71659756, 0.89779902, 0.71519928, 0.88699813,\n",
       "        0.6993989 , 0.90040226, 0.77039742, 1.06279817, 0.81240072,\n",
       "        1.00839653, 0.77719989, 0.94480038, 0.78499913, 0.92120762,\n",
       "        0.72390466, 0.86390324, 0.68079791, 0.86480074, 0.68020082,\n",
       "        0.88699808, 0.67739825, 0.90459728, 0.83519893, 0.94780011,\n",
       "        0.81599889, 1.07799878, 0.73439951, 1.03639965, 1.04293418,\n",
       "        1.05939779, 0.8271997 , 1.03830514, 0.80659909, 1.02759891,\n",
       "        0.80030684, 0.92459846, 0.72480154, 0.94679952, 0.73059945,\n",
       "        0.90879974, 0.7139998 , 0.90179992, 0.70019999, 0.87820239,\n",
       "        0.69279828, 0.85199957, 0.6771986 , 0.88060026, 0.71160107,\n",
       "        0.9075974 , 0.7490026 , 0.89019971, 0.71179967, 0.88060031,\n",
       "        0.69639931, 0.86779966, 0.6949995 , 0.90380034, 0.70919995,\n",
       "        0.87919745, 0.7224    , 0.91599841, 0.73039718, 0.90919967,\n",
       "        0.72559938, 0.95899711, 0.75210443, 0.95320005, 0.74960284,\n",
       "        0.94200191, 0.7493968 , 0.99919825, 0.78119936, 0.97439818,\n",
       "        0.79239955, 0.97119846, 0.77611351, 1.00930624, 0.77110996,\n",
       "        0.95189576, 0.75691338, 0.93892226, 0.75952973, 0.94114122,\n",
       "        0.76891556, 0.95993881, 0.73909106, 0.96387997, 0.72879996,\n",
       "        0.99219837, 0.74920106, 0.87659926, 0.70060101, 0.88340106,\n",
       "        0.7054008 , 0.89039917, 0.75399675, 0.94299965, 0.74039593,\n",
       "        0.91960015, 0.72839913, 0.9153995 , 0.7118001 , 0.93859935,\n",
       "        0.7043993 , 0.89779968, 0.70439858, 0.89739966, 0.73899646,\n",
       "        0.92100143, 0.73119812, 0.97120209, 0.73999968, 0.91819634,\n",
       "        0.65399857, 0.82760105, 0.66380019, 0.81919632, 0.70599408,\n",
       "        0.83959827, 0.68119721, 0.85639873, 0.70600042, 0.84819775,\n",
       "        0.68479838, 0.90839791, 0.68459802, 0.84899807, 0.67980151,\n",
       "        0.85139847, 0.6831974 , 0.86719818, 0.7187974 , 0.9119988 ,\n",
       "        0.74479938, 0.88839898, 0.7171957 , 0.8785984 , 0.67920074,\n",
       "        0.85879898, 0.74140153, 0.97779884, 0.7940011 , 0.99119964,\n",
       "        0.80239978, 1.0334002 , 0.71859994, 0.8958003 , 0.73919845,\n",
       "        0.90279636, 0.70380106, 0.87799616, 0.73240685, 0.93319826,\n",
       "        0.700597  , 0.93919859, 0.72499981, 0.91699657, 0.73020005,\n",
       "        0.91020122, 0.72879772, 0.91199751, 0.71380019, 0.97879939,\n",
       "        0.80400023, 0.99659524, 0.78480139, 1.02820168, 0.76319809,\n",
       "        0.91860137, 0.74660048, 0.94460001, 0.76600046, 0.91959734,\n",
       "        0.71560073, 0.88659878, 0.70959859, 0.88679934, 0.70699959,\n",
       "        0.89220243, 0.77739792, 0.91979718, 0.73219543, 0.9245997 ,\n",
       "        0.75120006, 0.91599979, 0.70999966, 0.88819847, 0.72279949,\n",
       "        0.94119873, 0.70699968, 0.90479774, 0.7444006 , 0.92979803,\n",
       "        0.66259623, 0.62820392, 0.50160041, 0.62499614]),\n",
       " 'std_fit_time': array([0.07604025, 0.09937098, 0.00847858, 0.01233632, 0.00309048,\n",
       "        0.00497831, 0.01227845, 0.01099797, 0.0512327 , 0.01571366,\n",
       "        0.00343701, 0.00919662, 0.00714038, 0.00661996, 0.01022935,\n",
       "        0.00429225, 0.00874479, 0.03316816, 0.01942444, 0.04692597,\n",
       "        0.06722106, 0.08309297, 0.0057853 , 0.00619578, 0.01270001,\n",
       "        0.0055053 , 0.01724626, 0.00481147, 0.00882493, 0.00762975,\n",
       "        0.0023731 , 0.02172963, 0.03343798, 0.06187456, 0.056027  ,\n",
       "        0.012653  , 0.04728347, 0.11785874, 0.03452641, 0.09898362,\n",
       "        0.01425703, 0.05045873, 0.03007081, 0.19082547, 0.04194316,\n",
       "        0.04831326, 0.02521208, 0.03167641, 0.02681156, 0.02061928,\n",
       "        0.03782561, 0.02822015, 0.00861792, 0.08844848, 0.00701272,\n",
       "        0.02271706, 0.01096173, 0.01137503, 0.01376293, 0.0052001 ,\n",
       "        0.01249923, 0.01525468, 0.01037296, 0.0416124 , 0.02406207,\n",
       "        0.09333892, 0.07109323, 0.05948874, 0.02470536, 0.02095565,\n",
       "        0.01164383, 0.00799738, 0.00318314, 0.01251065, 0.01114291,\n",
       "        0.02360737, 0.05764952, 0.03716482, 0.03057263, 0.04973383,\n",
       "        0.01824744, 0.03128053, 0.01558192, 0.00863935, 0.01258642,\n",
       "        0.01249702, 0.01830492, 0.04914964, 0.0279673 , 0.04372627,\n",
       "        0.02212606, 0.02332247, 0.02655406, 0.02923209, 0.0826945 ,\n",
       "        0.06485062, 0.09177249, 0.07156733, 0.01375878, 0.03083411,\n",
       "        0.01240142, 0.00788556, 0.03376564, 0.01360365, 0.01128247,\n",
       "        0.01331749, 0.01614471, 0.0152335 , 0.01346346, 0.01297185,\n",
       "        0.02766492, 0.03512705, 0.02214109, 0.01716167, 0.03325301,\n",
       "        0.00904108, 0.01566753, 0.01068628, 0.01611137, 0.00915052,\n",
       "        0.00340167, 0.02362449, 0.03975419, 0.00345184, 0.01880042,\n",
       "        0.02641436, 0.01180625, 0.02897602, 0.03939334, 0.03383842,\n",
       "        0.01588402, 0.01111645, 0.0177182 , 0.00646468, 0.0169554 ,\n",
       "        0.0218202 , 0.08106827, 0.05371753, 0.01244982, 0.02183105,\n",
       "        0.02053529, 0.06806843, 0.00739406, 0.00698749, 0.02179308,\n",
       "        0.01153504, 0.01324615, 0.01567117, 0.02590444, 0.02177815,\n",
       "        0.02057803, 0.01429706, 0.00945351, 0.00672325, 0.01214474,\n",
       "        0.00427169, 0.02907088, 0.01013122, 0.01373449, 0.02314967,\n",
       "        0.00875365, 0.01290835, 0.01322967, 0.00739273, 0.0166437 ,\n",
       "        0.00312459, 0.01059287, 0.01589471, 0.00870819, 0.0203969 ,\n",
       "        0.00567035, 0.01269633, 0.01082537, 0.01605628, 0.00958031,\n",
       "        0.00605377, 0.01506513, 0.02022018, 0.02018656, 0.01583152,\n",
       "        0.01146344, 0.00751106, 0.02659815, 0.00963328, 0.00851536,\n",
       "        0.01315807, 0.01149768, 0.01500364, 0.01119393, 0.01023083,\n",
       "        0.01774936, 0.01079612, 0.01775679, 0.01141249, 0.00656116,\n",
       "        0.01283925, 0.00770523, 0.00852318, 0.0058296 , 0.00787342,\n",
       "        0.11481221, 0.03561137, 0.01542773, 0.00837783, 0.00717668,\n",
       "        0.06988743, 0.00890869, 0.01523192, 0.01099208, 0.08524761,\n",
       "        0.00296771, 0.0157782 , 0.01254733, 0.02782225, 0.01620849,\n",
       "        0.01189066, 0.00662166, 0.01373095, 0.00411701, 0.01764467,\n",
       "        0.0072276 , 0.01590227, 0.00730528, 0.0389999 , 0.03478383,\n",
       "        0.01021717, 0.01057207, 0.02427906, 0.01281522, 0.02399505,\n",
       "        0.00343064, 0.01190889, 0.03165474, 0.05293444, 0.00640258,\n",
       "        0.00695628, 0.00741442, 0.00951613, 0.00881018, 0.00414678,\n",
       "        0.00702798, 0.01285838, 0.03579707, 0.0117728 , 0.00996904,\n",
       "        0.03101389, 0.01020611, 0.01710734, 0.03230462, 0.01657009,\n",
       "        0.00785836, 0.0733495 , 0.13017289, 0.00623055, 0.00971009,\n",
       "        0.00933015, 0.009465  , 0.01574796, 0.01083041, 0.01172652,\n",
       "        0.00488187, 0.00520058, 0.01756221, 0.04931661, 0.01323283,\n",
       "        0.00661443, 0.00676498, 0.0088468 , 0.13597119, 0.10669581,\n",
       "        0.02272953, 0.05199369, 0.02239379, 0.05043998, 0.01826782,\n",
       "        0.02352496, 0.03727069, 0.12117121, 0.09060157, 0.03008995,\n",
       "        0.09199667, 0.04830382, 0.02526953, 0.05809312, 0.03995419,\n",
       "        0.2057508 , 0.12071743, 0.01665591, 0.05672217, 0.10737535,\n",
       "        0.08790657, 0.04376651, 0.03101999, 0.03766507, 0.00671137,\n",
       "        0.02069536, 0.00754391, 0.01522953, 0.11097888, 0.07412977,\n",
       "        0.05601753, 0.04289378, 0.05587796, 0.03386564, 0.01760085,\n",
       "        0.06627341, 0.01750972, 0.00639761, 0.01346698, 0.0116265 ,\n",
       "        0.01751306, 0.02877792, 0.02979036, 0.06189563, 0.05744993,\n",
       "        0.10146784, 0.0471597 , 0.04480811, 0.05187981, 0.03765422,\n",
       "        0.04391818, 0.0075328 , 0.01230273, 0.01663092, 0.0050735 ,\n",
       "        0.03920373, 0.01238793, 0.04188362, 0.04614445, 0.05140733,\n",
       "        0.06498893, 0.13549305, 0.02712096, 0.10506484, 0.31535746,\n",
       "        0.04948871, 0.03983883, 0.0419799 , 0.07213411, 0.0376107 ,\n",
       "        0.07697668, 0.06029278, 0.00791016, 0.04717397, 0.01105777,\n",
       "        0.01602643, 0.00582955, 0.04102418, 0.03222169, 0.04136709,\n",
       "        0.01756511, 0.01608779, 0.01517109, 0.04985014, 0.00773448,\n",
       "        0.02044018, 0.02628508, 0.02531588, 0.01202669, 0.01383807,\n",
       "        0.01325978, 0.01334916, 0.01829878, 0.04293797, 0.02503991,\n",
       "        0.03068284, 0.00571307, 0.02614668, 0.00382558, 0.01522249,\n",
       "        0.01523772, 0.03991191, 0.05982912, 0.01470349, 0.00791419,\n",
       "        0.0110987 , 0.01105092, 0.06262978, 0.00483316, 0.01112883,\n",
       "        0.00913466, 0.00830316, 0.00624951, 0.03679479, 0.02357803,\n",
       "        0.00262405, 0.00582305, 0.00776185, 0.01605123, 0.03068213,\n",
       "        0.00959479, 0.02607884, 0.01589933, 0.04457813, 0.0109448 ,\n",
       "        0.07106561, 0.0523361 , 0.0077362 , 0.00553686, 0.01200889,\n",
       "        0.00733499, 0.02616619, 0.03289404, 0.04316473, 0.01711607,\n",
       "        0.01083666, 0.00960399, 0.0084055 , 0.01553835, 0.04397425,\n",
       "        0.00999329, 0.01282725, 0.01515944, 0.03225243, 0.00994276,\n",
       "        0.01056472, 0.00866044, 0.03813025, 0.01513951, 0.00730492,\n",
       "        0.00995896, 0.00922387, 0.01446998, 0.00736211, 0.03416386,\n",
       "        0.02832616, 0.00679554, 0.01070583, 0.01586259, 0.00877036,\n",
       "        0.00736256, 0.03337409, 0.00987194, 0.00654295, 0.0099452 ,\n",
       "        0.0156757 , 0.01289074, 0.0350882 , 0.00604469, 0.04223605,\n",
       "        0.04409938, 0.00821245, 0.02019289, 0.01661054, 0.00788412,\n",
       "        0.0176109 , 0.0491533 , 0.03128805, 0.02107734, 0.03301099,\n",
       "        0.034184  , 0.03185502, 0.01232155, 0.00798721, 0.03746777,\n",
       "        0.04084132, 0.00891071, 0.02151296, 0.07061813, 0.0678916 ,\n",
       "        0.00864049, 0.05313871, 0.00952868, 0.00912074, 0.00570863,\n",
       "        0.00661466, 0.01601761, 0.00948764, 0.01213988, 0.058406  ,\n",
       "        0.02211686, 0.0145721 , 0.00752129, 0.03237676, 0.04474696,\n",
       "        0.00746891, 0.01784943, 0.03574939, 0.03511714, 0.01089321,\n",
       "        0.00952105, 0.01061327, 0.00922253, 0.01275073, 0.00981933,\n",
       "        0.02711636, 0.02669912, 0.01297855, 0.0066133 , 0.03021996,\n",
       "        0.02843604, 0.01200164, 0.00405062, 0.01316547, 0.00685373,\n",
       "        0.03876707, 0.00596889, 0.02459378, 0.00909083, 0.0073572 ,\n",
       "        0.10489322, 0.014235  , 0.0068592 , 0.00839072]),\n",
       " 'mean_score_time': array([0.03880129, 0.04919963, 0.03822579, 0.04607997, 0.03762498,\n",
       "        0.04621258, 0.03960485, 0.04642959, 0.03720112, 0.05820789,\n",
       "        0.03700738, 0.04640164, 0.03660583, 0.04720497, 0.03680353,\n",
       "        0.04581141, 0.03760595, 0.051408  , 0.03676739, 0.04920592,\n",
       "        0.04239964, 0.04779997, 0.03779764, 0.05840874, 0.03820481,\n",
       "        0.04660535, 0.03739624, 0.04500866, 0.03660192, 0.04641104,\n",
       "        0.0386096 , 0.04581571, 0.04860182, 0.04779749, 0.03938746,\n",
       "        0.04762249, 0.03941135, 0.05320406, 0.04499788, 0.04746437,\n",
       "        0.03839722, 0.04700251, 0.03761749, 0.057792  , 0.04139719,\n",
       "        0.047399  , 0.03780093, 0.04560313, 0.03859892, 0.04699931,\n",
       "        0.04379797, 0.04659858, 0.03680038, 0.04580073, 0.03819909,\n",
       "        0.04599762, 0.03699985, 0.04639907, 0.03679876, 0.04659591,\n",
       "        0.03780141, 0.04500113, 0.03799944, 0.0464045 , 0.03833065,\n",
       "        0.04740243, 0.03892345, 0.05240035, 0.03760009, 0.04719987,\n",
       "        0.03819895, 0.04619985, 0.03859644, 0.04700212, 0.0357995 ,\n",
       "        0.05140181, 0.04390459, 0.04839921, 0.0537993 , 0.04739881,\n",
       "        0.03700194, 0.04740338, 0.03880639, 0.0498013 , 0.03739986,\n",
       "        0.04439993, 0.03720608, 0.04659815, 0.0379981 , 0.04700041,\n",
       "        0.04060016, 0.04519963, 0.04679775, 0.04919486, 0.0430028 ,\n",
       "        0.04799976, 0.04420033, 0.0482008 , 0.03680382, 0.04720235,\n",
       "        0.03840184, 0.04600377, 0.03719625, 0.04701238, 0.03900023,\n",
       "        0.04719934, 0.03779793, 0.04740119, 0.0406033 , 0.04640136,\n",
       "        0.03880343, 0.04700217, 0.03700142, 0.0472014 , 0.04020076,\n",
       "        0.04660406, 0.0382009 , 0.04539852, 0.03739996, 0.04720182,\n",
       "        0.03619857, 0.04900126, 0.03699899, 0.04660239, 0.03680172,\n",
       "        0.060008  , 0.03799982, 0.04792662, 0.03719854, 0.04800439,\n",
       "        0.03799849, 0.04620194, 0.03700109, 0.04640141, 0.03880138,\n",
       "        0.04660487, 0.05059967, 0.04540138, 0.03839798, 0.04600334,\n",
       "        0.0360003 , 0.04520144, 0.03900037, 0.0472034 , 0.03660026,\n",
       "        0.04800096, 0.03799977, 0.04419928, 0.03759899, 0.0470015 ,\n",
       "        0.03900104, 0.04540119, 0.03660078, 0.04580107, 0.03639998,\n",
       "        0.0444006 , 0.03700004, 0.04519935, 0.03699861, 0.04620175,\n",
       "        0.03660111, 0.04519911, 0.03620005, 0.04660187, 0.03619809,\n",
       "        0.04519992, 0.03880205, 0.045399  , 0.03579936, 0.04540043,\n",
       "        0.03719831, 0.04699912, 0.03540068, 0.04459867, 0.03699937,\n",
       "        0.0435997 , 0.03659687, 0.04639959, 0.03659959, 0.04600053,\n",
       "        0.03739648, 0.04659824, 0.03700128, 0.04439969, 0.03640013,\n",
       "        0.04559884, 0.03759837, 0.04499764, 0.03639932, 0.04639821,\n",
       "        0.03700094, 0.04720097, 0.03739848, 0.04579868, 0.03600006,\n",
       "        0.04539919, 0.036799  , 0.04600124, 0.03759999, 0.05960255,\n",
       "        0.03899832, 0.04880066, 0.03739004, 0.04759622, 0.03760095,\n",
       "        0.0449985 , 0.03759861, 0.04619884, 0.03799973, 0.04859958,\n",
       "        0.03720074, 0.04599924, 0.03799882, 0.0593977 , 0.03859978,\n",
       "        0.04640241, 0.03939857, 0.0449986 , 0.03660169, 0.04620085,\n",
       "        0.03620086, 0.04699821, 0.03719907, 0.04500113, 0.0372045 ,\n",
       "        0.04619951, 0.03640141, 0.04540129, 0.03660021, 0.04519968,\n",
       "        0.0377974 , 0.04620047, 0.03780189, 0.04679804, 0.03759961,\n",
       "        0.04599929, 0.03740067, 0.04560122, 0.03819981, 0.04680057,\n",
       "        0.05059996, 0.04560232, 0.04099908, 0.04419966, 0.03660111,\n",
       "        0.04559855, 0.03720007, 0.04579997, 0.03679934, 0.04419971,\n",
       "        0.03619938, 0.04819984, 0.04240336, 0.04699888, 0.03600159,\n",
       "        0.04640017, 0.03819981, 0.04638634, 0.03520198, 0.0458005 ,\n",
       "        0.03539729, 0.04519916, 0.03720064, 0.0445991 , 0.03779817,\n",
       "        0.04699917, 0.03659859, 0.04480319, 0.05621347, 0.04599905,\n",
       "        0.03760028, 0.0463985 , 0.03740177, 0.05400085, 0.03639812,\n",
       "        0.04660158, 0.03639798, 0.06380005, 0.04799728, 0.04940062,\n",
       "        0.04200177, 0.04799881, 0.03819933, 0.05439291, 0.03980145,\n",
       "        0.05320015, 0.0412003 , 0.04800014, 0.04039865, 0.05060225,\n",
       "        0.04500046, 0.06300359, 0.04920225, 0.05940046, 0.05020261,\n",
       "        0.0605988 , 0.04900002, 0.06039891, 0.07060122, 0.06040158,\n",
       "        0.05319943, 0.06240072, 0.05079904, 0.06139841, 0.0496007 ,\n",
       "        0.06380091, 0.0494009 , 0.06360426, 0.0506011 , 0.06259995,\n",
       "        0.05040002, 0.06159945, 0.05220394, 0.0712029 , 0.05619764,\n",
       "        0.07360168, 0.05279689, 0.06620378, 0.05239677, 0.06260085,\n",
       "        0.0569983 , 0.06140208, 0.04980054, 0.05999885, 0.04819946,\n",
       "        0.06360312, 0.04959974, 0.06560044, 0.06140208, 0.06539869,\n",
       "        0.0566031 , 0.07880144, 0.05180197, 0.06399941, 0.06939778,\n",
       "        0.07200136, 0.0609993 , 0.07580209, 0.05400195, 0.06600089,\n",
       "        0.05560012, 0.06140213, 0.04919724, 0.06380343, 0.04939976,\n",
       "        0.0625998 , 0.05180149, 0.06879935, 0.04959822, 0.06340117,\n",
       "        0.04920058, 0.06000175, 0.04999967, 0.0618032 , 0.04740076,\n",
       "        0.05979886, 0.05279899, 0.05959802, 0.04840002, 0.05919895,\n",
       "        0.04880099, 0.06019874, 0.04840155, 0.06259975, 0.05100245,\n",
       "        0.06020079, 0.04859834, 0.06039987, 0.04800014, 0.06120095,\n",
       "        0.04880147, 0.06019969, 0.05140123, 0.06340036, 0.0507987 ,\n",
       "        0.06199851, 0.05139923, 0.06400127, 0.05040112, 0.06220183,\n",
       "        0.05179739, 0.0627995 , 0.04979901, 0.06501079, 0.05104671,\n",
       "        0.06602154, 0.05263057, 0.06240273, 0.05020137, 0.06321001,\n",
       "        0.05220027, 0.06422949, 0.04961886, 0.07043195, 0.04980049,\n",
       "        0.07660022, 0.04839964, 0.06160078, 0.04879889, 0.06099863,\n",
       "        0.04880118, 0.0599999 , 0.04880223, 0.06060038, 0.04980354,\n",
       "        0.06020017, 0.04860435, 0.06040339, 0.04880257, 0.06339951,\n",
       "        0.04920287, 0.06100049, 0.04900136, 0.06060228, 0.04980221,\n",
       "        0.06059856, 0.04800134, 0.06199837, 0.04920144, 0.06020079,\n",
       "        0.04779978, 0.06000099, 0.04899907, 0.05959964, 0.0514061 ,\n",
       "        0.06080198, 0.04760213, 0.05960498, 0.04899983, 0.06360059,\n",
       "        0.04800081, 0.06280165, 0.04780059, 0.05920024, 0.04860196,\n",
       "        0.06120076, 0.04860077, 0.06099858, 0.04840055, 0.0623992 ,\n",
       "        0.04840059, 0.0602026 , 0.04920306, 0.06020207, 0.04880109,\n",
       "        0.05960069, 0.04980016, 0.06460004, 0.05259929, 0.06400199,\n",
       "        0.05240045, 0.06599755, 0.04780087, 0.06080103, 0.05120125,\n",
       "        0.06020083, 0.0496006 , 0.05980239, 0.05139227, 0.06359892,\n",
       "        0.04900103, 0.06340046, 0.04860172, 0.06040201, 0.0493988 ,\n",
       "        0.05999813, 0.04820151, 0.05980058, 0.04939904, 0.06399984,\n",
       "        0.05320086, 0.06400619, 0.05220151, 0.06599545, 0.04939938,\n",
       "        0.06019831, 0.04759955, 0.06200438, 0.04879999, 0.05920205,\n",
       "        0.04940162, 0.06099892, 0.04859982, 0.06000099, 0.0497973 ,\n",
       "        0.06239986, 0.05220037, 0.06060061, 0.05000453, 0.06080108,\n",
       "        0.04920197, 0.06019912, 0.04899869, 0.06060095, 0.04879909,\n",
       "        0.06220093, 0.04880075, 0.06019826, 0.04820242, 0.06160021,\n",
       "        0.04699898, 0.04579859, 0.03799901, 0.04820094]),\n",
       " 'std_score_time': array([1.72206699e-03, 3.65417353e-03, 3.34093643e-03, 1.70338564e-03,\n",
       "        1.35990502e-03, 2.30991841e-03, 2.58362668e-03, 4.01489358e-03,\n",
       "        7.47900430e-04, 2.54853013e-02, 1.42134040e-03, 2.64754175e-03,\n",
       "        8.05781999e-04, 2.31726408e-03, 1.83165416e-03, 2.13195559e-03,\n",
       "        1.02411288e-03, 8.47193504e-03, 1.63885461e-03, 4.01961837e-03,\n",
       "        4.17598966e-03, 2.48249269e-03, 1.71973624e-03, 2.78191113e-02,\n",
       "        3.06119308e-03, 1.74785324e-03, 1.50657667e-03, 1.66862318e-03,\n",
       "        1.62903077e-03, 1.49571187e-03, 1.49908951e-03, 1.31912040e-03,\n",
       "        4.02954938e-03, 2.04090688e-03, 3.26969199e-03, 7.51882229e-04,\n",
       "        1.35061890e-03, 9.49021956e-03, 8.00105367e-03, 2.17985061e-03,\n",
       "        1.01519619e-03, 1.99712515e-03, 7.92697890e-04, 1.29557653e-02,\n",
       "        3.49495751e-03, 5.35196643e-03, 2.64032900e-03, 2.05870680e-03,\n",
       "        2.33498087e-03, 1.99980758e-03, 4.70695698e-03, 1.74306304e-03,\n",
       "        1.16557176e-03, 1.32425848e-03, 1.46807143e-03, 2.09949000e-03,\n",
       "        8.92017943e-04, 1.20180675e-03, 1.46874459e-03, 4.93276565e-04,\n",
       "        1.60085144e-03, 1.09598197e-03, 1.89861246e-03, 2.41515962e-03,\n",
       "        1.79717845e-03, 1.01907792e-03, 1.95166035e-03, 9.15617807e-03,\n",
       "        1.35653271e-03, 2.63633681e-03, 1.47055665e-03, 7.47171835e-04,\n",
       "        1.35656737e-03, 1.54988346e-03, 7.46739124e-04, 8.82170527e-03,\n",
       "        1.06790546e-02, 1.01824620e-03, 3.06034425e-02, 1.62405575e-03,\n",
       "        1.09689361e-03, 1.96385077e-03, 1.32069100e-03, 1.94143566e-03,\n",
       "        1.62467404e-03, 8.00466090e-04, 1.93653417e-03, 7.99455962e-04,\n",
       "        1.67112138e-03, 2.44949155e-03, 4.79757775e-03, 1.72017287e-03,\n",
       "        1.52615741e-02, 1.94187952e-03, 4.60443722e-03, 2.82657090e-03,\n",
       "        1.00114620e-02, 2.47951452e-03, 3.99424108e-04, 1.16168860e-03,\n",
       "        7.99502399e-04, 1.66844275e-03, 1.32675219e-03, 9.03673137e-04,\n",
       "        1.41411410e-03, 2.48591547e-03, 1.60339863e-03, 1.85909447e-03,\n",
       "        3.07150352e-03, 1.35586204e-03, 7.46952914e-04, 2.09447455e-03,\n",
       "        8.95804279e-04, 1.16618227e-03, 4.70881996e-03, 1.95956014e-03,\n",
       "        1.16674749e-03, 1.36030270e-03, 4.87981188e-04, 1.93967231e-03,\n",
       "        1.16794996e-03, 2.45130015e-03, 8.94539493e-04, 7.99673267e-04,\n",
       "        9.83834509e-04, 2.70172848e-02, 1.67399922e-03, 1.37804183e-03,\n",
       "        7.47372618e-04, 1.67394488e-03, 8.94682501e-04, 7.49800588e-04,\n",
       "        1.41671425e-03, 1.02033874e-03, 2.31657747e-03, 1.49482489e-03,\n",
       "        1.75809399e-02, 7.99403671e-04, 2.15416875e-03, 1.09351693e-03,\n",
       "        8.95545725e-04, 2.40146030e-03, 1.67396848e-03, 1.32933983e-03,\n",
       "        1.85290405e-03, 1.26670660e-03, 1.89798312e-03, 1.16363782e-03,\n",
       "        2.05546881e-03, 1.89398745e-03, 2.36646222e-03, 1.96089543e-03,\n",
       "        1.02042429e-03, 2.13455839e-03, 1.49668192e-03, 1.74627915e-03,\n",
       "        1.67336977e-03, 1.47043416e-03, 1.79045997e-03, 1.59859254e-03,\n",
       "        1.62306544e-03, 1.72034220e-03, 7.45779855e-04, 1.01977794e-03,\n",
       "        9.77389274e-04, 1.72160529e-03, 3.12618616e-03, 1.49286047e-03,\n",
       "        7.50335225e-04, 2.05941400e-03, 2.03856921e-03, 1.26708266e-03,\n",
       "        1.85335018e-03, 1.02019979e-03, 1.89499368e-03, 7.98216196e-04,\n",
       "        1.35833687e-03, 2.41602467e-03, 1.02140633e-03, 2.09746604e-03,\n",
       "        1.01714992e-03, 2.94126197e-03, 1.09471587e-03, 1.95951304e-03,\n",
       "        1.49709065e-03, 7.97863813e-04, 1.74294772e-03, 1.26632934e-03,\n",
       "        1.49552469e-03, 1.35697192e-03, 1.41212435e-03, 3.24908104e-03,\n",
       "        2.05795838e-03, 1.60016656e-03, 8.94362381e-04, 1.35588298e-03,\n",
       "        1.33000775e-03, 1.41374590e-03, 3.32305307e-03, 2.67468120e-02,\n",
       "        1.89828723e-03, 2.31246678e-03, 1.02887658e-03, 2.87239688e-03,\n",
       "        1.74061724e-03, 1.41162090e-03, 1.74193733e-03, 1.16667241e-03,\n",
       "        6.31889529e-04, 1.35503270e-03, 2.63726012e-03, 2.45009169e-03,\n",
       "        1.78883161e-03, 2.63495547e-02, 3.71913472e-03, 1.85662004e-03,\n",
       "        3.38458578e-03, 8.93244508e-04, 1.62585171e-03, 1.83212799e-03,\n",
       "        1.60013676e-03, 2.27740510e-03, 9.79948975e-04, 1.67414426e-03,\n",
       "        1.32550912e-03, 1.59788814e-03, 1.35703632e-03, 2.15399193e-03,\n",
       "        7.97205627e-04, 1.47003112e-03, 3.98498362e-04, 1.59967554e-03,\n",
       "        1.60299603e-03, 2.31605214e-03, 1.62391766e-03, 1.99911666e-03,\n",
       "        1.35544277e-03, 1.85515606e-03, 2.55868655e-03, 2.31441530e-03,\n",
       "        2.72828358e-02, 1.85277039e-03, 1.89647681e-03, 9.79968558e-04,\n",
       "        4.90604551e-04, 7.99336330e-04, 1.46985988e-03, 1.16245946e-03,\n",
       "        1.16575512e-03, 9.80453300e-04, 1.16850564e-03, 4.21508805e-03,\n",
       "        4.45736951e-03, 1.09658665e-03, 1.26696939e-03, 1.01974873e-03,\n",
       "        1.83292866e-03, 1.36294013e-03, 3.99021180e-04, 2.48169732e-03,\n",
       "        1.02222323e-03, 2.03964257e-03, 2.04308565e-03, 4.88753817e-04,\n",
       "        1.72197738e-03, 1.89853644e-03, 1.35578867e-03, 1.46929767e-03,\n",
       "        2.74576604e-02, 1.78718311e-03, 2.57637837e-03, 1.85447195e-03,\n",
       "        1.62300641e-03, 9.93833101e-03, 1.20161103e-03, 1.35723309e-03,\n",
       "        1.02136461e-03, 2.31704228e-02, 1.71459257e-02, 1.08209929e-02,\n",
       "        3.84782984e-03, 3.52242496e-03, 1.94070950e-03, 9.39016487e-03,\n",
       "        3.12438195e-03, 8.06002256e-03, 6.04772999e-03, 2.28058307e-03,\n",
       "        4.58668730e-03, 5.85312300e-03, 5.17639531e-03, 4.51438853e-03,\n",
       "        1.16740090e-03, 1.35640403e-03, 3.18725824e-03, 2.24605962e-03,\n",
       "        2.09660224e-03, 7.99074493e-04, 1.73971716e-02, 1.95945146e-03,\n",
       "        2.71426885e-03, 1.01650419e-03, 2.31216614e-03, 1.85454000e-03,\n",
       "        1.19804682e-03, 3.43024981e-03, 1.36039091e-03, 3.26420037e-03,\n",
       "        3.32398815e-03, 4.75752148e-03, 1.95670660e-03, 1.62328194e-03,\n",
       "        2.64080344e-03, 8.63476921e-03, 4.48569083e-03, 1.00921053e-02,\n",
       "        4.70694862e-03, 3.60225721e-03, 4.54468993e-03, 2.15491210e-03,\n",
       "        5.99926357e-03, 1.35693861e-03, 1.60245929e-03, 8.93457809e-04,\n",
       "        7.46405061e-04, 3.26265654e-03, 2.24368546e-03, 7.05979847e-03,\n",
       "        6.40476476e-03, 5.81934563e-03, 6.74044850e-03, 2.82530892e-02,\n",
       "        4.83073895e-03, 3.57888954e-03, 2.12484357e-02, 1.54819189e-03,\n",
       "        5.21406998e-03, 6.85370806e-03, 2.44917732e-03, 2.75538195e-03,\n",
       "        7.11629833e-03, 1.36033617e-03, 7.48063213e-04, 1.83593834e-03,\n",
       "        8.01855618e-04, 2.15438663e-03, 3.43180525e-03, 1.96519108e-02,\n",
       "        2.33215591e-03, 3.61263785e-03, 1.47139334e-03, 8.91860287e-04,\n",
       "        2.60524317e-03, 3.65453316e-03, 4.90817972e-04, 7.48179285e-04,\n",
       "        6.43142587e-03, 7.98490687e-04, 4.87473302e-04, 4.00857592e-04,\n",
       "        7.48532319e-04, 9.81380447e-04, 4.89532146e-04, 1.01608570e-03,\n",
       "        3.03500878e-03, 7.50602222e-04, 1.01943332e-03, 7.99848909e-04,\n",
       "        1.41404449e-03, 2.04269835e-03, 1.16571032e-03, 1.72120061e-03,\n",
       "        3.66978125e-03, 2.57730734e-03, 2.03794147e-03, 8.94312083e-04,\n",
       "        2.86849268e-03, 3.16770369e-03, 1.95954832e-03, 1.94068757e-03,\n",
       "        1.93944663e-03, 1.47008028e-03, 7.49046422e-04, 4.51212321e-03,\n",
       "        1.44166564e-03, 6.65082302e-03, 5.78427042e-03, 1.85402911e-03,\n",
       "        1.32792000e-03, 2.31458908e-03, 5.07319389e-03, 5.50447240e-03,\n",
       "        7.87265088e-04, 1.04676210e-02, 2.13455504e-03, 3.31999540e-02,\n",
       "        1.96073090e-03, 2.72833028e-03, 3.99756584e-04, 2.19164665e-03,\n",
       "        7.49931821e-04, 6.32337662e-04, 1.16734244e-03, 1.19912170e-03,\n",
       "        1.59952929e-03, 1.16602613e-03, 4.92048229e-04, 8.01542433e-04,\n",
       "        4.00681171e-04, 3.07094420e-03, 3.99878194e-04, 2.09651165e-03,\n",
       "        6.32560923e-04, 4.88903247e-04, 1.94167298e-03, 1.35312701e-03,\n",
       "        5.30854908e-06, 3.28537414e-03, 1.16586199e-03, 9.79211716e-04,\n",
       "        7.48700901e-04, 1.10207215e-03, 2.09553368e-03, 1.01885284e-03,\n",
       "        2.15403531e-03, 3.25159297e-03, 4.89164588e-04, 1.01655313e-03,\n",
       "        2.00209659e-03, 8.75703190e-03, 1.09845718e-03, 5.15162266e-03,\n",
       "        9.79433858e-04, 1.16811005e-03, 1.35816207e-03, 1.71749227e-03,\n",
       "        4.91327802e-04, 1.78899151e-03, 4.89864808e-04, 4.40964855e-03,\n",
       "        1.01858949e-03, 1.47091385e-03, 1.32889765e-03, 1.72411815e-03,\n",
       "        2.13591751e-03, 4.89570793e-04, 2.92916821e-03, 3.71995376e-03,\n",
       "        3.19819123e-03, 2.75554091e-03, 2.72823513e-03, 4.14732141e-03,\n",
       "        3.98934135e-04, 7.50696885e-04, 3.12239742e-03, 9.78571376e-04,\n",
       "        8.01095111e-04, 7.47021432e-04, 3.54575917e-03, 5.74284780e-03,\n",
       "        1.09563098e-03, 3.07069315e-03, 1.02123186e-03, 1.01811765e-03,\n",
       "        4.89964336e-04, 8.95855920e-04, 7.48728749e-04, 7.48826315e-04,\n",
       "        1.74103176e-03, 4.47172682e-03, 2.13787900e-03, 2.09967179e-03,\n",
       "        1.16875711e-03, 2.09880749e-03, 7.95605663e-04, 1.47136981e-03,\n",
       "        4.90281823e-04, 1.41411163e-03, 1.83513207e-03, 7.46906278e-04,\n",
       "        2.33608775e-03, 1.54991250e-03, 4.90253130e-04, 6.34597001e-04,\n",
       "        7.51962628e-04, 1.49971675e-03, 1.71977613e-03, 4.90349449e-04,\n",
       "        2.53205381e-03, 3.18510565e-03, 1.94027978e-03, 2.92826441e-03,\n",
       "        6.30602180e-04, 4.89844311e-04, 4.00112221e-04, 7.47271859e-04,\n",
       "        4.00569194e-04, 7.46753106e-04, 1.16607617e-03, 1.74278272e-03,\n",
       "        1.00578549e-02, 1.83214987e-03, 1.41299998e-03, 2.78344604e-03]),\n",
       " 'param_bootstrap': masked_array(data=[True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    True, True, True, True, True, True, True, True, True,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_criterion': masked_array(data=['gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'gini', 'gini',\n",
       "                    'gini', 'gini', 'gini', 'gini', 'gini', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'entropy', 'entropy',\n",
       "                    'entropy', 'entropy', 'entropy', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss', 'log_loss',\n",
       "                    'log_loss', 'log_loss', 'log_loss'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "                    6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7,\n",
       "                    7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
       "                    9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
       "                    6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n",
       "                    9, 9, 9, 9, 9, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5,\n",
       "                    5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
       "                    7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=['sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'log2', 'log2', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'log2', 'log2', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'sqrt', 'sqrt', 'log2', 'log2', 'log2', 'log2',\n",
       "                    'log2', 'log2', 'sqrt', 'sqrt', 'sqrt', 'sqrt', 'sqrt',\n",
       "                    'sqrt', 'log2', 'log2', 'log2', 'log2', 'log2', 'log2'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_samples_split': masked_array(data=[1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3,\n",
       "                    1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 1, 2, 2, 3, 3],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500, 400, 500,\n",
       "                    400, 500, 400, 500, 400, 500, 400, 500, 400, 500, 400,\n",
       "                    500, 400, 500, 400, 500, 400, 500, 400, 500],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': True,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'gini',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'entropy',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 3,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 4,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 5,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 6,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 7,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 8,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'sqrt',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 1,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 2,\n",
       "   'n_estimators': 500},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 400},\n",
       "  {'bootstrap': False,\n",
       "   'criterion': 'log_loss',\n",
       "   'max_depth': 9,\n",
       "   'max_features': 'log2',\n",
       "   'min_samples_split': 3,\n",
       "   'n_estimators': 500}],\n",
       " 'split0_test_score': array([0.65863636, 0.65863636, 0.65863636, 0.65809524, 0.63652174,\n",
       "        0.65863636, 0.80974747, 0.80974747, 0.80724556, 0.65863636,\n",
       "        0.80974747, 0.80724556, 0.83077498, 0.83077498, 0.65863636,\n",
       "        0.67966387, 0.67966387, 0.67966387, 0.83077498, 0.83077498,\n",
       "        0.83077498, 0.80974747, 0.83077498, 0.82888889, 0.83077498,\n",
       "        0.7997619 , 0.6529972 , 0.67966387, 0.77077498, 0.77077498,\n",
       "        0.83077498, 0.78601307, 0.70833333, 0.7997619 , 0.78601307,\n",
       "        0.82888889, 0.73142857, 0.83077498, 0.61378788, 0.70833333,\n",
       "        0.67515152, 0.85292929, 0.76253968, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.70833333, 0.78601307, 0.7397619 , 0.755     ,\n",
       "        0.77077498, 0.61378788, 0.73142857, 0.755     , 0.78601307,\n",
       "        0.73934641, 0.73934641, 0.70646027, 0.73934641, 0.73934641,\n",
       "        0.64156863, 0.80920635, 0.73934641, 0.63688312, 0.7397619 ,\n",
       "        0.80920635, 0.73934641, 0.73934641, 0.78601307, 0.78601307,\n",
       "        0.73934641, 0.70833333, 0.80920635, 0.73934641, 0.76253968,\n",
       "        0.69142857, 0.76253968, 0.70833333, 0.78601307, 0.78601307,\n",
       "        0.73934641, 0.80920635, 0.80920635, 0.78601307, 0.67777778,\n",
       "        0.65613445, 0.63652174, 0.65613445, 0.63652174, 0.67777778,\n",
       "        0.82888889, 0.82888889, 0.80724556, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.83077498, 0.67777778, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.76888889, 0.77077498,\n",
       "        0.83077498, 0.7397619 , 0.6529972 , 0.82888889, 0.78397661,\n",
       "        0.78601307, 0.73730994, 0.82888889, 0.76888889, 0.82888889,\n",
       "        0.77077498, 0.77077498, 0.83077498, 0.83077498, 0.77077498,\n",
       "        0.76888889, 0.78397661, 0.78397661, 0.77077498, 0.78601307,\n",
       "        0.78397661, 0.73934641, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.85292929, 0.83077498, 0.76888889, 0.78601307, 0.78601307,\n",
       "        0.83077498, 0.78601307, 0.78397661, 0.78601307, 0.80920635,\n",
       "        0.73934641, 0.83077498, 0.77077498, 0.77077498, 0.83077498,\n",
       "        0.83077498, 0.78601307, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.78397661, 0.80920635, 0.78601307, 0.73934641, 0.66823529,\n",
       "        0.755     , 0.77077498, 0.78601307, 0.78397661, 0.78397661,\n",
       "        0.73934641, 0.78397661, 0.78601307, 0.65613445, 0.63652174,\n",
       "        0.80724556, 0.63652174, 0.65613445, 0.65863636, 0.80724556,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.80724556, 0.82888889,\n",
       "        0.67966387, 0.67966387, 0.67777778, 0.67777778, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.83077498, 0.76888889, 0.83077498,\n",
       "        0.83077498, 0.83077498, 0.83077498, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.83077498,\n",
       "        0.83077498, 0.83077498, 0.65188312, 0.77077498, 0.67515152,\n",
       "        0.78397661, 0.78601307, 0.73730994, 0.78601307, 0.78397661,\n",
       "        0.83077498, 0.7397619 , 0.77077498, 0.80920635, 0.78601307,\n",
       "        0.77077498, 0.73934641, 0.78601307, 0.73934641, 0.78397661,\n",
       "        0.73934641, 0.82888889, 0.78397661, 0.73934641, 0.73934641,\n",
       "        0.78601307, 0.67966387, 0.83077498, 0.77077498, 0.78397661,\n",
       "        0.78601307, 0.78601307, 0.78397661, 0.78601307, 0.78397661,\n",
       "        0.76515873, 0.77077498, 0.83077498, 0.73934641, 0.77077498,\n",
       "        0.77077498, 0.78601307, 0.78601307, 0.78601307, 0.78601307,\n",
       "        0.78601307, 0.77077498, 0.65863636, 0.65863636, 0.80724556,\n",
       "        0.65863636, 0.65863636, 0.66186869, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.83077498,\n",
       "        0.80974747, 0.83077498, 0.80974747, 0.83077498, 0.83077498,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.77077498, 0.77077498, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73730994, 0.73730994, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.70603175, 0.68430184, 0.67777778, 0.70603175,\n",
       "        0.70603175, 0.67777778, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.76888889, 0.82888889, 0.82888889,\n",
       "        0.73730994, 0.77077498, 0.78601307, 0.77077498, 0.76888889,\n",
       "        0.77077498, 0.78601307, 0.78601307, 0.78397661, 0.78601307,\n",
       "        0.78397661, 0.78601307, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.78601307, 0.78601307,\n",
       "        0.78601307, 0.78601307, 0.73934641, 0.78601307, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.78601307, 0.73934641, 0.73934641, 0.78601307, 0.73934641,\n",
       "        0.78601307, 0.78601307, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.73934641, 0.78601307,\n",
       "        0.78601307, 0.78601307, 0.73934641, 0.73934641, 0.78601307,\n",
       "        0.76253968, 0.76253968, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.67777778, 0.70603175, 0.67777778, 0.70603175, 0.70603175,\n",
       "        0.67777778, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.82888889,\n",
       "        0.82888889, 0.82888889, 0.82888889, 0.82888889, 0.77077498,\n",
       "        0.83077498, 0.76888889, 0.82888889, 0.78397661, 0.78397661,\n",
       "        0.78601307, 0.73934641, 0.78601307, 0.78601307, 0.73934641,\n",
       "        0.78601307, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.78601307, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.78601307, 0.78601307, 0.78601307,\n",
       "        0.73934641, 0.73934641, 0.78601307, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.78601307, 0.73934641, 0.78601307, 0.78601307,\n",
       "        0.78601307, 0.73934641, 0.73934641, 0.73934641, 0.73934641,\n",
       "        0.73934641, 0.73934641, 0.73934641, 0.78601307, 0.78601307,\n",
       "        0.73934641, 0.78601307, 0.78601307, 0.73934641]),\n",
       " 'split1_test_score': array([0.65377778, 0.66833333, 0.59351648, 0.646     , 0.62208791,\n",
       "        0.65377778, 0.63878788, 0.69812253, 0.85288443, 0.85288443,\n",
       "        0.66833333, 0.69812253, 0.63878788, 0.646     , 0.61431169,\n",
       "        0.66833333, 0.68953964, 0.61431169, 0.84642857, 0.84642857,\n",
       "        0.84642857, 0.84642857, 0.85288443, 0.82273292, 0.63878788,\n",
       "        0.63878788, 0.64285714, 0.66175889, 0.69166667, 0.63800764,\n",
       "        0.82273292, 0.84642857, 0.84642857, 0.84642857, 0.84642857,\n",
       "        0.84642857, 0.69166667, 0.66797101, 0.66175889, 0.63878788,\n",
       "        0.69166667, 0.66797101, 0.82273292, 0.84642857, 0.82273292,\n",
       "        0.84642857, 0.82273292, 0.84642857, 0.66797101, 0.84642857,\n",
       "        0.66797101, 0.66797101, 0.60952381, 0.66797101, 0.79761905,\n",
       "        0.82273292, 0.82273292, 0.84642857, 0.82273292, 0.79761905,\n",
       "        0.66797101, 0.66797101, 0.66797101, 0.64285714, 0.66797101,\n",
       "        0.66797101, 0.82273292, 0.82273292, 0.84642857, 0.84642857,\n",
       "        0.84642857, 0.84642857, 0.64285714, 0.66797101, 0.69166667,\n",
       "        0.66797101, 0.66797101, 0.66797101, 0.81860806, 0.84642857,\n",
       "        0.82273292, 0.84642857, 0.82273292, 0.79276955, 0.66833333,\n",
       "        0.66833333, 0.646     , 0.646     , 0.66833333, 0.66833333,\n",
       "        0.84430155, 0.82309524, 0.79354978, 0.8165208 , 0.85288443,\n",
       "        0.82435786, 0.68953964, 0.6968599 , 0.66175889, 0.66833333,\n",
       "        0.66175889, 0.66833333, 0.84642857, 0.84430155, 0.87481538,\n",
       "        0.81860806, 0.84642857, 0.84642857, 0.66175889, 0.68953964,\n",
       "        0.79354978, 0.66797101, 0.68953964, 0.8165208 , 0.87481538,\n",
       "        0.84642857, 0.87481538, 0.81860806, 0.81860806, 0.84642857,\n",
       "        0.60952381, 0.66833333, 0.66797101, 0.66384615, 0.68953964,\n",
       "        0.66797101, 0.82273292, 0.81860806, 0.81860806, 0.81860806,\n",
       "        0.82273292, 0.81860806, 0.63878788, 0.66384615, 0.84642857,\n",
       "        0.69166667, 0.69166667, 0.72005348, 0.81860806, 0.81860806,\n",
       "        0.81860806, 0.84642857, 0.81860806, 0.82273292, 0.84642857,\n",
       "        0.66797101, 0.63463768, 0.66797101, 0.69166667, 0.66384615,\n",
       "        0.79276955, 0.84642857, 0.81860806, 0.84642857, 0.81860806,\n",
       "        0.84642857, 0.64285714, 0.69166667, 0.64130435, 0.68953964,\n",
       "        0.66384615, 0.66175889, 0.81860806, 0.84642857, 0.81860806,\n",
       "        0.87481538, 0.81860806, 0.81860806, 0.66833333, 0.646     ,\n",
       "        0.66833333, 0.66833333, 0.66833333, 0.66833333, 0.84430155,\n",
       "        0.79674051, 0.80338083, 0.79674051, 0.8165208 , 0.79674051,\n",
       "        0.68953964, 0.66833333, 0.66175889, 0.68953964, 0.66175889,\n",
       "        0.66833333, 0.87481538, 0.81860806, 0.84642857, 0.84642857,\n",
       "        0.84642857, 0.84642857, 0.8165208 , 0.66833333, 0.66175889,\n",
       "        0.63463768, 0.68953964, 0.66833333, 0.87481538, 0.84642857,\n",
       "        0.81860806, 0.84642857, 0.87481538, 0.84642857, 0.72005348,\n",
       "        0.63878788, 0.69166667, 0.66175889, 0.63463768, 0.69166667,\n",
       "        0.81860806, 0.79276955, 0.81860806, 0.84642857, 0.84642857,\n",
       "        0.81860806, 0.69812253, 0.69812253, 0.66833333, 0.66797101,\n",
       "        0.66175889, 0.82273292, 0.84642857, 0.81860806, 0.81860806,\n",
       "        0.84642857, 0.81860806, 0.84642857, 0.66384615, 0.84642857,\n",
       "        0.63800764, 0.64285714, 0.66797101, 0.84642857, 0.81860806,\n",
       "        0.84642857, 0.81860806, 0.81860806, 0.81860806, 0.84642857,\n",
       "        0.66797101, 0.69166667, 0.66797101, 0.63800764, 0.66797101,\n",
       "        0.66797101, 0.81860806, 0.79276955, 0.81860806, 0.84642857,\n",
       "        0.84642857, 0.81860806, 0.61431169, 0.61431169, 0.61431169,\n",
       "        0.61431169, 0.61431169, 0.61431169, 0.71904762, 0.75021645,\n",
       "        0.71603759, 0.71904762, 0.71603759, 0.71603759, 0.63878788,\n",
       "        0.63463768, 0.61212121, 0.66175889, 0.76991342, 0.63878788,\n",
       "        0.77142857, 0.84908425, 0.84908425, 0.77142857, 0.77142857,\n",
       "        0.80952381, 0.66384615, 0.84642857, 0.66175889, 0.8165208 ,\n",
       "        0.78939959, 0.63463768, 0.84908425, 0.84908425, 0.84908425,\n",
       "        0.87857143, 0.84908425, 0.84908425, 0.81860806, 0.81860806,\n",
       "        0.66384615, 0.66384615, 0.66384615, 0.81860806, 0.84908425,\n",
       "        0.87857143, 0.87857143, 0.87857143, 0.84908425, 0.87857143,\n",
       "        0.81      , 0.69166667, 0.81860806, 0.81860806, 0.81860806,\n",
       "        0.89809524, 0.87857143, 0.87857143, 0.87857143, 0.89809524,\n",
       "        0.87857143, 0.87857143, 0.66384615, 0.89809524, 0.81      ,\n",
       "        0.81860806, 0.66384615, 0.81860806, 0.89809524, 0.87857143,\n",
       "        0.87857143, 0.87857143, 0.87857143, 0.91714286, 0.81860806,\n",
       "        0.66384615, 0.69166667, 0.81860806, 0.81860806, 0.81860806,\n",
       "        0.84908425, 0.87857143, 0.85324675, 0.87857143, 0.87857143,\n",
       "        0.87857143, 0.646     , 0.646     , 0.646     , 0.646     ,\n",
       "        0.646     , 0.646     , 0.82742857, 0.8007619 , 0.82309524,\n",
       "        0.84430155, 0.8007619 , 0.82309524, 0.82309524, 0.8007619 ,\n",
       "        0.8007619 , 0.82309524, 0.8007619 , 0.8007619 , 0.90753968,\n",
       "        0.82309524, 0.82435786, 0.82309524, 0.82991342, 0.8007619 ,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.93773292, 0.85288443, 0.93773292, 0.85288443,\n",
       "        0.82991342, 0.85288443, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.85288443, 0.82309524, 0.93773292, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.93773292,\n",
       "        0.85288443, 0.82309524, 0.85288443, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.82273292,\n",
       "        0.85288443, 0.93773292, 0.85288443, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.93773292, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.646     , 0.8007619 , 0.82309524, 0.8007619 , 0.646     ,\n",
       "        0.646     , 0.80338083, 0.82691024, 0.80542125, 0.84430155,\n",
       "        0.82309524, 0.8007619 , 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.8007619 , 0.8007619 ,\n",
       "        0.88520635, 0.8007619 , 0.8007619 , 0.8007619 , 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.82309524, 0.82309524, 0.82309524,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.93773292, 0.82309524, 0.82309524, 0.82309524, 0.85288443,\n",
       "        0.85288443, 0.82309524, 0.85288443, 0.85288443, 0.93773292,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443, 0.82309524,\n",
       "        0.82309524, 0.82309524, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.93773292, 0.85288443, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.79354978, 0.82309524,\n",
       "        0.82309524, 0.93773292, 0.85288443, 0.93773292, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.82309524, 0.79354978, 0.79354978,\n",
       "        0.85288443, 0.85288443, 0.82309524, 0.85288443, 0.85288443,\n",
       "        0.85288443, 0.85288443, 0.85288443, 0.85288443]),\n",
       " 'split2_test_score': array([0.628815  , 0.59823529, 0.59823529, 0.58823529, 0.59823529,\n",
       "        0.59823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.628815  , 0.628815  , 0.628815  ,\n",
       "        0.628815  , 0.628815  , 0.628815  , 0.67214834, 0.66338681,\n",
       "        0.66338681, 0.66338681, 0.66338681, 0.67214834, 0.66338681,\n",
       "        0.66338681, 0.66338681, 0.66338681, 0.66338681, 0.63672014,\n",
       "        0.63672014, 0.81338681, 0.75338681, 0.63672014, 0.66338681,\n",
       "        0.63672014, 0.75338681, 0.6129972 , 0.693863  , 0.63672014,\n",
       "        0.66338681, 0.63672014, 0.63672014, 0.75338681, 0.63672014,\n",
       "        0.81338681, 0.63672014, 0.64548167, 0.66338681, 0.63672014,\n",
       "        0.63672014, 0.66338681, 0.81338681, 0.63672014, 0.75338681,\n",
       "        0.75338681, 0.75338681, 0.63672014, 0.75338681, 0.75338681,\n",
       "        0.75338681, 0.66338681, 0.63672014, 0.63672014, 0.67204482,\n",
       "        0.66338681, 0.75338681, 0.75338681, 0.63672014, 0.75338681,\n",
       "        0.75338681, 0.66338681, 0.63672014, 0.63672014, 0.63672014,\n",
       "        0.66338681, 0.66338681, 0.66338681, 0.68502415, 0.75338681,\n",
       "        0.75338681, 0.76920635, 0.63672014, 0.67204482, 0.59823529,\n",
       "        0.59823529, 0.63823529, 0.59823529, 0.59823529, 0.59823529,\n",
       "        0.67214834, 0.67214834, 0.67214834, 0.63823529, 0.67214834,\n",
       "        0.63823529, 0.63823529, 0.628815  , 0.628815  , 0.63823529,\n",
       "        0.63823529, 0.628815  , 0.81338681, 0.67214834, 0.843863  ,\n",
       "        0.67214834, 0.81338681, 0.81338681, 0.778815  , 0.75338681,\n",
       "        0.81338681, 0.64548167, 0.66338681, 0.843863  , 0.843863  ,\n",
       "        0.843863  , 0.843863  , 0.81338681, 0.81338681, 0.81338681,\n",
       "        0.843863  , 0.78871148, 0.75338681, 0.693863  , 0.75338681,\n",
       "        0.81338681, 0.843863  , 0.75338681, 0.78871148, 0.75338681,\n",
       "        0.64548167, 0.843863  , 0.843863  , 0.75338681, 0.75338681,\n",
       "        0.75338681, 0.843863  , 0.843863  , 0.78871148, 0.75338681,\n",
       "        0.75338681, 0.64548167, 0.64548167, 0.843863  , 0.63672014,\n",
       "        0.75338681, 0.78871148, 0.75338681, 0.75338681, 0.693863  ,\n",
       "        0.78871148, 0.78871148, 0.843863  , 0.79292929, 0.79292929,\n",
       "        0.88340548, 0.75338681, 0.78871148, 0.693863  , 0.63672014,\n",
       "        0.75338681, 0.81338681, 0.81338681, 0.75338681, 0.79292929,\n",
       "        0.75338681, 0.75338681, 0.81338681, 0.59823529, 0.59823529,\n",
       "        0.63823529, 0.63823529, 0.59823529, 0.59823529, 0.63823529,\n",
       "        0.63823529, 0.67214834, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.628815  , 0.63823529, 0.63823529, 0.67214834, 0.778815  ,\n",
       "        0.67214834, 0.67214834, 0.843863  , 0.81338681, 0.843863  ,\n",
       "        0.70187166, 0.81338681, 0.81338681, 0.75338681, 0.81338681,\n",
       "        0.81338681, 0.66338681, 0.66338681, 0.81338681, 0.75338681,\n",
       "        0.70187166, 0.81338681, 0.78871148, 0.843863  , 0.78871148,\n",
       "        0.75338681, 0.693863  , 0.75338681, 0.66338681, 0.75338681,\n",
       "        0.78871148, 0.75338681, 0.78871148, 0.75338681, 0.843863  ,\n",
       "        0.78871148, 0.75338681, 0.78871148, 0.75338681, 0.78871148,\n",
       "        0.81338681, 0.66338681, 0.75338681, 0.843863  , 0.64548167,\n",
       "        0.75338681, 0.75338681, 0.81338681, 0.843863  , 0.75338681,\n",
       "        0.843863  , 0.75338681, 0.75338681, 0.843863  , 0.68005348,\n",
       "        0.78871148, 0.64548167, 0.78871148, 0.75338681, 0.75338681,\n",
       "        0.75338681, 0.81338681, 0.75338681, 0.81338681, 0.843863  ,\n",
       "        0.75338681, 0.75338681, 0.75338681, 0.64548167, 0.81338681,\n",
       "        0.81338681, 0.75338681, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.63823529, 0.63823529, 0.63823529, 0.63823529,\n",
       "        0.63823529, 0.67214834, 0.63823529, 0.67214834, 0.63823529,\n",
       "        0.71169082, 0.67214834, 0.71169082, 0.71169082, 0.71169082,\n",
       "        0.71169082, 0.67633053, 0.70187166, 0.843863  , 0.843863  ,\n",
       "        0.843863  , 0.843863  , 0.68502415, 0.65511841, 0.69466089,\n",
       "        0.68005348, 0.74141414, 0.68502415, 0.72966387, 0.843863  ,\n",
       "        0.76368984, 0.843863  , 0.81823529, 0.81823529, 0.66141414,\n",
       "        0.69466089, 0.69466089, 0.69466089, 0.71959596, 0.69466089,\n",
       "        0.76368984, 0.67204482, 0.76368984, 0.76368984, 0.78871148,\n",
       "        0.67633053, 0.66141414, 0.66141414, 0.71959596, 0.69466089,\n",
       "        0.69466089, 0.66141414, 0.76368984, 0.65511841, 0.76368984,\n",
       "        0.76368984, 0.81823529, 0.78871148, 0.69466089, 0.69466089,\n",
       "        0.69466089, 0.66141414, 0.69466089, 0.69466089, 0.76368984,\n",
       "        0.76368984, 0.76368984, 0.76368984, 0.81823529, 0.76368984,\n",
       "        0.69466089, 0.66141414, 0.69466089, 0.69466089, 0.66141414,\n",
       "        0.66141414, 0.63823529, 0.67777778, 0.67777778, 0.63823529,\n",
       "        0.67777778, 0.67777778, 0.71169082, 0.71169082, 0.71169082,\n",
       "        0.71169082, 0.67777778, 0.71169082, 0.85292929, 0.66724638,\n",
       "        0.71169082, 0.70181818, 0.67777778, 0.71169082, 0.74141414,\n",
       "        0.74141414, 0.74141414, 0.74141414, 0.74141414, 0.74141414,\n",
       "        0.88340548, 0.843863  , 0.88340548, 0.843863  , 0.88340548,\n",
       "        0.85292929, 0.74141414, 0.74141414, 0.71959596, 0.71959596,\n",
       "        0.88340548, 0.71959596, 0.85777778, 0.843863  , 0.88340548,\n",
       "        0.88340548, 0.843863  , 0.88340548, 0.71959596, 0.82825397,\n",
       "        0.71959596, 0.71959596, 0.71959596, 0.71959596, 0.81823529,\n",
       "        0.71047619, 0.82825397, 0.82825397, 0.88340548, 0.82825397,\n",
       "        0.82825397, 0.82825397, 0.71959596, 0.82825397, 0.88340548,\n",
       "        0.82825397, 0.80323232, 0.82825397, 0.82825397, 0.82825397,\n",
       "        0.65142857, 0.80323232, 0.71959596, 0.82825397, 0.71959596,\n",
       "        0.71959596, 0.71959596, 0.82825397, 0.82825397, 0.80323232,\n",
       "        0.88340548, 0.80323232, 0.82825397, 0.78871148, 0.71959596,\n",
       "        0.71959596, 0.71959596, 0.71959596, 0.82825397, 0.71959596,\n",
       "        0.63823529, 0.63823529, 0.67777778, 0.63823529, 0.67777778,\n",
       "        0.67777778, 0.67777778, 0.71169082, 0.71169082, 0.67777778,\n",
       "        0.71169082, 0.67777778, 0.71169082, 0.81338681, 0.71169082,\n",
       "        0.67777778, 0.71169082, 0.85292929, 0.74141414, 0.74141414,\n",
       "        0.74141414, 0.74141414, 0.71169082, 0.74141414, 0.82825397,\n",
       "        0.88340548, 0.88340548, 0.88340548, 0.88340548, 0.88340548,\n",
       "        0.71959596, 0.71959596, 0.82825397, 0.74141414, 0.88340548,\n",
       "        0.88340548, 0.88340548, 0.82825397, 0.80323232, 0.73229437,\n",
       "        0.88340548, 0.88340548, 0.71959596, 0.88340548, 0.71959596,\n",
       "        0.88340548, 0.71959596, 0.71959596, 0.80323232, 0.82825397,\n",
       "        0.85777778, 0.82825397, 0.88340548, 0.85777778, 0.88340548,\n",
       "        0.71959596, 0.71959596, 0.71959596, 0.82825397, 0.71959596,\n",
       "        0.82825397, 0.82825397, 0.85777778, 0.82825397, 0.82825397,\n",
       "        0.82825397, 0.71959596, 0.88340548, 0.71959596, 0.71959596,\n",
       "        0.82825397, 0.88340548, 0.82825397, 0.78871148, 0.80323232,\n",
       "        0.82825397, 0.88340548, 0.82825397, 0.82825397, 0.88340548,\n",
       "        0.71959596, 0.82825397, 0.88340548, 0.82825397]),\n",
       " 'split3_test_score': array([0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.64571429, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.6738756 ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.6738756 ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.61341615, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.61341615, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.64571429, 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.61341615,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.68      , 0.64571429, 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.68      , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.64080201, 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.61341615, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.61341615, 0.61341615,\n",
       "        0.61341615, 0.61341615, 0.61341615, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.60545455, 0.68      ,\n",
       "        0.68      , 0.68      , 0.64571429, 0.64571429, 0.68      ,\n",
       "        0.68      , 0.64571429, 0.64571429, 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.6738756 , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.6738756 , 0.68      , 0.64080201,\n",
       "        0.6738756 , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64080201, 0.64080201,\n",
       "        0.68      , 0.6738756 , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.60545455, 0.60545455, 0.61341615, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.61341615, 0.61341615, 0.60545455,\n",
       "        0.61341615, 0.61341615, 0.60545455, 0.60545455, 0.60545455,\n",
       "        0.60545455, 0.60545455, 0.60545455, 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.64571429,\n",
       "        0.64571429, 0.64571429, 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.64571429, 0.68      , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.6738756 , 0.6738756 , 0.68      , 0.64571429, 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      , 0.68      ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.6738756 , 0.64080201,\n",
       "        0.68      , 0.68      , 0.68      , 0.6738756 , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.6738756 , 0.6738756 ,\n",
       "        0.6738756 , 0.6738756 , 0.6738756 , 0.68      , 0.68      ,\n",
       "        0.68      , 0.68      , 0.68      , 0.68      ]),\n",
       " 'split4_test_score': array([0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61206349, 0.66239316, 0.61984127,\n",
       "        0.61984127, 0.61239316, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.61239316, 0.61239316, 0.61984127, 0.61239316,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.61239316, 0.61239316, 0.61984127,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.6557265 , 0.61206349,\n",
       "        0.61984127, 0.61239316, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.66555556, 0.66555556,\n",
       "        0.61206349, 0.61984127, 0.6557265 , 0.61984127, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61239316, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.61984127, 0.61239316,\n",
       "        0.61984127, 0.61239316, 0.61239316, 0.61239316, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.61239316, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.6557265 , 0.66555556, 0.61984127, 0.61239316, 0.61239316,\n",
       "        0.61984127, 0.61239316, 0.61239316, 0.61239316, 0.61984127,\n",
       "        0.61984127, 0.66555556, 0.61984127, 0.66555556, 0.6557265 ,\n",
       "        0.61984127, 0.61984127, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.61239316, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.66555556, 0.66239316,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.66555556, 0.61984127, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61239316, 0.61984127,\n",
       "        0.61984127, 0.61984127, 0.61984127, 0.61239316, 0.61984127,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.66555556, 0.61984127,\n",
       "        0.61984127, 0.61239316, 0.61984127, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.66555556, 0.61984127, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61239316, 0.61239316, 0.61984127,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61984127, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.66555556, 0.66555556, 0.6557265 , 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61239316, 0.61239316, 0.61239316, 0.61239316,\n",
       "        0.61239316, 0.61239316, 0.66239316, 0.65461538, 0.70017094,\n",
       "        0.66239316, 0.70017094, 0.70017094, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.61206349,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.61206349,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.69920635, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.61984127, 0.66239316,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61206349, 0.66555556, 0.66555556,\n",
       "        0.66239316, 0.61984127, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66555556, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.61984127, 0.64495425, 0.64495425, 0.64495425,\n",
       "        0.64495425, 0.66239316, 0.64495425, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.61984127,\n",
       "        0.66555556, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.61984127, 0.66239316, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66239316, 0.66239316,\n",
       "        0.61984127, 0.66239316, 0.66239316, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.61984127, 0.66239316, 0.61984127, 0.61984127,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.61984127, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.61984127, 0.61984127,\n",
       "        0.66239316, 0.61984127, 0.61984127, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.64495425, 0.64495425, 0.64495425, 0.64495425,\n",
       "        0.64495425, 0.64495425, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.66555556, 0.66555556, 0.61984127, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66239316, 0.61984127, 0.66239316, 0.66555556, 0.66555556,\n",
       "        0.61984127, 0.66555556, 0.66555556, 0.66555556, 0.66239316,\n",
       "        0.66239316, 0.66239316, 0.66239316, 0.66239316, 0.66239316,\n",
       "        0.66555556, 0.66555556, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.61984127, 0.66239316, 0.61984127, 0.61984127,\n",
       "        0.66239316, 0.66239316, 0.66555556, 0.66555556, 0.66555556,\n",
       "        0.66555556, 0.66555556, 0.61984127, 0.61984127, 0.61984127,\n",
       "        0.66239316, 0.61984127, 0.66239316, 0.61984127]),\n",
       " 'mean_test_score': array([0.63330499, 0.63010016, 0.61513679, 0.62352527, 0.61642815,\n",
       "        0.62718905, 0.66241329, 0.67428022, 0.70473222, 0.67501038,\n",
       "        0.66832238, 0.67377984, 0.67278668, 0.66617716, 0.62541177,\n",
       "        0.6404216 , 0.64466287, 0.62806172, 0.73834901, 0.72808633,\n",
       "        0.72808633, 0.7223912 , 0.7293775 , 0.72472228, 0.68655819,\n",
       "        0.68949843, 0.66095934, 0.67007302, 0.6942768 , 0.66906881,\n",
       "        0.71801386, 0.74764432, 0.72010838, 0.71655038, 0.71764432,\n",
       "        0.72237577, 0.69526466, 0.68231689, 0.66299307, 0.66587938,\n",
       "        0.66600925, 0.7006352 , 0.70287718, 0.72631099, 0.69972815,\n",
       "        0.73831099, 0.69203591, 0.7140633 , 0.6741922 , 0.71674085,\n",
       "        0.68420434, 0.65814025, 0.69997895, 0.68104934, 0.72465754,\n",
       "        0.72157186, 0.72157186, 0.69640043, 0.72157186, 0.71654908,\n",
       "        0.6816964 , 0.69722395, 0.67791862, 0.65043738, 0.67436825,\n",
       "        0.68808109, 0.72157186, 0.72157186, 0.71231099, 0.73564432,\n",
       "        0.72631099, 0.70210838, 0.67623536, 0.67791862, 0.68729641,\n",
       "        0.66296998, 0.67874776, 0.67508353, 0.71789731, 0.73441944,\n",
       "        0.72157186, 0.74344689, 0.71221051, 0.70864412, 0.63392844,\n",
       "        0.62959978, 0.62921057, 0.62513311, 0.63482009, 0.63392844,\n",
       "        0.71412692, 0.70988566, 0.6996479 , 0.70178816, 0.7158435 ,\n",
       "        0.70335557, 0.68553479, 0.67597192, 0.66895172, 0.68129352,\n",
       "        0.671213  , 0.64004439, 0.75770911, 0.72903601, 0.76948171,\n",
       "        0.72389731, 0.75770911, 0.75770911, 0.71100367, 0.70499425,\n",
       "        0.74751057, 0.67975403, 0.66115298, 0.75782279, 0.75900963,\n",
       "        0.75522918, 0.7496763 , 0.75065538, 0.73865538, 0.75770911,\n",
       "        0.70480061, 0.70553221, 0.71039481, 0.69766508, 0.7118514 ,\n",
       "        0.71916045, 0.74859314, 0.72967293, 0.73409754, 0.73008022,\n",
       "        0.70891687, 0.73884213, 0.71370104, 0.70042698, 0.72780061,\n",
       "        0.72674185, 0.7355149 , 0.72652933, 0.73714516, 0.73008022,\n",
       "        0.74052222, 0.7140633 , 0.7080919 , 0.74900043, 0.71843927,\n",
       "        0.68525196, 0.71993594, 0.69839481, 0.7122768 , 0.70484213,\n",
       "        0.74241946, 0.74419888, 0.73884213, 0.73421949, 0.73798872,\n",
       "        0.76273039, 0.70105831, 0.72238936, 0.66652424, 0.65886727,\n",
       "        0.7035577 , 0.70915239, 0.74208022, 0.73523703, 0.73758142,\n",
       "        0.73198835, 0.72967293, 0.74208022, 0.62959978, 0.62121057,\n",
       "        0.667822  , 0.63526956, 0.62959978, 0.63924302, 0.71152602,\n",
       "        0.6978321 , 0.70594277, 0.69942442, 0.69745949, 0.6978321 ,\n",
       "        0.64466287, 0.64230566, 0.64061356, 0.65295231, 0.69895172,\n",
       "        0.67893328, 0.73513878, 0.75824024, 0.75085197, 0.76380435,\n",
       "        0.72854893, 0.75770911, 0.76124763, 0.69809006, 0.71429525,\n",
       "        0.71572815, 0.6898514 , 0.69246728, 0.76189685, 0.74570911,\n",
       "        0.72984198, 0.75770911, 0.7584514 , 0.76231472, 0.72787624,\n",
       "        0.71370104, 0.71237204, 0.67337402, 0.682871  , 0.68400925,\n",
       "        0.73822748, 0.72491252, 0.72889415, 0.73564432, 0.75333227,\n",
       "        0.74609754, 0.70736536, 0.71149005, 0.70615355, 0.71765023,\n",
       "        0.71829525, 0.71420434, 0.73564432, 0.73884213, 0.70958152,\n",
       "        0.72631099, 0.73865538, 0.74723703, 0.70937937, 0.72780061,\n",
       "        0.72268785, 0.67514982, 0.71268053, 0.75218156, 0.71500626,\n",
       "        0.74270926, 0.70849919, 0.73673786, 0.73008022, 0.73523703,\n",
       "        0.70641442, 0.7242768 , 0.71757186, 0.70725928, 0.71649005,\n",
       "        0.70753767, 0.73008022, 0.72491252, 0.70849919, 0.74764432,\n",
       "        0.74764432, 0.7270326 , 0.63580621, 0.63425066, 0.67308361,\n",
       "        0.63580621, 0.64336177, 0.64400823, 0.6908039 , 0.69862999,\n",
       "        0.6902019 , 0.6908039 , 0.6902019 , 0.6902019 , 0.66506324,\n",
       "        0.67072611, 0.67721093, 0.67615035, 0.70876937, 0.66506324,\n",
       "        0.73088029, 0.73164578, 0.74641142, 0.72402315, 0.73088029,\n",
       "        0.73849934, 0.69117446, 0.73292615, 0.71124763, 0.74905715,\n",
       "        0.74363291, 0.71268053, 0.72316959, 0.71718845, 0.72509694,\n",
       "        0.7276656 , 0.7340403 , 0.72316959, 0.71977763, 0.7494746 ,\n",
       "        0.70248759, 0.71852222, 0.71339668, 0.74434906, 0.70871233,\n",
       "        0.7297695 , 0.7297695 , 0.7297695 , 0.72157358, 0.7297695 ,\n",
       "        0.73049348, 0.68849781, 0.73221509, 0.73221509, 0.73721942,\n",
       "        0.72402595, 0.72312015, 0.72312015, 0.73475651, 0.73367426,\n",
       "        0.7297695 , 0.72312015, 0.70126271, 0.72639824, 0.73049348,\n",
       "        0.73221509, 0.7121718 , 0.73721942, 0.73367426, 0.7297695 ,\n",
       "        0.7297695 , 0.72312015, 0.7297695 , 0.73748378, 0.73221509,\n",
       "        0.70126271, 0.70682681, 0.72151668, 0.74312418, 0.73221509,\n",
       "        0.72387206, 0.71460977, 0.72470456, 0.7297695 , 0.72312015,\n",
       "        0.72312015, 0.65384775, 0.65581794, 0.65451313, 0.64311257,\n",
       "        0.66016392, 0.64537027, 0.72368342, 0.7199424 , 0.72440907,\n",
       "        0.72865033, 0.71664758, 0.72440907, 0.7551847 , 0.71358145,\n",
       "        0.72247034, 0.72496248, 0.71568773, 0.72247034, 0.76404718,\n",
       "        0.74715829, 0.74741081, 0.72830114, 0.74166478, 0.74269162,\n",
       "        0.75787324, 0.74980061, 0.76075673, 0.74980061, 0.75504618,\n",
       "        0.75847101, 0.76151066, 0.74454096, 0.75673973, 0.74017733,\n",
       "        0.75942736, 0.74017733, 0.76248833, 0.75037204, 0.75828054,\n",
       "        0.75828054, 0.74947274, 0.75828054, 0.75714702, 0.76190893,\n",
       "        0.73166695, 0.74017733, 0.73084399, 0.73166695, 0.75120434,\n",
       "        0.72369468, 0.74725023, 0.74602535, 0.75828054, 0.75658357,\n",
       "        0.75339855, 0.75257559, 0.73084399, 0.76190893, 0.78057559,\n",
       "        0.75339855, 0.75157924, 0.75198319, 0.74725023, 0.74536847,\n",
       "        0.70747526, 0.73220374, 0.74017733, 0.74406522, 0.72563664,\n",
       "        0.74017733, 0.75714702, 0.75257559, 0.73941063, 0.74969748,\n",
       "        0.76887703, 0.75161752, 0.75320807, 0.73615672, 0.73084399,\n",
       "        0.73084399, 0.74781369, 0.74017733, 0.74406522, 0.72233361,\n",
       "        0.64597216, 0.67406495, 0.68238164, 0.68320781, 0.66016392,\n",
       "        0.64537027, 0.71209126, 0.72517207, 0.72087427, 0.7202754 ,\n",
       "        0.72440907, 0.71315979, 0.72693701, 0.73813335, 0.72693701,\n",
       "        0.7201544 , 0.72693701, 0.7551847 , 0.74269162, 0.74269162,\n",
       "        0.75958051, 0.74269162, 0.73674696, 0.74269162, 0.7466788 ,\n",
       "        0.76970911, 0.74818903, 0.77618903, 0.75120658, 0.76720658,\n",
       "        0.74017733, 0.73084399, 0.76190893, 0.74454096, 0.7636059 ,\n",
       "        0.78990893, 0.75828054, 0.74725023, 0.7262459 , 0.73401615,\n",
       "        0.76423838, 0.75142339, 0.73084399, 0.7636059 , 0.74781369,\n",
       "        0.7636059 , 0.72233361, 0.74017733, 0.75753708, 0.74725023,\n",
       "        0.74278726, 0.74602535, 0.76423838, 0.75225569, 0.7636059 ,\n",
       "        0.73084399, 0.75714702, 0.74017733, 0.76190893, 0.74017733,\n",
       "        0.75198319, 0.75198319, 0.76722129, 0.74011626, 0.73941063,\n",
       "        0.74725023, 0.74863664, 0.7636059 , 0.74741176, 0.73166695,\n",
       "        0.76190893, 0.7636059 , 0.74725023, 0.73220777, 0.73511193,\n",
       "        0.75198319, 0.7630135 , 0.7368825 , 0.75339855, 0.76442885,\n",
       "        0.73084399, 0.75339855, 0.77293923, 0.74406522]),\n",
       " 'std_test_score': array([0.02018857, 0.0282988 , 0.02349974, 0.02563475, 0.01340772,\n",
       "        0.02474103, 0.07470846, 0.07472569, 0.10386764, 0.0907098 ,\n",
       "        0.07376866, 0.07381978, 0.07948064, 0.08334235, 0.01826466,\n",
       "        0.02863557, 0.03359629, 0.02690517, 0.08219511, 0.0924826 ,\n",
       "        0.0924826 , 0.08988269, 0.09415573, 0.08511274, 0.07498207,\n",
       "        0.05670078, 0.0124952 , 0.00805937, 0.0396023 , 0.05458611,\n",
       "        0.09096453, 0.08765979, 0.0779925 , 0.09039242, 0.08567199,\n",
       "        0.09631205, 0.0460883 , 0.0786885 , 0.02709771, 0.02678076,\n",
       "        0.02480316, 0.07746221, 0.07875594, 0.0780426 , 0.07408225,\n",
       "        0.08553635, 0.07334164, 0.08820193, 0.03858134, 0.07570443,\n",
       "        0.04555919, 0.02290912, 0.06872653, 0.03961308, 0.07084539,\n",
       "        0.07100689, 0.07100689, 0.08185057, 0.07100689, 0.06423919,\n",
       "        0.0379514 , 0.05628551, 0.03384323, 0.01631725, 0.04058633,\n",
       "        0.06388666, 0.07100689, 0.07100689, 0.08962262, 0.08174606,\n",
       "        0.0780426 , 0.0786181 , 0.06992402, 0.03384323, 0.04186767,\n",
       "        0.02726378, 0.04657194, 0.01839466, 0.07336791, 0.08261196,\n",
       "        0.07100689, 0.08578306, 0.08754794, 0.06998183, 0.03283191,\n",
       "        0.02780766, 0.01462133, 0.02251778, 0.02922836, 0.03283191,\n",
       "        0.10254503, 0.09738186, 0.08531416, 0.09935049, 0.10475772,\n",
       "        0.10119295, 0.07696193, 0.08266442, 0.08208084, 0.07720829,\n",
       "        0.0819758 , 0.02812399, 0.09064652, 0.09035593, 0.10055467,\n",
       "        0.08417357, 0.09064652, 0.09064652, 0.05177042, 0.04894346,\n",
       "        0.08276397, 0.03198761, 0.02424532, 0.09054728, 0.098968  ,\n",
       "        0.09067552, 0.09837205, 0.08807032, 0.08034163, 0.09064652,\n",
       "        0.09011303, 0.06411552, 0.07384208, 0.07105634, 0.04207805,\n",
       "        0.06062822, 0.08840871, 0.07435407, 0.07643094, 0.07465538,\n",
       "        0.08095072, 0.08584169, 0.08686606, 0.03818612, 0.07589555,\n",
       "        0.07083992, 0.08449251, 0.07635546, 0.07811785, 0.07465538,\n",
       "        0.08079021, 0.08820193, 0.07979944, 0.08857531, 0.09219228,\n",
       "        0.05236217, 0.07595165, 0.0560315 , 0.04186052, 0.06432693,\n",
       "        0.0792545 , 0.0821791 , 0.08584169, 0.08227443, 0.07869081,\n",
       "        0.09927828, 0.07051426, 0.05369809, 0.04483246, 0.02641466,\n",
       "        0.04172622, 0.07174209, 0.08190717, 0.08149879, 0.07844608,\n",
       "        0.08708317, 0.07435407, 0.08190717, 0.02780766, 0.01803851,\n",
       "        0.07280991, 0.01907449, 0.02780766, 0.03078309, 0.09573745,\n",
       "        0.09500181, 0.09301972, 0.0934949 , 0.09404848, 0.09500181,\n",
       "        0.03359629, 0.02811511, 0.02646358, 0.03369214, 0.08904527,\n",
       "        0.07941797, 0.0985903 , 0.09082133, 0.097317  , 0.09510067,\n",
       "        0.09311748, 0.09064652, 0.07261655, 0.05550818, 0.08918703,\n",
       "        0.08924062, 0.07403976, 0.07209315, 0.0989426 , 0.08635118,\n",
       "        0.08130981, 0.09064652, 0.09464151, 0.09737512, 0.07525153,\n",
       "        0.06978108, 0.06005045, 0.04451623, 0.04634925, 0.04262381,\n",
       "        0.07550946, 0.06903098, 0.07208425, 0.08174606, 0.09278283,\n",
       "        0.08542802, 0.03391124, 0.06172141, 0.06695528, 0.05713598,\n",
       "        0.06203368, 0.06087633, 0.08174606, 0.08584169, 0.07804945,\n",
       "        0.0780426 , 0.08262461, 0.08748513, 0.07737876, 0.07589555,\n",
       "        0.0786773 , 0.04531397, 0.06972443, 0.0898419 , 0.07545439,\n",
       "        0.08445574, 0.08018996, 0.07786691, 0.07465538, 0.08149879,\n",
       "        0.04359441, 0.05757554, 0.06602181, 0.06256308, 0.08024628,\n",
       "        0.04513855, 0.07465538, 0.06903098, 0.08018996, 0.08765979,\n",
       "        0.08765979, 0.07259238, 0.0228789 , 0.02122282, 0.07480025,\n",
       "        0.0228789 , 0.03396392, 0.03427782, 0.07838407, 0.07981299,\n",
       "        0.07817612, 0.07838407, 0.07817612, 0.07817612, 0.08394365,\n",
       "        0.0720723 , 0.08139907, 0.07014699, 0.08063058, 0.08394365,\n",
       "        0.06147743, 0.08828149, 0.07748459, 0.06830332, 0.06147743,\n",
       "        0.06804082, 0.0434386 , 0.06726022, 0.07376706, 0.07123931,\n",
       "        0.0667896 , 0.07389097, 0.06801272, 0.07228509, 0.06703395,\n",
       "        0.07958014, 0.06535111, 0.06801272, 0.06111226, 0.07163841,\n",
       "        0.04115122, 0.06844393, 0.05920116, 0.0653419 , 0.07998938,\n",
       "        0.07890506, 0.07890506, 0.07890506, 0.07570546, 0.07890506,\n",
       "        0.05463365, 0.02685695, 0.05718785, 0.05718785, 0.0607095 ,\n",
       "        0.09290285, 0.08288403, 0.08288403, 0.07730017, 0.08630799,\n",
       "        0.07890506, 0.08288403, 0.04188691, 0.09076209, 0.05463365,\n",
       "        0.05718785, 0.05993824, 0.0607095 , 0.08630799, 0.07890506,\n",
       "        0.07890506, 0.08288403, 0.07890506, 0.09359408, 0.05718785,\n",
       "        0.04188691, 0.03823573, 0.07179538, 0.06658232, 0.05718785,\n",
       "        0.06789719, 0.09051568, 0.06943543, 0.07890506, 0.08288403,\n",
       "        0.08288403, 0.03098481, 0.02835931, 0.02714274, 0.0344982 ,\n",
       "        0.0335765 , 0.02948181, 0.09181658, 0.0841906 , 0.08882482,\n",
       "        0.0938025 , 0.08340799, 0.08882482, 0.10005064, 0.08607101,\n",
       "        0.08306945, 0.08806557, 0.08503396, 0.08306945, 0.09241674,\n",
       "        0.06953369, 0.06981075, 0.06628678, 0.0785957 , 0.06508739,\n",
       "        0.08369217, 0.08073531, 0.09150379, 0.08073531, 0.09509017,\n",
       "        0.07488795, 0.09851577, 0.06983096, 0.09966976, 0.07056782,\n",
       "        0.09667937, 0.07056782, 0.07682054, 0.07252151, 0.08359601,\n",
       "        0.08359601, 0.08661865, 0.08359601, 0.09978432, 0.07729383,\n",
       "        0.08120018, 0.07056782, 0.06687796, 0.08120018, 0.0739978 ,\n",
       "        0.05583826, 0.06866317, 0.06989549, 0.08359601, 0.07011089,\n",
       "        0.08920692, 0.07663443, 0.06687796, 0.07729383, 0.11050321,\n",
       "        0.08920692, 0.06556208, 0.07710838, 0.06866317, 0.08458156,\n",
       "        0.08259477, 0.08918033, 0.07056782, 0.08773548, 0.07264276,\n",
       "        0.07056782, 0.09978432, 0.07663443, 0.07755577, 0.08218326,\n",
       "        0.08807795, 0.07278576, 0.07589703, 0.08133929, 0.06687796,\n",
       "        0.06687796, 0.09882572, 0.07056782, 0.08773548, 0.07698995,\n",
       "        0.02442451, 0.07211845, 0.07551191, 0.0674086 , 0.0335765 ,\n",
       "        0.02948181, 0.08835344, 0.08968151, 0.08510086, 0.09781874,\n",
       "        0.08882482, 0.08593024, 0.08763414, 0.10267897, 0.08763414,\n",
       "        0.08983638, 0.08763414, 0.10005064, 0.06508739, 0.06508739,\n",
       "        0.08566675, 0.06508739, 0.06627905, 0.06508739, 0.07726502,\n",
       "        0.09563332, 0.10128883, 0.08713774, 0.1020822 , 0.08348039,\n",
       "        0.07056782, 0.06687796, 0.07729383, 0.06983096, 0.08960478,\n",
       "        0.10858081, 0.08359601, 0.06866317, 0.08159119, 0.06597824,\n",
       "        0.0888965 , 0.09082916, 0.06687796, 0.08960478, 0.09882572,\n",
       "        0.08960478, 0.07698995, 0.07056782, 0.07274978, 0.06866317,\n",
       "        0.08893657, 0.06989549, 0.0888965 , 0.0897727 , 0.08960478,\n",
       "        0.06687796, 0.09978432, 0.07056782, 0.07729383, 0.07056782,\n",
       "        0.07710838, 0.07710838, 0.0835956 , 0.06414221, 0.07755577,\n",
       "        0.06866317, 0.10890087, 0.08960478, 0.10969751, 0.08120018,\n",
       "        0.07729383, 0.08960478, 0.06866317, 0.0545007 , 0.05772569,\n",
       "        0.07710838, 0.09008303, 0.08179972, 0.08920692, 0.10047836,\n",
       "        0.06687796, 0.08920692, 0.08902032, 0.08773548]),\n",
       " 'rank_test_score': array([491, 492, 504, 501, 503, 498, 459, 437, 371, 435, 449, 440, 443,\n",
       "        452, 499, 481, 474, 497, 160, 237, 237, 284, 231, 260, 412, 408,\n",
       "        461, 446, 398, 447, 308,  98, 300, 316, 311, 286, 397, 420, 457,\n",
       "        454, 453, 381, 374, 250, 384, 161, 400, 327, 438, 314, 415, 465,\n",
       "        383, 423, 262, 290, 290, 396, 290, 317, 421, 395, 427, 470, 436,\n",
       "        410, 290, 290, 337, 174, 250, 376, 430, 427, 411, 458, 426, 434,\n",
       "        309, 183, 290, 127, 339, 355, 489, 493, 496, 500, 487, 489, 326,\n",
       "        348, 385, 377, 319, 373, 413, 432, 448, 422, 444, 482,  47, 232,\n",
       "          6, 269,  47,  47, 346, 368, 101, 424, 460,  46,  36,  59,  88,\n",
       "         83, 157,  47, 370, 367, 347, 393, 342, 304,  94, 229, 185, 215,\n",
       "        352, 154, 329, 382, 240, 247, 177, 248, 169, 215, 140, 327, 358,\n",
       "         92, 306, 414, 302, 389, 338, 369, 136, 122, 154, 184, 164,  22,\n",
       "        380, 285, 451, 464, 372, 351, 137, 178, 165, 198, 229, 137, 493,\n",
       "        502, 450, 486, 493, 483, 343, 391, 366, 386, 394, 391, 474, 479,\n",
       "        480, 469, 387, 425, 180,  44,  82,  14, 235,  47,  32, 390, 324,\n",
       "        320, 407, 399,  30, 117, 219,  47,  38,  24, 239, 330, 336, 441,\n",
       "        418, 416, 162, 258, 233, 174,  67, 114, 361, 344, 365, 310, 307,\n",
       "        325, 174, 154, 349, 250, 157, 110, 350, 240, 281, 433, 334,  72,\n",
       "        322, 130, 356, 172, 215, 178, 364, 266, 312, 362, 318, 359, 215,\n",
       "        258, 356,  98,  98, 243, 484, 488, 442, 484, 477, 476, 402, 388,\n",
       "        404, 402, 404, 404, 455, 445, 429, 431, 353, 455, 203, 202, 113,\n",
       "        268, 203, 159, 401, 190, 345,  91, 126, 334, 273, 313, 256, 242,\n",
       "        186, 273, 303,  89, 375, 305, 332, 121, 354, 220, 220, 220, 289,\n",
       "        220, 213, 409, 191, 191, 167, 267, 275, 275, 182, 188, 220, 275,\n",
       "        378, 249, 213, 191, 340, 167, 188, 220, 220, 275, 220, 166, 191,\n",
       "        378, 363, 296, 128, 191, 270, 323, 261, 220, 275, 275, 468, 466,\n",
       "        467, 478, 462, 472, 272, 301, 263, 234, 315, 263,  60, 331, 282,\n",
       "        257, 321, 282,  13, 111, 103, 236, 139, 131,  45,  85,  33,  85,\n",
       "         62,  37,  31, 119,  57, 141,  35, 141,  23,  84,  39,  39,  90,\n",
       "         39,  54,  25, 199, 141, 205, 199,  81, 271, 104, 115,  39,  58,\n",
       "         63,  69, 205,  25,   2,  63,  78,  73, 104, 118, 360, 197, 141,\n",
       "        123, 254, 141,  54,  69, 152,  87,   7,  77,  68, 173, 205, 205,\n",
       "         96, 141, 123, 287, 471, 439, 419, 417, 462, 472, 341, 255, 297,\n",
       "        298, 263, 333, 244, 163, 244, 299, 244,  60, 131, 131,  34, 131,\n",
       "        171, 131, 112,   5,  95,   3,  80,   9, 141, 205,  25, 119,  15,\n",
       "          1,  39, 104, 253, 187,  11,  79, 205,  15,  96,  15, 287, 141,\n",
       "         53, 104, 129, 115,  11,  71,  15, 205,  54, 141,  25, 141,  73,\n",
       "         73,   8, 151, 152, 104,  93,  15, 102, 199,  25,  15, 104, 196,\n",
       "        181,  73,  21, 170,  63,  10, 205,  63,   4, 123]),\n",
       " 'split0_train_score': array([0.81011968, 0.81011968, 0.81011968, 0.81011968, 0.81011968,\n",
       "        0.80778208, 0.87492426, 0.86191671, 0.87094332, 0.81830839,\n",
       "        0.85371554, 0.85674428, 0.94488003, 0.93776265, 0.93776265,\n",
       "        0.95308363, 0.92879611, 0.95401603, 0.97411477, 0.96699634,\n",
       "        0.95401603, 0.97411477, 0.96699634, 0.94689866, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.84411465,\n",
       "        0.80318287, 0.81830839, 0.83899986, 0.81011968, 0.83899986,\n",
       "        0.87344655, 0.87335762, 0.87344655, 0.86813187, 0.86824539,\n",
       "        0.86824539, 0.96231884, 0.95401603, 0.93950243, 0.95401603,\n",
       "        0.95401603, 0.95401603, 0.95401603, 0.97411477, 0.97411477,\n",
       "        0.97411477, 0.97411477, 0.94689866, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.80318287, 0.83512923,\n",
       "        0.81313934, 0.80829765, 0.81173957, 0.86101307, 0.87344655,\n",
       "        0.85455978, 0.87344655, 0.87012286, 0.87344655, 0.86813187,\n",
       "        0.95401603, 0.96231884, 0.96231884, 0.95401603, 0.95401603,\n",
       "        0.94863524, 0.95401603, 0.95401603, 0.97411477, 0.97411477,\n",
       "        0.95401603, 0.97411477, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.8459801 , 0.84693489, 0.85367307,\n",
       "        0.81011968, 0.83633855, 0.85367307, 0.87628788, 0.87628788,\n",
       "        0.87377326, 0.87377326, 0.86599613, 0.86482804, 0.94488003,\n",
       "        0.94488003, 0.92962921, 0.92962921, 0.94488003, 0.94488003,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.97411477, 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        0.99169719, 1.        , 0.99169719, 0.99169719, 0.99169719,\n",
       "        0.99169719, 0.99169719, 0.99169719, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.83256416, 0.80956349, 0.80528983, 0.80695665,\n",
       "        0.80695665, 0.81550929, 0.90155083, 0.86204931, 0.89321814,\n",
       "        0.86813144, 0.870382  , 0.90155083, 0.96231884, 0.95401603,\n",
       "        0.95401603, 0.95401603, 0.95401603, 0.95401603, 0.96879274,\n",
       "        0.95401603, 0.9434188 , 0.9434188 , 0.95401603, 0.9434188 ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.80695665, 0.80695665, 0.81038286, 0.80183489, 0.8479349 ,\n",
       "        0.81173957, 0.870382  , 0.85399586, 0.90155083, 0.90155083,\n",
       "        0.90155083, 0.90155083, 0.95401603, 0.95401603, 0.95401603,\n",
       "        0.95401603, 0.95401603, 0.95401603, 0.97411477, 0.95401603,\n",
       "        0.97411477, 0.96879274, 0.95401603, 0.95401603, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split1_train_score': array([0.79546485, 0.80066099, 0.84974834, 0.84857882, 0.80398298,\n",
       "        0.84047224, 0.82856368, 0.84708645, 0.81037005, 0.82646757,\n",
       "        0.83785777, 0.85620403, 0.91259216, 0.91826156, 0.90167326,\n",
       "        0.90496927, 0.90903156, 0.90496927, 0.95148936, 0.94981428,\n",
       "        0.94981428, 0.94981428, 0.95519521, 0.97698887, 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.96231884, 0.98241758,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.80239708,\n",
       "        0.80918294, 0.80354869, 0.80354869, 0.85478978, 0.79344877,\n",
       "        0.84047224, 0.84047224, 0.83195411, 0.85620403, 0.87163089,\n",
       "        0.83117815, 0.93697368, 0.93697368, 0.94434548, 0.94434548,\n",
       "        0.92935079, 0.92935079, 0.95693637, 0.95693637, 0.96231884,\n",
       "        0.94981428, 0.94436926, 0.95693637, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.83888634, 0.83195411,\n",
       "        0.83195411, 0.80918294, 0.81208956, 0.83888634, 0.84949245,\n",
       "        0.85768879, 0.84949245, 0.85768879, 0.89789918, 0.85761277,\n",
       "        0.92935079, 0.94434548, 0.94974913, 0.92935079, 0.92935079,\n",
       "        0.94434548, 0.96231884, 0.95693637, 0.95693637, 0.95693637,\n",
       "        0.96231884, 0.95693637, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.87004499, 0.87004499, 0.8683089 ,\n",
       "        0.86311276, 0.83888634, 0.87274922, 0.88737287, 0.87009617,\n",
       "        0.88737287, 0.87163089, 0.88066129, 0.88066129, 0.92935079,\n",
       "        0.93697368, 0.93697368, 0.92935079, 0.92935079, 0.92935079,\n",
       "        0.94436926, 0.94436926, 0.94436926, 0.94436926, 0.94436926,\n",
       "        0.97149711, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.83888634, 0.83888634, 0.83358634, 0.82820818,\n",
       "        0.83358634, 0.82820818, 0.83092859, 0.83092859, 0.82887923,\n",
       "        0.82887923, 0.82887923, 0.82201831, 0.92549283, 0.91636003,\n",
       "        0.92549283, 0.92549283, 0.92549283, 0.93347252, 0.94651413,\n",
       "        0.94651413, 0.94651413, 0.94651413, 0.9411781 , 0.95242375,\n",
       "        0.99267399, 1.        , 0.99267399, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.83888634, 0.83888634, 0.83888634, 0.82820818, 0.82820818,\n",
       "        0.83358634, 0.82887923, 0.82887923, 0.82201831, 0.82887923,\n",
       "        0.82887923, 0.82887923, 0.92433972, 0.91636003, 0.92549283,\n",
       "        0.93074353, 0.91636003, 0.93876521, 0.9411781 , 0.9411781 ,\n",
       "        0.9411781 , 0.9411781 , 0.9411781 , 0.9411781 , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split2_train_score': array([0.85540828, 0.86472728, 0.86071429, 0.81953526, 0.85540828,\n",
       "        0.85540828, 0.89746791, 0.860417  , 0.85892857, 0.89746791,\n",
       "        0.89746791, 0.89746791, 0.94762971, 0.94762971, 0.94762971,\n",
       "        0.94233535, 0.93876521, 0.94762971, 0.97513935, 0.97513935,\n",
       "        0.97513935, 0.97513935, 0.97513935, 0.97513935, 1.        ,\n",
       "        0.98241758, 1.        , 0.97513935, 0.96231884, 0.97513935,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.85540828,\n",
       "        0.86071429, 0.85980335, 0.85205861, 0.86071429, 0.85980335,\n",
       "        0.89944378, 0.90480942, 0.89138073, 0.88946054, 0.90916073,\n",
       "        0.92343381, 0.94651413, 0.94762971, 0.95508685, 0.94762971,\n",
       "        0.94974913, 0.93774884, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.98241758, 0.97411477, 0.97411477, 0.98241758, 1.        ,\n",
       "        0.98241758, 1.        , 0.98241758, 0.98241758, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.87003968, 0.86071429,\n",
       "        0.86071429, 0.86071429, 0.85280181, 0.85446429, 0.88814252,\n",
       "        0.87768199, 0.8965074 , 0.88946054, 0.91506892, 0.88629274,\n",
       "        0.94651413, 0.93876521, 0.94762971, 0.95401603, 0.94651413,\n",
       "        0.95401603, 0.96564127, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.98241758, 0.97411477, 1.        , 0.98241758, 0.98241758,\n",
       "        1.        , 0.98241758, 0.98241758, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.84145494, 0.84145494, 0.84505596,\n",
       "        0.80563795, 0.83604078, 0.84145494, 0.88048934, 0.87521639,\n",
       "        0.88048934, 0.87521639, 0.88048934, 0.87038303, 0.93876521,\n",
       "        0.93876521, 0.93876521, 0.93876521, 0.93876521, 0.93876521,\n",
       "        0.96656664, 0.96656664, 0.96656664, 0.96656664, 0.96656664,\n",
       "        0.96656664, 0.99267399, 0.99267399, 0.99267399, 0.99267399,\n",
       "        0.99267399, 0.99267399, 0.99169719, 0.99169719, 1.        ,\n",
       "        0.99169719, 0.99169719, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.87432468, 0.86876477, 0.84607294, 0.87432468,\n",
       "        0.87432468, 0.84607294, 0.87446701, 0.8646152 , 0.86962469,\n",
       "        0.86962469, 0.8646152 , 0.8646152 , 0.96763746, 0.95988853,\n",
       "        0.93876521, 0.93876521, 0.95070792, 0.95877295, 0.96564127,\n",
       "        0.96564127, 0.96564127, 0.96656664, 0.96564127, 0.96564127,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.99267399,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.86876477, 0.86366722, 0.8354349 , 0.87432468, 0.86366722,\n",
       "        0.83604078, 0.85963486, 0.86437607, 0.8646152 , 0.87446701,\n",
       "        0.86962469, 0.8646152 , 0.95070792, 0.93774884, 0.95070792,\n",
       "        0.95877295, 0.95780134, 0.95877295, 0.96564127, 0.96564127,\n",
       "        0.96564127, 0.96564127, 0.96564127, 0.96564127, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split3_train_score': array([0.84802551, 0.8428501 , 0.84802551, 0.84802551, 0.84802551,\n",
       "        0.83975335, 0.89961559, 0.9004474 , 0.88257004, 0.9004474 ,\n",
       "        0.88709663, 0.89517114, 0.94333333, 0.94132595, 0.95045177,\n",
       "        0.94132595, 0.94333333, 0.93579976, 0.96699634, 0.96699634,\n",
       "        0.97411477, 0.96699634, 0.96699634, 0.96699634, 1.        ,\n",
       "        1.        , 0.99169719, 1.        , 0.99169719, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.8697124 ,\n",
       "        0.87285657, 0.87803197, 0.87803197, 0.86382741, 0.87285657,\n",
       "        0.89517114, 0.88709663, 0.88630631, 0.87789988, 0.88709663,\n",
       "        0.87789988, 0.96656664, 0.97411477, 0.94290364, 0.97411477,\n",
       "        0.96699634, 0.97411477, 0.96699634, 0.96699634, 0.97411477,\n",
       "        0.97411477, 0.97411477, 0.97411477, 1.        , 1.        ,\n",
       "        1.        , 0.99169719, 0.99169719, 0.99169719, 1.        ,\n",
       "        0.99169719, 1.        , 1.        , 1.        , 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.88703991, 0.8428501 ,\n",
       "        0.8428501 , 0.87803197, 0.88703991, 0.84802551, 0.88630631,\n",
       "        0.88630631, 0.88630631, 0.88709663, 0.88709663, 0.88709663,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.96656664, 0.96656664,\n",
       "        0.97411477, 0.96699634, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.97411477, 0.97411477, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99169719, 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.99169719, 0.99169719, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.84802551, 0.84802551, 0.85481948,\n",
       "        0.85481948, 0.85481948, 0.84802551, 0.87789988, 0.86906397,\n",
       "        0.86375306, 0.88177293, 0.89944684, 0.87789988, 0.95743064,\n",
       "        0.95743064, 0.95743064, 0.95743064, 0.96656664, 0.95743064,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.97411477, 0.99169719, 0.99169719, 0.99169719, 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.85343377, 0.86393544, 0.86052936, 0.85875164,\n",
       "        0.85860917, 0.86393544, 0.87479161, 0.87479161, 0.88901099,\n",
       "        0.87985762, 0.87479161, 0.87479161, 0.96656664, 0.96656664,\n",
       "        0.96656664, 0.96656664, 0.96656664, 0.96656664, 0.97411477,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.97411477, 0.97411477,\n",
       "        0.99169719, 1.        , 1.        , 0.99169719, 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.85860917, 0.85860917, 0.85860917, 0.85875164, 0.85860917,\n",
       "        0.85860917, 0.88901099, 0.87985762, 0.87985762, 0.87479161,\n",
       "        0.87479161, 0.87985762, 0.97411477, 0.96656664, 0.96656664,\n",
       "        0.96656664, 0.96656664, 0.96656664, 0.97411477, 0.97411477,\n",
       "        0.97411477, 0.97411477, 0.97411477, 0.97411477, 1.        ,\n",
       "        0.99169719, 1.        , 0.99169719, 1.        , 0.99169719,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'split4_train_score': array([0.84934715, 0.84188336, 0.88791462, 0.83195688, 0.81884902,\n",
       "        0.85060887, 0.87011655, 0.91156049, 0.94862639, 0.87785495,\n",
       "        0.9065676 , 0.89476665, 0.97709402, 0.96954694, 0.9617548 ,\n",
       "        0.97709402, 0.97709402, 0.97489094, 0.97724316, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.97724316, 0.97724316, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 1.        , 0.98441948,\n",
       "        0.98441948, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.98441948, 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.88446466,\n",
       "        0.89496096, 0.87733717, 0.86441812, 0.89216994, 0.87429488,\n",
       "        0.91060999, 0.92252439, 0.90035358, 0.92252439, 0.91060999,\n",
       "        0.91808858, 0.98441948, 0.98441948, 0.96954694, 0.98441948,\n",
       "        0.97709402, 0.98441948, 0.97904429, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 1.        , 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 1.        ,\n",
       "        0.98441948, 0.98441948, 1.        , 0.98441948, 0.98441948,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.85206963, 0.88969284,\n",
       "        0.88791462, 0.85827002, 0.86715261, 0.82493259, 0.93711042,\n",
       "        0.90289453, 0.91060999, 0.92347267, 0.91808858, 0.93781311,\n",
       "        0.97904429, 0.97709402, 0.97709402, 0.98441948, 0.97709402,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.98441948, 1.        ,\n",
       "        0.98441948, 0.98441948, 0.98441948, 1.        , 1.        ,\n",
       "        0.98441948, 1.        , 1.        , 0.98441948, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.84436054, 0.81071552, 0.84211619,\n",
       "        0.82114607, 0.8495927 , 0.84436054, 0.89605275, 0.90530163,\n",
       "        0.87278852, 0.87278852, 0.87278852, 0.90530163, 0.97709402,\n",
       "        0.97709402, 0.96001841, 0.97709402, 0.97709402, 0.97709402,\n",
       "        0.98441948, 0.97904429, 0.98441948, 0.97621435, 0.97621435,\n",
       "        0.97904429, 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.86478338, 0.85796445, 0.8552221 , 0.85796445,\n",
       "        0.8571185 , 0.85796445, 0.94438798, 0.93225006, 0.89236418,\n",
       "        0.88328693, 0.91334463, 0.89173011, 0.96954694, 0.97904429,\n",
       "        0.97904429, 0.96954694, 0.97171997, 0.97709402, 0.97904429,\n",
       "        0.97904429, 0.97904429, 0.97904429, 0.97904429, 0.97904429,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.86478338, 0.86758175, 0.86758175, 0.85796445, 0.82653523,\n",
       "        0.85796445, 0.88328693, 0.89236418, 0.88328693, 0.90046416,\n",
       "        0.91588621, 0.9156507 , 0.97904429, 0.97904429, 0.98441948,\n",
       "        0.98441948, 0.98441948, 0.98441948, 0.97904429, 0.97904429,\n",
       "        0.97904429, 0.97904429, 0.97904429, 0.97904429, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'mean_train_score': array([0.83167309, 0.83204828, 0.85130449, 0.83164323, 0.82727709,\n",
       "        0.83880496, 0.8741376 , 0.87628561, 0.87428767, 0.86410924,\n",
       "        0.87654109, 0.8800708 , 0.94510585, 0.94290537, 0.93985444,\n",
       "        0.94376164, 0.93940405, 0.94346114, 0.9689966 , 0.96867316,\n",
       "        0.96750078, 0.97009684, 0.96831408, 0.96865328, 0.9968839 ,\n",
       "        0.99336741, 0.99522333, 0.99191177, 0.98015087, 0.98839528,\n",
       "        0.9968839 , 0.9968839 , 0.9968839 , 1.        , 0.9968839 ,\n",
       "        0.9968839 , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.9968839 , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 0.85121941,\n",
       "        0.84817952, 0.84740592, 0.84741145, 0.85632422, 0.84788069,\n",
       "        0.88382874, 0.88565206, 0.87668826, 0.88284414, 0.88934873,\n",
       "        0.88376916, 0.95935855, 0.95943074, 0.95027707, 0.96090509,\n",
       "        0.95544126, 0.95592998, 0.96622156, 0.97131635, 0.97381653,\n",
       "        0.97297618, 0.97022661, 0.96729681, 0.99648352, 0.9968839 ,\n",
       "        0.99336741, 0.99522333, 0.99170685, 0.99170685, 1.        ,\n",
       "        0.99522333, 0.9968839 , 1.        , 0.9968839 , 0.99522333,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 0.85024369, 0.85206812,\n",
       "        0.84731449, 0.84289938, 0.84616469, 0.84546436, 0.88689965,\n",
       "        0.87582628, 0.88327254, 0.8855683 , 0.89831997, 0.88738942,\n",
       "        0.956608  , 0.95932766, 0.9621813 , 0.95767379, 0.95470832,\n",
       "        0.9611062 , 0.96667839, 0.96872028, 0.97274003, 0.97274003,\n",
       "        0.97145734, 0.97274003, 0.9968839 , 0.99336741, 0.99648352,\n",
       "        0.9968839 , 0.99336741, 0.98819037, 1.        , 1.        ,\n",
       "        0.9968839 , 1.        , 0.99833944, 0.99522333, 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 0.84997322, 0.84343517, 0.85279472,\n",
       "        0.83096719, 0.84313557, 0.85205266, 0.88362054, 0.87919321,\n",
       "        0.87563541, 0.8750364 , 0.87987642, 0.87981477, 0.94950414,\n",
       "        0.95102872, 0.94456343, 0.94645397, 0.95133134, 0.94950414,\n",
       "        0.96871699, 0.96764195, 0.96871699, 0.96707596, 0.96707596,\n",
       "        0.97306752, 0.99521368, 0.99687424, 0.99687424, 0.99687424,\n",
       "        0.99687424, 0.9985348 , 0.99667888, 0.99667888, 0.99833944,\n",
       "        0.99667888, 0.99667888, 0.99833944, 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 0.85279847, 0.8478229 , 0.84014011, 0.84524112,\n",
       "        0.84611907, 0.84233806, 0.88522521, 0.87292695, 0.87461945,\n",
       "        0.86595598, 0.87040253, 0.87094121, 0.95831254, 0.9551751 ,\n",
       "        0.952777  , 0.95087753, 0.95370068, 0.95798443, 0.96682144,\n",
       "        0.9638661 , 0.96174665, 0.96193173, 0.96279889, 0.96292858,\n",
       "        0.99687424, 1.        , 0.9985348 , 0.99833944, 0.99687424,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        0.84760007, 0.84714023, 0.842179  , 0.84421677, 0.84499094,\n",
       "        0.83958806, 0.8662388 , 0.86389459, 0.87026578, 0.87603057,\n",
       "        0.87814652, 0.87811072, 0.95644455, 0.95074717, 0.95624058,\n",
       "        0.95890373, 0.9558327 , 0.96050806, 0.96681864, 0.96279889,\n",
       "        0.96681864, 0.96575423, 0.96279889, 0.96279889, 0.9985348 ,\n",
       "        0.99833944, 1.        , 0.99833944, 1.        , 0.99833944,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "        1.        , 1.        , 1.        , 1.        ]),\n",
       " 'std_train_score': array([0.02416079, 0.02344078, 0.02505278, 0.01526537, 0.02063921,\n",
       "        0.01661987, 0.0256406 , 0.02505585, 0.04455871, 0.03503376,\n",
       "        0.02633761, 0.01928936, 0.02044478, 0.01654299, 0.02056483,\n",
       "        0.02328416, 0.0222324 , 0.02305184, 0.0094068 , 0.01141086,\n",
       "        0.01328899, 0.01155599, 0.00777033, 0.01150084, 0.00623221,\n",
       "        0.00814786, 0.0062866 , 0.01033157, 0.01537198, 0.00996593,\n",
       "        0.00623221, 0.00623221, 0.00623221, 0.        , 0.00623221,\n",
       "        0.00623221, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00623221, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.02792448,\n",
       "        0.03605555, 0.03084689, 0.02546816, 0.02644882, 0.03001785,\n",
       "        0.02480843, 0.02800368, 0.02400031, 0.02266668, 0.01793824,\n",
       "        0.03403633, 0.01646105, 0.01739517, 0.01095542, 0.01565463,\n",
       "        0.01623272, 0.02089502, 0.00961785, 0.00908626, 0.00699837,\n",
       "        0.01232198, 0.01353066, 0.01348761, 0.00703297, 0.00623221,\n",
       "        0.00814786, 0.0062866 , 0.00744243, 0.00744243, 0.        ,\n",
       "        0.0062866 , 0.00623221, 0.        , 0.00623221, 0.0062866 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.02861079, 0.02129247,\n",
       "        0.02550191, 0.02871238, 0.0300051 , 0.01260273, 0.02864717,\n",
       "        0.01803994, 0.02082953, 0.02222636, 0.01682945, 0.02757295,\n",
       "        0.01822873, 0.01543068, 0.01209234, 0.01801868, 0.01644608,\n",
       "        0.01548838, 0.00995253, 0.01148687, 0.00885251, 0.00885251,\n",
       "        0.01168689, 0.00885251, 0.00623221, 0.00814786, 0.00703297,\n",
       "        0.00623221, 0.00814786, 0.00682107, 0.        , 0.        ,\n",
       "        0.00623221, 0.        , 0.00332112, 0.0062866 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.01026331, 0.01905728, 0.00915948,\n",
       "        0.02355807, 0.00765245, 0.01112359, 0.00727926, 0.01335113,\n",
       "        0.0079258 , 0.00356817, 0.01119727, 0.0139106 , 0.01653418,\n",
       "        0.01487366, 0.01198896, 0.01840656, 0.01776796, 0.01653418,\n",
       "        0.01343336, 0.01230108, 0.01343336, 0.01181908, 0.01181908,\n",
       "        0.00406554, 0.00392426, 0.0038407 , 0.0038407 , 0.0038407 ,\n",
       "        0.0038407 , 0.0029304 , 0.00406753, 0.00406753, 0.00332112,\n",
       "        0.00406753, 0.00406753, 0.00332112, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.01555888, 0.02165489, 0.01967679, 0.02427382,\n",
       "        0.02350877, 0.01813294, 0.03725448, 0.0330877 , 0.02443366,\n",
       "        0.01942418, 0.02691084, 0.02763122, 0.01657999, 0.02111717,\n",
       "        0.01909144, 0.0167071 , 0.01609327, 0.01453296, 0.01113783,\n",
       "        0.01213819, 0.01438948, 0.01444423, 0.0137451 , 0.01329051,\n",
       "        0.0038407 , 0.        , 0.0029304 , 0.00332112, 0.0038407 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.0227694 , 0.02238473, 0.01991266, 0.02592006, 0.01526565,\n",
       "        0.01745644, 0.02128721, 0.02186151, 0.02683302, 0.0263648 ,\n",
       "        0.02994391, 0.03022452, 0.01945514, 0.02196861, 0.01939928,\n",
       "        0.01747385, 0.02235034, 0.01500914, 0.01352595, 0.0137451 ,\n",
       "        0.01352595, 0.01311302, 0.0137451 , 0.0137451 , 0.0029304 ,\n",
       "        0.00332112, 0.        , 0.00332112, 0.        , 0.00332112,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        ])}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grid Search 2\n",
    "\n",
    "# Usando um grid completo de todos os parâmetros\n",
    "param_grid = {\"max_depth\": range(3,10,1),\n",
    "              \"min_samples_split\": range(1,4),\n",
    "              \"max_features\": ['sqrt', 'log2'],\n",
    "              \"criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "              \"n_estimators\": range(400,600,100),\n",
    "              \"bootstrap\": [True, False]}\n",
    "\n",
    "# Executando o Grid Search\n",
    "grid_search = GridSearchCV(modelo4_multi, param_grid = param_grid, return_train_score = True, scoring = 'f1_macro')\n",
    "start = time.time()\n",
    "grid_search.fit(X_falha_treino, y_falha_treino)\n",
    "\n",
    "print(\"GridSearchCV executou em %.2f segundos para todas as combinações de candidatos a parâmetros do modelo.\"\n",
    "      % (time.time() - start))\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ee1211b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': False,\n",
       " 'criterion': 'log_loss',\n",
       " 'max_depth': 5,\n",
       " 'max_features': 'log2',\n",
       " 'min_samples_split': 3,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fd3983a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7899089273897457"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41dfbc3",
   "metadata": {},
   "source": [
    "A segunda tentativa de otimização não resultou em uma melhora expressiva da métrica, vamos ver como fica a matriz de confusão e a acurácia do modelo em teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6913febb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Matriz de confusão\n",
      " [[19  0  0  0  0]\n",
      " [ 0 11  1  1  0]\n",
      " [ 0  1  9  5  1]\n",
      " [ 0  0  0  3  0]\n",
      " [ 0  0  0  1  6]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Modelo': 'XGBoost Classifier Grid Search 2',\n",
       " 'Versão': 'Otimizada',\n",
       " 'Precision': 0.7947619047619047,\n",
       " 'Recall': 0.8531593406593407,\n",
       " 'F1 Score': 0.7781978021978022,\n",
       " 'Acurácia': 0.8275862068965517}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes_multi = grid_search.predict(X_falha_val)\n",
    "\n",
    "# Avaliação do modelo\n",
    "# Matriz de confusão\n",
    "print('\\nMatriz de confusão\\n', confusion_matrix(y_falha_val, previsoes_multi))\n",
    "\n",
    "# Dicionário de métricas e metadados\n",
    "dict_model =   {'Modelo': 'XGBoost Classifier Grid Search 2',\n",
    "                'Versão': 'Otimizada',\n",
    "                'Precision':precision_score(y_falha_val, previsoes_multi, average = 'macro', zero_division=0),\n",
    "                'Recall':recall_score(y_falha_val, previsoes_multi, average = 'macro', zero_division=0),\n",
    "                'F1 Score':f1_score(y_falha_val, previsoes_multi, average = 'macro', zero_division=0),\n",
    "                'Acurácia':accuracy_score(y_falha_val, previsoes_multi)}\n",
    "\n",
    "dict_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e795d2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 3, 4, 1, 0, 2, 2, 0, 3, 4, 1, 1, 0, 4, 2, 4, 0, 2, 0, 0, 3,\n",
       "       0, 1, 0, 1, 2, 0, 0, 1, 3, 1, 3, 0, 0, 1, 0, 3, 0, 4, 3, 0, 3, 2,\n",
       "       2, 0, 1, 4, 1, 2, 1, 2, 0, 4, 0, 3, 2, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes_multi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7797489",
   "metadata": {},
   "source": [
    "No nosso conjunto de dados de validação, o modelo acertou todas 48 das 58 previsões, atingindo 82,75% de acurácia. Vamos usar esse como o segundo modelo do nosso pipeline para fazer as previsões completas e ver como ficam as métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f243d43a",
   "metadata": {},
   "source": [
    "# Avaliação do pipeline completo\n",
    "\n",
    "Ao longo da modelagem, construímos 2 modelos preditivos. Um de classificação binária, para prever se havia ou não falha nas máquinas, e outro de classificação multiclasse, para prever se, dado que o modelo previu que havia uma falha, que tipo de falha ela seria. O segundo modelo ainda pode classificar como \"No Failure\", sendo assim um segundo filtro. \n",
    "\n",
    "Avaliamos cada modelo e suas previsões separadamente, mas é interessante prever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2f99132a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5089</th>\n",
       "      <td>0.206892</td>\n",
       "      <td>0.948209</td>\n",
       "      <td>0.698529</td>\n",
       "      <td>-1.078106</td>\n",
       "      <td>0.524390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>0.307330</td>\n",
       "      <td>-0.059381</td>\n",
       "      <td>-0.499925</td>\n",
       "      <td>0.232524</td>\n",
       "      <td>0.686992</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>1.060613</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>-0.691228</td>\n",
       "      <td>0.402605</td>\n",
       "      <td>0.174797</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>0.457986</td>\n",
       "      <td>0.612346</td>\n",
       "      <td>-0.342382</td>\n",
       "      <td>0.142480</td>\n",
       "      <td>0.406504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2330</th>\n",
       "      <td>0.809518</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>0.360936</td>\n",
       "      <td>-0.777961</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>2.785976</td>\n",
       "      <td>-2.338711</td>\n",
       "      <td>0.520325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6532</th>\n",
       "      <td>-0.747267</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>1.030495</td>\n",
       "      <td>-1.068101</td>\n",
       "      <td>0.691057</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6002</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.335661</td>\n",
       "      <td>0.940470</td>\n",
       "      <td>-1.408264</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>-1.902302</td>\n",
       "      <td>-2.141734</td>\n",
       "      <td>0.659143</td>\n",
       "      <td>-0.898019</td>\n",
       "      <td>0.552846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>-1.098799</td>\n",
       "      <td>-0.663935</td>\n",
       "      <td>-0.460539</td>\n",
       "      <td>0.112466</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "5089           0.206892               0.948209              0.698529   \n",
       "2028           0.307330              -0.059381             -0.499925   \n",
       "2438           1.060613               0.881037             -0.691228   \n",
       "4076           0.457986               0.612346             -0.342382   \n",
       "2330           0.809518               0.343655              0.360936   \n",
       "...                 ...                    ...                   ...   \n",
       "6252          -1.249456              -1.335661              2.785976   \n",
       "6532          -0.747267              -0.529589              1.030495   \n",
       "6002          -1.400113              -1.335661              0.940470   \n",
       "664           -1.902302              -2.141734              0.659143   \n",
       "454           -1.098799              -0.663935             -0.460539   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  prediction  \n",
       "5089  -1.078106       0.524390  0.0  1.0  0.0           1  \n",
       "2028   0.232524       0.686992  0.0  0.0  1.0           1  \n",
       "2438   0.402605       0.174797  0.0  1.0  0.0           1  \n",
       "4076   0.142480       0.406504  0.0  1.0  0.0           1  \n",
       "2330  -0.777961       0.008130  1.0  0.0  0.0           1  \n",
       "...         ...            ...  ...  ...  ...         ...  \n",
       "6252  -2.338711       0.520325  0.0  0.0  1.0           1  \n",
       "6532  -1.068101       0.691057  0.0  0.0  1.0           1  \n",
       "6002  -1.408264       0.813008  0.0  1.0  0.0           1  \n",
       "664   -0.898019       0.552846  0.0  1.0  0.0           1  \n",
       "454    0.112466       0.000000  0.0  1.0  0.0           1  \n",
       "\n",
       "[1667 rows x 9 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_bin = modelo3.predict(X_val) # Previsoes do modelo binario\n",
    "\n",
    "validacao_com_prev = X_val.copy()\n",
    "validacao_com_prev['prediction'] = prev_bin\n",
    "\n",
    "prev_falha = X_val[prev_bin == 0] # Registros em que o primeiro modelo previu falha\n",
    "\n",
    "prev_multi = grid_search.predict(prev_falha)# Previsões do modelo multiclasse\n",
    "prev_multi = np.where(prev_multi == 0, prev_multi, prev_multi+1) # Convertendo de volta para o encoding original\n",
    "\n",
    "validacao_com_prev_slice =  prev_falha.copy()\n",
    "validacao_com_prev_slice['prediction'] = prev_multi\n",
    "\n",
    "validacao_com_prev.loc[prev_falha.index] = validacao_com_prev_slice\n",
    "\n",
    "validacao_com_prev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c3084130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  18,    1,    0,    0,    0,    0],\n",
       "       [  13, 1557,    9,    4,   12,   14],\n",
       "       [   0,    1,   11,    1,    0,    0],\n",
       "       [   0,    9,    1,    6,    0,    0],\n",
       "       [   0,    3,    0,    0,    0,    0],\n",
       "       [   0,    6,    0,    0,    0,    1]], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz de confusão\n",
    "confusion_matrix(y_val,validacao_com_prev['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a129eeea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9612943356523417"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, validacao_com_prev['prediction'], average = 'weighted', zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81e6850f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47996891480440834"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_val, validacao_com_prev['prediction'], average = 'macro', zero_division=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4233460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9556088782243551"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_val, validacao_com_prev['prediction'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63b6317",
   "metadata": {},
   "source": [
    "Atingimos um F1 Score de 98,47%. Podemos então partir para previsão em novos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e1e29",
   "metadata": {},
   "source": [
    "# Previsões para o conjunto sem labels\n",
    "\n",
    "O objetivo final da modelagem era utilizar o modelo para prever o tipo de falha em 3333 registros que estão armazenados na variável _df\\_teste_. Para isso, contudo, precisamos aplicar todas as transformações que foram feitas na etapa de pré-processamento, como encoding, padronização e normalização. O LabelEncoding da variável target não é necessário visto que esse conjunto de dados não possui labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4ab15d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste = pd.read_csv(\"desafio_manutencao_preditiva_teste.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2a41e892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>1.435606</td>\n",
       "      <td>-1.338230</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-0.010416</td>\n",
       "      <td>0.732764</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.066971</td>\n",
       "      <td>-0.438033</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.295297</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>0.743541</td>\n",
       "      <td>-0.417789</td>\n",
       "      <td>0.276423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.064991</td>\n",
       "      <td>2.090145</td>\n",
       "      <td>0.068356</td>\n",
       "      <td>-0.918029</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>1.963354</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-0.702481</td>\n",
       "      <td>1.192985</td>\n",
       "      <td>0.369919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-0.319876</td>\n",
       "      <td>-0.157664</td>\n",
       "      <td>0.735772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.485591</td>\n",
       "      <td>-0.162333</td>\n",
       "      <td>-0.357760</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>0.056235</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>0.839192</td>\n",
       "      <td>-1.238182</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3333 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "0             -1.249456              -0.932625              1.435606   \n",
       "1              0.357548               0.343655             -0.010416   \n",
       "2             -1.400113              -1.066971             -0.438033   \n",
       "3             -0.295297              -0.596762              0.743541   \n",
       "4              2.064991               2.090145              0.068356   \n",
       "...                 ...                    ...                   ...   \n",
       "3328           1.261488               1.284073             -1.304521   \n",
       "3329           0.357548               0.679519             -0.702481   \n",
       "3330           0.407767               0.813864             -0.319876   \n",
       "3331           1.412145               1.485591             -0.162333   \n",
       "3332           0.056235              -0.529589              0.839192   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "0     -1.338230       0.284553  0.0  1.0  0.0  \n",
       "1      0.732764       0.780488  0.0  1.0  0.0  \n",
       "2      0.202509       0.166667  0.0  1.0  0.0  \n",
       "3     -0.417789       0.276423  0.0  1.0  0.0  \n",
       "4     -0.918029       0.036585  0.0  1.0  0.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "3328   1.963354       0.699187  0.0  1.0  0.0  \n",
       "3329   1.192985       0.369919  0.0  1.0  0.0  \n",
       "3330  -0.157664       0.735772  0.0  1.0  0.0  \n",
       "3331  -0.357760       0.211382  0.0  1.0  0.0  \n",
       "3332  -1.238182       0.386179  0.0  0.0  1.0  \n",
       "\n",
       "[3333 rows x 8 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_teste = df_teste.drop(columns = ['udi', 'product_id'])\n",
    "\n",
    "# One Hot Encoding\n",
    "cat_encoded_teste = onehot.transform(df_teste[[categorical]])\n",
    "df_teste[onehot.categories_[0]] = cat_encoded_teste.toarray()\n",
    "df_teste.drop(columns='type', inplace = True)\n",
    "\n",
    "# Scaling\n",
    "df_teste[scale] = scaler.transform(df_teste[scale])\n",
    "\n",
    "# Normalizing\n",
    "df_teste[normalize] = normalizer.transform(df_teste[normalize])\n",
    "\n",
    "df_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad18ea63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previsões do Modelo de classificação binária\n",
    "previsoes_finais_bin = modelo3.predict(df_teste)\n",
    "previsoes_finais_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4481c126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>1.435606</td>\n",
       "      <td>-1.338230</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-0.010416</td>\n",
       "      <td>0.732764</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.066971</td>\n",
       "      <td>-0.438033</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.295297</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>0.743541</td>\n",
       "      <td>-0.417789</td>\n",
       "      <td>0.276423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.064991</td>\n",
       "      <td>2.090145</td>\n",
       "      <td>0.068356</td>\n",
       "      <td>-0.918029</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>1.963354</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-0.702481</td>\n",
       "      <td>1.192985</td>\n",
       "      <td>0.369919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-0.319876</td>\n",
       "      <td>-0.157664</td>\n",
       "      <td>0.735772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.485591</td>\n",
       "      <td>-0.162333</td>\n",
       "      <td>-0.357760</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>0.056235</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>0.839192</td>\n",
       "      <td>-1.238182</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3333 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "0             -1.249456              -0.932625              1.435606   \n",
       "1              0.357548               0.343655             -0.010416   \n",
       "2             -1.400113              -1.066971             -0.438033   \n",
       "3             -0.295297              -0.596762              0.743541   \n",
       "4              2.064991               2.090145              0.068356   \n",
       "...                 ...                    ...                   ...   \n",
       "3328           1.261488               1.284073             -1.304521   \n",
       "3329           0.357548               0.679519             -0.702481   \n",
       "3330           0.407767               0.813864             -0.319876   \n",
       "3331           1.412145               1.485591             -0.162333   \n",
       "3332           0.056235              -0.529589              0.839192   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  prediction  \n",
       "0     -1.338230       0.284553  0.0  1.0  0.0           1  \n",
       "1      0.732764       0.780488  0.0  1.0  0.0           1  \n",
       "2      0.202509       0.166667  0.0  1.0  0.0           1  \n",
       "3     -0.417789       0.276423  0.0  1.0  0.0           1  \n",
       "4     -0.918029       0.036585  0.0  1.0  0.0           1  \n",
       "...         ...            ...  ...  ...  ...         ...  \n",
       "3328   1.963354       0.699187  0.0  1.0  0.0           0  \n",
       "3329   1.192985       0.369919  0.0  1.0  0.0           1  \n",
       "3330  -0.157664       0.735772  0.0  1.0  0.0           1  \n",
       "3331  -0.357760       0.211382  0.0  1.0  0.0           1  \n",
       "3332  -1.238182       0.386179  0.0  0.0  1.0           1  \n",
       "\n",
       "[3333 rows x 9 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Salvando os resultados intermediários num novo DataFrame\n",
    "df_final = df_teste.copy()\n",
    "df_final['prediction'] = previsoes_finais_bin\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "84e0fb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-0.948143</td>\n",
       "      <td>-1.604352</td>\n",
       "      <td>2.318973</td>\n",
       "      <td>-1.978538</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.960175</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.719360</td>\n",
       "      <td>0.892841</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>1.351246</td>\n",
       "      <td>-1.484570</td>\n",
       "      <td>2.723719</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>1.491871</td>\n",
       "      <td>-1.468293</td>\n",
       "      <td>0.849593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-0.044203</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>-0.612456</td>\n",
       "      <td>1.813282</td>\n",
       "      <td>0.418699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>-0.496173</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>0.902845</td>\n",
       "      <td>0.918699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>-1.550769</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-0.775626</td>\n",
       "      <td>1.703230</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-1.248255</td>\n",
       "      <td>2.113426</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-0.769999</td>\n",
       "      <td>1.293033</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>1.963354</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "118           -0.948143              -1.604352              2.318973   \n",
       "139            0.960175              -0.260899             -0.719360   \n",
       "160            0.407767               1.351246             -1.484570   \n",
       "175           -0.596610               0.410828              1.491871   \n",
       "183           -0.044203               0.881037             -0.612456   \n",
       "...                 ...                    ...                   ...   \n",
       "3308          -0.496173               0.276483             -0.972555   \n",
       "3309          -1.550769              -1.470007             -0.775626   \n",
       "3310          -1.500550              -1.470007             -1.248255   \n",
       "3311           1.713458               1.216900             -0.769999   \n",
       "3328           1.261488               1.284073             -1.304521   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  \n",
       "118   -1.978538       0.821138  0.0  1.0  0.0  \n",
       "139    0.892841       0.906504  0.0  1.0  0.0  \n",
       "160    2.723719       0.646341  0.0  0.0  1.0  \n",
       "175   -1.468293       0.849593  0.0  1.0  0.0  \n",
       "183    1.813282       0.418699  0.0  1.0  0.0  \n",
       "...         ...            ...  ...  ...  ...  \n",
       "3308   0.902845       0.918699  0.0  1.0  0.0  \n",
       "3309   1.703230       0.821138  0.0  1.0  0.0  \n",
       "3310   2.113426       0.813008  0.0  1.0  0.0  \n",
       "3311   1.293033       0.845528  0.0  1.0  0.0  \n",
       "3328   1.963354       0.699187  0.0  1.0  0.0  \n",
       "\n",
       "[208 rows x 8 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conjunto de dados cujo modelo de classificação binária previu como falha\n",
    "df_teste2 = df_teste[previsoes_finais_bin == 0]\n",
    "df_teste2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "976cfddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Previsões do Modelo de classificação multiclasse\n",
    "previsoes_finais = grid_search.predict(df_teste2)\n",
    "previsoes_finais = np.where(previsoes_finais == 0, previsoes_finais, previsoes_finais+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "58b0f08e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>-0.948143</td>\n",
       "      <td>-1.604352</td>\n",
       "      <td>2.318973</td>\n",
       "      <td>-1.978538</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.960175</td>\n",
       "      <td>-0.260899</td>\n",
       "      <td>-0.719360</td>\n",
       "      <td>0.892841</td>\n",
       "      <td>0.906504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>1.351246</td>\n",
       "      <td>-1.484570</td>\n",
       "      <td>2.723719</td>\n",
       "      <td>0.646341</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>-0.596610</td>\n",
       "      <td>0.410828</td>\n",
       "      <td>1.491871</td>\n",
       "      <td>-1.468293</td>\n",
       "      <td>0.849593</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-0.044203</td>\n",
       "      <td>0.881037</td>\n",
       "      <td>-0.612456</td>\n",
       "      <td>1.813282</td>\n",
       "      <td>0.418699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3308</th>\n",
       "      <td>-0.496173</td>\n",
       "      <td>0.276483</td>\n",
       "      <td>-0.972555</td>\n",
       "      <td>0.902845</td>\n",
       "      <td>0.918699</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3309</th>\n",
       "      <td>-1.550769</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-0.775626</td>\n",
       "      <td>1.703230</td>\n",
       "      <td>0.821138</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3310</th>\n",
       "      <td>-1.500550</td>\n",
       "      <td>-1.470007</td>\n",
       "      <td>-1.248255</td>\n",
       "      <td>2.113426</td>\n",
       "      <td>0.813008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>1.713458</td>\n",
       "      <td>1.216900</td>\n",
       "      <td>-0.769999</td>\n",
       "      <td>1.293033</td>\n",
       "      <td>0.845528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>1.963354</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "118           -0.948143              -1.604352              2.318973   \n",
       "139            0.960175              -0.260899             -0.719360   \n",
       "160            0.407767               1.351246             -1.484570   \n",
       "175           -0.596610               0.410828              1.491871   \n",
       "183           -0.044203               0.881037             -0.612456   \n",
       "...                 ...                    ...                   ...   \n",
       "3308          -0.496173               0.276483             -0.972555   \n",
       "3309          -1.550769              -1.470007             -0.775626   \n",
       "3310          -1.500550              -1.470007             -1.248255   \n",
       "3311           1.713458               1.216900             -0.769999   \n",
       "3328           1.261488               1.284073             -1.304521   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  prediction  \n",
       "118   -1.978538       0.821138  0.0  1.0  0.0           5  \n",
       "139    0.892841       0.906504  0.0  1.0  0.0           2  \n",
       "160    2.723719       0.646341  0.0  0.0  1.0           3  \n",
       "175   -1.468293       0.849593  0.0  1.0  0.0           5  \n",
       "183    1.813282       0.418699  0.0  1.0  0.0           4  \n",
       "...         ...            ...  ...  ...  ...         ...  \n",
       "3308   0.902845       0.918699  0.0  1.0  0.0           2  \n",
       "3309   1.703230       0.821138  0.0  1.0  0.0           2  \n",
       "3310   2.113426       0.813008  0.0  1.0  0.0           2  \n",
       "3311   1.293033       0.845528  0.0  1.0  0.0           2  \n",
       "3328   1.963354       0.699187  0.0  1.0  0.0           0  \n",
       "\n",
       "[208 rows x 9 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final_slice = df_teste2.copy()\n",
    "df_final_slice['prediction'] = previsoes_finais\n",
    "\n",
    "df_final_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7bb54c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.loc[df_final_slice.index] = df_final_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "204932ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_temperature_k</th>\n",
       "      <th>process_temperature_k</th>\n",
       "      <th>rotational_speed_rpm</th>\n",
       "      <th>torque_nm</th>\n",
       "      <th>tool_wear_min</th>\n",
       "      <th>H</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.249456</td>\n",
       "      <td>-0.932625</td>\n",
       "      <td>1.435606</td>\n",
       "      <td>-1.338230</td>\n",
       "      <td>0.284553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.343655</td>\n",
       "      <td>-0.010416</td>\n",
       "      <td>0.732764</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.400113</td>\n",
       "      <td>-1.066971</td>\n",
       "      <td>-0.438033</td>\n",
       "      <td>0.202509</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.295297</td>\n",
       "      <td>-0.596762</td>\n",
       "      <td>0.743541</td>\n",
       "      <td>-0.417789</td>\n",
       "      <td>0.276423</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.064991</td>\n",
       "      <td>2.090145</td>\n",
       "      <td>0.068356</td>\n",
       "      <td>-0.918029</td>\n",
       "      <td>0.036585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>1.261488</td>\n",
       "      <td>1.284073</td>\n",
       "      <td>-1.304521</td>\n",
       "      <td>1.963354</td>\n",
       "      <td>0.699187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>0.357548</td>\n",
       "      <td>0.679519</td>\n",
       "      <td>-0.702481</td>\n",
       "      <td>1.192985</td>\n",
       "      <td>0.369919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3330</th>\n",
       "      <td>0.407767</td>\n",
       "      <td>0.813864</td>\n",
       "      <td>-0.319876</td>\n",
       "      <td>-0.157664</td>\n",
       "      <td>0.735772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>1.412145</td>\n",
       "      <td>1.485591</td>\n",
       "      <td>-0.162333</td>\n",
       "      <td>-0.357760</td>\n",
       "      <td>0.211382</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>0.056235</td>\n",
       "      <td>-0.529589</td>\n",
       "      <td>0.839192</td>\n",
       "      <td>-1.238182</td>\n",
       "      <td>0.386179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3333 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      air_temperature_k  process_temperature_k  rotational_speed_rpm  \\\n",
       "0             -1.249456              -0.932625              1.435606   \n",
       "1              0.357548               0.343655             -0.010416   \n",
       "2             -1.400113              -1.066971             -0.438033   \n",
       "3             -0.295297              -0.596762              0.743541   \n",
       "4              2.064991               2.090145              0.068356   \n",
       "...                 ...                    ...                   ...   \n",
       "3328           1.261488               1.284073             -1.304521   \n",
       "3329           0.357548               0.679519             -0.702481   \n",
       "3330           0.407767               0.813864             -0.319876   \n",
       "3331           1.412145               1.485591             -0.162333   \n",
       "3332           0.056235              -0.529589              0.839192   \n",
       "\n",
       "      torque_nm  tool_wear_min    H    L    M  prediction  \n",
       "0     -1.338230       0.284553  0.0  1.0  0.0           1  \n",
       "1      0.732764       0.780488  0.0  1.0  0.0           1  \n",
       "2      0.202509       0.166667  0.0  1.0  0.0           1  \n",
       "3     -0.417789       0.276423  0.0  1.0  0.0           1  \n",
       "4     -0.918029       0.036585  0.0  1.0  0.0           1  \n",
       "...         ...            ...  ...  ...  ...         ...  \n",
       "3328   1.963354       0.699187  0.0  1.0  0.0           0  \n",
       "3329   1.192985       0.369919  0.0  1.0  0.0           1  \n",
       "3330  -0.157664       0.735772  0.0  1.0  0.0           1  \n",
       "3331  -0.357760       0.211382  0.0  1.0  0.0           1  \n",
       "3332  -1.238182       0.386179  0.0  0.0  1.0           1  \n",
       "\n",
       "[3333 rows x 9 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8fe9cacd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['prediction'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0920cf",
   "metadata": {},
   "source": [
    "# Convertendo os resultados de volta para os labels\n",
    "\n",
    "Quando fizemos o LabelEncoding da variável target, os labels tornaram-se valores numéricos. Para leitura humana, o resultado final tem mais valor sendo transformado de volta para os labels originais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7533a414",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     No Failure\n",
       "1                     No Failure\n",
       "2                     No Failure\n",
       "3                     No Failure\n",
       "4                     No Failure\n",
       "                  ...           \n",
       "3328    Heat Dissipation Failure\n",
       "3329                  No Failure\n",
       "3330                  No Failure\n",
       "3331                  No Failure\n",
       "3332                  No Failure\n",
       "Length: 3333, dtype: object"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes_finais_transformadas = pd.Series(le.inverse_transform(df_final['prediction']))\n",
    "previsoes_finais_transformadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d63f9e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Failure                  3125\n",
       "Tool Wear Failure             56\n",
       "Heat Dissipation Failure      56\n",
       "Overstrain Failure            42\n",
       "Power Failure                 32\n",
       "Random Failures               22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantidade de previsões de cada tipo\n",
    "previsoes_finais_transformadas.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9e35c8",
   "metadata": {},
   "source": [
    "# Salvando os resultados num arquivo CSV\n",
    "\n",
    "Para finalizar o trabalho, salvamos os resultados finais num arquivo CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "72c1904c",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_finais_transformadas = previsoes_finais_transformadas.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "88193801",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsoes_finais_transformadas.to_csv(\"predicted.csv\", index = False, header=['rowNumber','predictedValues'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
